{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T18:09:11.310751Z",
     "start_time": "2025-08-20T18:09:11.296926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import traceback\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "dir2 = os.path.abspath('')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "dir0 = os.path.dirname(dir1)\n",
    "\n",
    "if dir1 not in sys.path: sys.path.append(dir0)\n",
    "\n",
    "from src.config import PPOConfig, EmbeddingStrategy\n",
    "from src.experiments import ExperimentSuite\n",
    "from src.utils import ExperimentUtils\n",
    "\n",
    "\n",
    "def regular_train_high(scenario, strategy, testing_agent, steps, all_iters, my_device, url_train):\n",
    "    base_config_balance_5_agents = PPOConfig(\n",
    "        scenario_name=scenario, max_agents=testing_agent, use_strategy_defaults=True, max_steps=steps, n_agents=testing_agent, n_iters=all_iters\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        \"strategy\": [strategy],\n",
    "    }\n",
    "    suite = ExperimentSuite(base_config=base_config_balance_5_agents, param_grid=param_grid, name=\"test_all\", device=my_device)\n",
    "    suite.run_all_confidence(k=10, profile_once=False)\n",
    "\n",
    "    path_to_strategy = url_train + str(strategy) + '.csv'\n",
    "    suite_utils = ExperimentUtils(experiment_suite=suite, path=path_to_strategy)\n",
    "    suite_utils.save_df_to_file()\n",
    "\n",
    "def run_mostly_low(strategies, scenario, file_name, training_agent, testing_agent, all_iters=80, steps=200, percentage=0.9, do_high=True):\n",
    "    my_device = torch.device(\"cpu\")\n",
    "\n",
    "    assert training_agent <= testing_agent\n",
    "\n",
    "    url = \"saved_experiments\" + \"/\" + file_name\n",
    "    url_train_high = \"saved_experiments\" + \"/\" + file_name + '_just_high'\n",
    "\n",
    "    # In the future it could be handled better with simple normalization in the MLP.\n",
    "    for strategy in strategies:\n",
    "        base_config_balance_5_agents = PPOConfig(\n",
    "            scenario_name=scenario, max_agents=testing_agent, use_strategy_defaults=True, max_steps=steps, n_agents=training_agent, n_iters=int(percentage*all_iters)\n",
    "        )\n",
    "\n",
    "        param_grid = {\n",
    "            \"strategy\": [strategy],\n",
    "        }\n",
    "        # Train for 90% of iterations on 'training agents'\n",
    "        suite = ExperimentSuite(base_config=base_config_balance_5_agents, param_grid=param_grid, name=\"test_all\", device=my_device)\n",
    "        suite.run_all_confidence(k=10, profile_once=False, update=True)\n",
    "        \n",
    "        # Train for 10% of iterations on 'testing agents'\n",
    "        change_to_config = {\"n_agents\": testing_agent, \"n_iters\": int((1-percentage)*all_iters)}\n",
    "        suite.create_and_run_experiments_with_updated_config(change_to_config, create_new=False, k=10)\n",
    "\n",
    "        path_to_strategy_trained = url + str(strategy) + '.csv'\n",
    "        suite_utils = ExperimentUtils(experiment_suite=suite, path=path_to_strategy_trained)\n",
    "        suite_utils.save_df_to_file()\n",
    "        \n",
    "        if do_high:\n",
    "            # Train 100% on testing agents\n",
    "            regular_train_high(scenario, strategy, testing_agent, steps, all_iters, my_device, url_train_high)"
   ],
   "id": "999f9c9be0c45351",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T18:09:11.328039Z",
     "start_time": "2025-08-20T18:09:11.325072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# file_name='2_balance_mostly_low'\n",
    "# training_agent = 5\n",
    "# strategies = [\n",
    "#     EmbeddingStrategy.CONCAT,\n",
    "#     EmbeddingStrategy.MLP,\n",
    "#     EmbeddingStrategy.MLP_LOCAL,\n",
    "#     EmbeddingStrategy.MLP_GLOBAL,\n",
    "#     EmbeddingStrategy.GRAPH_SAGE,\n",
    "#     EmbeddingStrategy.GRAPH_GAT,\n",
    "#     EmbeddingStrategy.GRAPH_GAT_v2,\n",
    "#     EmbeddingStrategy.SET_TRANSFORMER_INV,\n",
    "#     EmbeddingStrategy.SAB_TRANSFORMER,\n",
    "#     EmbeddingStrategy.ISAB_TRANSFORMER\n",
    "# ]\n",
    "# \n",
    "# run_mostly_low(strategies, scenario='balance', file_name=file_name, training_agent=training_agent, testing_agent=20, all_iters=80)"
   ],
   "id": "c2259370728c2a7f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T21:34:52.279340Z",
     "start_time": "2025-08-20T18:09:11.342071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_name='2_balance_75_low'\n",
    "training_agent = 5\n",
    "strategies = [\n",
    "    # EmbeddingStrategy.CONCAT,\n",
    "    # EmbeddingStrategy.MLP,\n",
    "    # EmbeddingStrategy.MLP_LOCAL,\n",
    "    # EmbeddingStrategy.MLP_GLOBAL,\n",
    "    # EmbeddingStrategy.GRAPH_SAGE,\n",
    "    # EmbeddingStrategy.GRAPH_GAT,\n",
    "    EmbeddingStrategy.GRAPH_GAT_v2,\n",
    "    EmbeddingStrategy.SET_TRANSFORMER_INV,\n",
    "    EmbeddingStrategy.SAB_TRANSFORMER,\n",
    "    EmbeddingStrategy.ISAB_TRANSFORMER\n",
    "]\n",
    "\n",
    "run_mostly_low(strategies, scenario='balance', file_name=file_name, training_agent=training_agent, testing_agent=20, all_iters=80, percentage=0.75, do_high=False)"
   ],
   "id": "6a9b27bd20bed66",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 20:09:11,428 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:09:11,482 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -9.69063663482666:   2%|▏         | 1/60 [00:01<01:51,  1.88s/it]\u001B[A\n",
      "episode_reward_mean = -9.85783863067627:   3%|▎         | 2/60 [00:03<01:40,  1.74s/it]\u001B[A\n",
      "episode_reward_mean = -8.959911346435547:   5%|▌         | 3/60 [00:05<01:39,  1.74s/it]\u001B[A\n",
      "episode_reward_mean = -10.561514854431152:   7%|▋         | 4/60 [00:07<01:39,  1.77s/it]\u001B[A\n",
      "episode_reward_mean = -8.402442932128906:   8%|▊         | 5/60 [00:08<01:34,  1.72s/it] \u001B[A\n",
      "episode_reward_mean = -5.497735977172852:  10%|█         | 6/60 [00:10<01:30,  1.68s/it]\u001B[A\n",
      "episode_reward_mean = -4.109395503997803:  12%|█▏        | 7/60 [00:12<01:29,  1.68s/it]\u001B[A\n",
      "episode_reward_mean = -1.1521426439285278:  13%|█▎        | 8/60 [00:13<01:26,  1.66s/it]\u001B[A\n",
      "episode_reward_mean = -1.9453825950622559:  15%|█▌        | 9/60 [00:15<01:24,  1.65s/it]\u001B[A\n",
      "episode_reward_mean = -0.8924632668495178:  17%|█▋        | 10/60 [00:16<01:22,  1.64s/it]\u001B[A\n",
      "episode_reward_mean = 1.6333075761795044:  18%|█▊        | 11/60 [00:18<01:20,  1.64s/it] \u001B[A\n",
      "episode_reward_mean = 7.702722549438477:  20%|██        | 12/60 [00:20<01:18,  1.64s/it] \u001B[A\n",
      "episode_reward_mean = 9.581941604614258:  22%|██▏       | 13/60 [00:21<01:16,  1.64s/it]\u001B[A\n",
      "episode_reward_mean = 14.850412368774414:  23%|██▎       | 14/60 [00:23<01:15,  1.63s/it]\u001B[A\n",
      "episode_reward_mean = 18.663969039916992:  25%|██▌       | 15/60 [00:25<01:14,  1.65s/it]\u001B[A\n",
      "episode_reward_mean = 32.313621520996094:  27%|██▋       | 16/60 [00:26<01:12,  1.65s/it]\u001B[A\n",
      "episode_reward_mean = 31.76364517211914:  28%|██▊       | 17/60 [00:28<01:10,  1.64s/it] \u001B[A\n",
      "episode_reward_mean = 33.617645263671875:  30%|███       | 18/60 [00:29<01:08,  1.63s/it]\u001B[A\n",
      "episode_reward_mean = 42.938438415527344:  32%|███▏      | 19/60 [00:31<01:06,  1.62s/it]\u001B[A\n",
      "episode_reward_mean = 44.72184753417969:  33%|███▎      | 20/60 [00:33<01:04,  1.61s/it] \u001B[A\n",
      "episode_reward_mean = 35.32384490966797:  35%|███▌      | 21/60 [00:34<01:02,  1.61s/it]\u001B[A\n",
      "episode_reward_mean = 43.511348724365234:  37%|███▋      | 22/60 [00:36<01:01,  1.62s/it]\u001B[A\n",
      "episode_reward_mean = 53.68043899536133:  38%|███▊      | 23/60 [00:38<01:00,  1.63s/it] \u001B[A\n",
      "episode_reward_mean = 58.93443298339844:  40%|████      | 24/60 [00:39<00:58,  1.63s/it]\u001B[A\n",
      "episode_reward_mean = 54.169960021972656:  42%|████▏     | 25/60 [00:41<00:57,  1.64s/it]\u001B[A\n",
      "episode_reward_mean = 55.942161560058594:  43%|████▎     | 26/60 [00:43<00:56,  1.65s/it]\u001B[A\n",
      "episode_reward_mean = 61.70270919799805:  45%|████▌     | 27/60 [00:44<00:54,  1.64s/it] \u001B[A\n",
      "episode_reward_mean = 71.42106628417969:  47%|████▋     | 28/60 [00:46<00:52,  1.63s/it]\u001B[A\n",
      "episode_reward_mean = 72.1534423828125:  48%|████▊     | 29/60 [00:47<00:50,  1.63s/it] \u001B[A\n",
      "episode_reward_mean = 78.49398803710938:  50%|█████     | 30/60 [00:49<00:48,  1.62s/it]\u001B[A\n",
      "episode_reward_mean = 76.99620819091797:  52%|█████▏    | 31/60 [00:51<00:47,  1.63s/it]\u001B[A\n",
      "episode_reward_mean = 80.76785278320312:  53%|█████▎    | 32/60 [00:52<00:45,  1.64s/it]\u001B[A\n",
      "episode_reward_mean = 85.23188781738281:  55%|█████▌    | 33/60 [00:54<00:44,  1.64s/it]\u001B[A\n",
      "episode_reward_mean = 87.0326156616211:  57%|█████▋    | 34/60 [00:56<00:42,  1.65s/it] \u001B[A\n",
      "episode_reward_mean = 79.22256469726562:  58%|█████▊    | 35/60 [00:57<00:41,  1.65s/it]\u001B[A\n",
      "episode_reward_mean = 80.06181335449219:  60%|██████    | 36/60 [00:59<00:39,  1.65s/it]\u001B[A\n",
      "episode_reward_mean = 78.8393325805664:  62%|██████▏   | 37/60 [01:01<00:37,  1.64s/it] \u001B[A\n",
      "episode_reward_mean = 90.27216339111328:  63%|██████▎   | 38/60 [01:02<00:36,  1.64s/it]\u001B[A\n",
      "episode_reward_mean = 91.62459564208984:  65%|██████▌   | 39/60 [01:04<00:34,  1.64s/it]\u001B[A\n",
      "episode_reward_mean = 88.14530181884766:  67%|██████▋   | 40/60 [01:05<00:32,  1.65s/it]\u001B[A\n",
      "episode_reward_mean = 92.16136169433594:  68%|██████▊   | 41/60 [01:07<00:31,  1.65s/it]\u001B[A\n",
      "episode_reward_mean = 84.5028305053711:  70%|███████   | 42/60 [01:09<00:29,  1.64s/it] \u001B[A\n",
      "episode_reward_mean = 87.42626190185547:  72%|███████▏  | 43/60 [01:10<00:27,  1.64s/it]\u001B[A\n",
      "episode_reward_mean = 82.48924255371094:  73%|███████▎  | 44/60 [01:12<00:26,  1.65s/it]\u001B[A\n",
      "episode_reward_mean = 87.22644805908203:  75%|███████▌  | 45/60 [01:14<00:24,  1.64s/it]\u001B[A\n",
      "episode_reward_mean = 81.8969497680664:  77%|███████▋  | 46/60 [01:15<00:23,  1.65s/it] \u001B[A\n",
      "episode_reward_mean = 91.72028350830078:  78%|███████▊  | 47/60 [01:17<00:21,  1.65s/it]\u001B[A\n",
      "episode_reward_mean = 89.7980728149414:  80%|████████  | 48/60 [01:19<00:19,  1.65s/it] \u001B[A\n",
      "episode_reward_mean = 101.61454772949219:  82%|████████▏ | 49/60 [01:20<00:18,  1.65s/it]\u001B[A\n",
      "episode_reward_mean = 87.87593078613281:  83%|████████▎ | 50/60 [01:22<00:16,  1.65s/it] \u001B[A\n",
      "episode_reward_mean = 94.42874145507812:  85%|████████▌ | 51/60 [01:24<00:15,  1.67s/it]\u001B[A\n",
      "episode_reward_mean = 89.69570922851562:  87%|████████▋ | 52/60 [01:25<00:13,  1.67s/it]\u001B[A\n",
      "episode_reward_mean = 98.19944763183594:  88%|████████▊ | 53/60 [01:27<00:11,  1.67s/it]\u001B[A\n",
      "episode_reward_mean = 101.25125122070312:  90%|█████████ | 54/60 [01:29<00:09,  1.65s/it]\u001B[A\n",
      "episode_reward_mean = 98.72077941894531:  92%|█████████▏| 55/60 [01:30<00:08,  1.65s/it] \u001B[A\n",
      "episode_reward_mean = 89.61692810058594:  93%|█████████▎| 56/60 [01:32<00:06,  1.65s/it]\u001B[A\n",
      "episode_reward_mean = 90.7981948852539:  95%|█████████▌| 57/60 [01:34<00:04,  1.65s/it] \u001B[A\n",
      "episode_reward_mean = 94.18428039550781:  97%|█████████▋| 58/60 [01:35<00:03,  1.66s/it]\u001B[A\n",
      "episode_reward_mean = 87.37586212158203:  98%|█████████▊| 59/60 [01:37<00:01,  1.66s/it]\u001B[A\n",
      "episode_reward_mean = 81.96728515625: 100%|██████████| 60/60 [01:39<00:00,  1.65s/it]   \u001B[A\n",
      "2025-08-20 20:10:50,589 [torchrl][INFO] Training time: 56.93 seconds\n",
      "2025-08-20 20:10:50,594 [torchrl][INFO] macs: 11.62 MMac  Params: 10.37 k\n",
      "2025-08-20 20:10:51,398 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:10:51,431 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -11.054300308227539:   2%|▏         | 1/60 [00:01<01:39,  1.69s/it]\u001B[A\n",
      "episode_reward_mean = -5.944056034088135:   3%|▎         | 2/60 [00:03<01:41,  1.74s/it] \u001B[A\n",
      "episode_reward_mean = -4.386639595031738:   5%|▌         | 3/60 [00:05<01:38,  1.72s/it]\u001B[A\n",
      "episode_reward_mean = -3.6795716285705566:   7%|▋         | 4/60 [00:06<01:35,  1.70s/it]\u001B[A\n",
      "episode_reward_mean = -3.918520212173462:   8%|▊         | 5/60 [00:08<01:33,  1.70s/it] \u001B[A\n",
      "episode_reward_mean = -5.7284464836120605:  10%|█         | 6/60 [00:10<01:31,  1.69s/it]\u001B[A\n",
      "episode_reward_mean = -3.42891263961792:  12%|█▏        | 7/60 [00:11<01:29,  1.69s/it]  \u001B[A\n",
      "episode_reward_mean = 0.0077043804340064526:  13%|█▎        | 8/60 [00:13<01:28,  1.69s/it]\u001B[A\n",
      "episode_reward_mean = -2.88665771484375:  15%|█▌        | 9/60 [00:15<01:26,  1.69s/it]    \u001B[A\n",
      "episode_reward_mean = 0.7106437087059021:  17%|█▋        | 10/60 [00:16<01:24,  1.69s/it]\u001B[A\n",
      "episode_reward_mean = 5.078415393829346:  18%|█▊        | 11/60 [00:18<01:22,  1.68s/it] \u001B[A\n",
      "episode_reward_mean = 4.459517955780029:  20%|██        | 12/60 [00:20<01:20,  1.67s/it]\u001B[A\n",
      "episode_reward_mean = 9.835956573486328:  22%|██▏       | 13/60 [00:21<01:18,  1.68s/it]\u001B[A\n",
      "episode_reward_mean = 14.265141487121582:  23%|██▎       | 14/60 [00:23<01:18,  1.70s/it]\u001B[A\n",
      "episode_reward_mean = 22.358057022094727:  25%|██▌       | 15/60 [00:25<01:16,  1.70s/it]\u001B[A\n",
      "episode_reward_mean = 20.530000686645508:  27%|██▋       | 16/60 [00:27<01:14,  1.69s/it]\u001B[A\n",
      "episode_reward_mean = 24.35661506652832:  28%|██▊       | 17/60 [00:28<01:12,  1.68s/it] \u001B[A\n",
      "episode_reward_mean = 23.53805160522461:  30%|███       | 18/60 [00:30<01:10,  1.68s/it]\u001B[A\n",
      "episode_reward_mean = 16.14837646484375:  32%|███▏      | 19/60 [00:32<01:09,  1.69s/it]\u001B[A\n",
      "episode_reward_mean = 20.68949317932129:  33%|███▎      | 20/60 [00:33<01:07,  1.68s/it]\u001B[A\n",
      "episode_reward_mean = 19.99191665649414:  35%|███▌      | 21/60 [00:35<01:05,  1.68s/it]\u001B[A\n",
      "episode_reward_mean = 25.329486846923828:  37%|███▋      | 22/60 [00:37<01:04,  1.69s/it]\u001B[A\n",
      "episode_reward_mean = 34.2035026550293:  38%|███▊      | 23/60 [00:38<01:02,  1.69s/it]  \u001B[A\n",
      "episode_reward_mean = 33.68109893798828:  40%|████      | 24/60 [00:40<01:00,  1.69s/it]\u001B[A\n",
      "episode_reward_mean = 27.459611892700195:  42%|████▏     | 25/60 [00:42<00:59,  1.69s/it]\u001B[A\n",
      "episode_reward_mean = 34.01673126220703:  43%|████▎     | 26/60 [00:43<00:57,  1.69s/it] \u001B[A\n",
      "episode_reward_mean = 31.78656005859375:  45%|████▌     | 27/60 [00:45<00:55,  1.69s/it]\u001B[A\n",
      "episode_reward_mean = 34.83943176269531:  47%|████▋     | 28/60 [00:47<00:54,  1.69s/it]\u001B[A\n",
      "episode_reward_mean = 39.048736572265625:  48%|████▊     | 29/60 [00:49<00:52,  1.70s/it]\u001B[A\n",
      "episode_reward_mean = 30.98133087158203:  50%|█████     | 30/60 [00:50<00:51,  1.70s/it] \u001B[A\n",
      "episode_reward_mean = 45.961036682128906:  52%|█████▏    | 31/60 [00:52<00:49,  1.72s/it]\u001B[A\n",
      "episode_reward_mean = 43.821590423583984:  53%|█████▎    | 32/60 [00:54<00:48,  1.72s/it]\u001B[A\n",
      "episode_reward_mean = 42.836936950683594:  55%|█████▌    | 33/60 [00:55<00:46,  1.71s/it]\u001B[A\n",
      "episode_reward_mean = 42.76924133300781:  57%|█████▋    | 34/60 [00:57<00:44,  1.70s/it] \u001B[A\n",
      "episode_reward_mean = 55.91565704345703:  58%|█████▊    | 35/60 [00:59<00:42,  1.70s/it]\u001B[A\n",
      "episode_reward_mean = 58.70785903930664:  60%|██████    | 36/60 [01:01<00:41,  1.72s/it]\u001B[A\n",
      "episode_reward_mean = 66.16551971435547:  62%|██████▏   | 37/60 [01:02<00:39,  1.71s/it]\u001B[A\n",
      "episode_reward_mean = 65.12644958496094:  63%|██████▎   | 38/60 [01:04<00:37,  1.70s/it]\u001B[A\n",
      "episode_reward_mean = 74.79179382324219:  65%|██████▌   | 39/60 [01:06<00:35,  1.70s/it]\u001B[A\n",
      "episode_reward_mean = 70.15492248535156:  67%|██████▋   | 40/60 [01:07<00:34,  1.70s/it]\u001B[A\n",
      "episode_reward_mean = 68.5571060180664:  68%|██████▊   | 41/60 [01:09<00:32,  1.70s/it] \u001B[A\n",
      "episode_reward_mean = 67.04293823242188:  70%|███████   | 42/60 [01:11<00:30,  1.70s/it]\u001B[A\n",
      "episode_reward_mean = 70.90150451660156:  72%|███████▏  | 43/60 [01:12<00:28,  1.69s/it]\u001B[A\n",
      "episode_reward_mean = 75.38634490966797:  73%|███████▎  | 44/60 [01:14<00:27,  1.69s/it]\u001B[A\n",
      "episode_reward_mean = 82.425537109375:  75%|███████▌  | 45/60 [01:16<00:25,  1.70s/it]  \u001B[A\n",
      "episode_reward_mean = 87.0561752319336:  77%|███████▋  | 46/60 [01:18<00:23,  1.71s/it]\u001B[A\n",
      "episode_reward_mean = 77.05516815185547:  78%|███████▊  | 47/60 [01:19<00:22,  1.70s/it]\u001B[A\n",
      "episode_reward_mean = 82.30750274658203:  80%|████████  | 48/60 [01:21<00:20,  1.70s/it]\u001B[A\n",
      "episode_reward_mean = 89.868896484375:  82%|████████▏ | 49/60 [01:23<00:18,  1.71s/it]  \u001B[A\n",
      "episode_reward_mean = 89.89478302001953:  83%|████████▎ | 50/60 [01:24<00:17,  1.72s/it]\u001B[A\n",
      "episode_reward_mean = 89.42758178710938:  85%|████████▌ | 51/60 [01:26<00:15,  1.71s/it]\u001B[A\n",
      "episode_reward_mean = 81.07073211669922:  87%|████████▋ | 52/60 [01:28<00:13,  1.71s/it]\u001B[A\n",
      "episode_reward_mean = 100.48981475830078:  88%|████████▊ | 53/60 [01:30<00:11,  1.71s/it]\u001B[A\n",
      "episode_reward_mean = 104.89871215820312:  90%|█████████ | 54/60 [01:31<00:10,  1.71s/it]\u001B[A\n",
      "episode_reward_mean = 89.86400604248047:  92%|█████████▏| 55/60 [01:33<00:08,  1.74s/it] \u001B[A\n",
      "episode_reward_mean = 96.35346984863281:  93%|█████████▎| 56/60 [01:35<00:06,  1.74s/it]\u001B[A\n",
      "episode_reward_mean = 86.78873443603516:  95%|█████████▌| 57/60 [01:36<00:05,  1.73s/it]\u001B[A\n",
      "episode_reward_mean = 89.44931030273438:  97%|█████████▋| 58/60 [01:38<00:03,  1.73s/it]\u001B[A\n",
      "episode_reward_mean = 92.16746520996094:  98%|█████████▊| 59/60 [01:40<00:01,  1.73s/it]\u001B[A\n",
      "episode_reward_mean = 97.1611557006836: 100%|██████████| 60/60 [01:42<00:00,  1.70s/it] \u001B[A\n",
      "2025-08-20 20:12:33,614 [torchrl][INFO] Training time: 57.61 seconds\n",
      "2025-08-20 20:12:33,620 [torchrl][INFO] macs: 11.62 MMac  Params: 10.37 k\n",
      "2025-08-20 20:12:34,520 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:12:34,554 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -10.898261070251465:   2%|▏         | 1/60 [00:01<01:42,  1.74s/it]\u001B[A\n",
      "episode_reward_mean = -9.639571189880371:   3%|▎         | 2/60 [00:03<01:40,  1.74s/it] \u001B[A\n",
      "episode_reward_mean = -9.212427139282227:   5%|▌         | 3/60 [00:05<01:38,  1.73s/it]\u001B[A\n",
      "episode_reward_mean = -9.956984519958496:   7%|▋         | 4/60 [00:06<01:37,  1.74s/it]\u001B[A\n",
      "episode_reward_mean = -6.843161582946777:   8%|▊         | 5/60 [00:08<01:35,  1.74s/it]\u001B[A\n",
      "episode_reward_mean = -7.290886878967285:  10%|█         | 6/60 [00:10<01:36,  1.79s/it]\u001B[A\n",
      "episode_reward_mean = -7.411915302276611:  12%|█▏        | 7/60 [00:12<01:38,  1.86s/it]\u001B[A\n",
      "episode_reward_mean = -3.722925901412964:  13%|█▎        | 8/60 [00:14<01:34,  1.83s/it]\u001B[A\n",
      "episode_reward_mean = -4.251438140869141:  15%|█▌        | 9/60 [00:16<01:31,  1.79s/it]\u001B[A\n",
      "episode_reward_mean = -1.3925743103027344:  17%|█▋        | 10/60 [00:17<01:29,  1.78s/it]\u001B[A\n",
      "episode_reward_mean = 4.176548957824707:  18%|█▊        | 11/60 [00:19<01:26,  1.77s/it]  \u001B[A\n",
      "episode_reward_mean = 8.654861450195312:  20%|██        | 12/60 [00:21<01:24,  1.77s/it]\u001B[A\n",
      "episode_reward_mean = 19.122337341308594:  22%|██▏       | 13/60 [00:23<01:22,  1.76s/it]\u001B[A\n",
      "episode_reward_mean = 25.286582946777344:  23%|██▎       | 14/60 [00:24<01:20,  1.75s/it]\u001B[A\n",
      "episode_reward_mean = 22.59224510192871:  25%|██▌       | 15/60 [00:26<01:18,  1.75s/it] \u001B[A\n",
      "episode_reward_mean = 30.573909759521484:  27%|██▋       | 16/60 [00:28<01:18,  1.78s/it]\u001B[A\n",
      "episode_reward_mean = 28.474388122558594:  28%|██▊       | 17/60 [00:30<01:16,  1.77s/it]\u001B[A\n",
      "episode_reward_mean = 38.82630157470703:  30%|███       | 18/60 [00:32<01:20,  1.91s/it] \u001B[A\n",
      "episode_reward_mean = 41.28593063354492:  32%|███▏      | 19/60 [00:34<01:20,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 44.69017028808594:  33%|███▎      | 20/60 [00:36<01:15,  1.90s/it]\u001B[A\n",
      "episode_reward_mean = 45.78254699707031:  35%|███▌      | 21/60 [00:37<01:11,  1.84s/it]\u001B[A\n",
      "episode_reward_mean = 51.702659606933594:  37%|███▋      | 22/60 [00:39<01:09,  1.83s/it]\u001B[A\n",
      "episode_reward_mean = 51.71691131591797:  38%|███▊      | 23/60 [00:41<01:06,  1.79s/it] \u001B[A\n",
      "episode_reward_mean = 55.70500564575195:  40%|████      | 24/60 [00:43<01:03,  1.77s/it]\u001B[A\n",
      "episode_reward_mean = 56.30260467529297:  42%|████▏     | 25/60 [00:45<01:03,  1.82s/it]\u001B[A\n",
      "episode_reward_mean = 62.558494567871094:  43%|████▎     | 26/60 [00:47<01:06,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 61.17803192138672:  45%|████▌     | 27/60 [00:49<01:02,  1.89s/it] \u001B[A\n",
      "episode_reward_mean = 53.89869689941406:  47%|████▋     | 28/60 [00:50<00:59,  1.84s/it]\u001B[A\n",
      "episode_reward_mean = 56.61077880859375:  48%|████▊     | 29/60 [00:52<00:56,  1.82s/it]\u001B[A\n",
      "episode_reward_mean = 62.504661560058594:  50%|█████     | 30/60 [00:54<00:53,  1.80s/it]\u001B[A\n",
      "episode_reward_mean = 68.22001647949219:  52%|█████▏    | 31/60 [00:56<00:51,  1.78s/it] \u001B[A\n",
      "episode_reward_mean = 71.3949203491211:  53%|█████▎    | 32/60 [00:57<00:49,  1.78s/it] \u001B[A\n",
      "episode_reward_mean = 75.24662780761719:  55%|█████▌    | 33/60 [00:59<00:47,  1.77s/it]\u001B[A\n",
      "episode_reward_mean = 79.16458129882812:  57%|█████▋    | 34/60 [01:01<00:45,  1.76s/it]\u001B[A\n",
      "episode_reward_mean = 78.46888732910156:  58%|█████▊    | 35/60 [01:03<00:43,  1.76s/it]\u001B[A\n",
      "episode_reward_mean = 81.40056610107422:  60%|██████    | 36/60 [01:04<00:42,  1.76s/it]\u001B[A\n",
      "episode_reward_mean = 79.47425079345703:  62%|██████▏   | 37/60 [01:06<00:40,  1.75s/it]\u001B[A\n",
      "episode_reward_mean = 76.70037841796875:  63%|██████▎   | 38/60 [01:08<00:38,  1.75s/it]\u001B[A\n",
      "episode_reward_mean = 80.12069702148438:  65%|██████▌   | 39/60 [01:10<00:37,  1.77s/it]\u001B[A\n",
      "episode_reward_mean = 87.76565551757812:  67%|██████▋   | 40/60 [01:11<00:35,  1.77s/it]\u001B[A\n",
      "episode_reward_mean = 77.7677230834961:  68%|██████▊   | 41/60 [01:13<00:33,  1.76s/it] \u001B[A\n",
      "episode_reward_mean = 81.01828002929688:  70%|███████   | 42/60 [01:15<00:31,  1.76s/it]\u001B[A\n",
      "episode_reward_mean = 76.53446960449219:  72%|███████▏  | 43/60 [01:17<00:30,  1.77s/it]\u001B[A\n",
      "episode_reward_mean = 80.68455505371094:  73%|███████▎  | 44/60 [01:19<00:28,  1.79s/it]\u001B[A\n",
      "episode_reward_mean = 78.49751281738281:  75%|███████▌  | 45/60 [01:20<00:26,  1.78s/it]\u001B[A\n",
      "episode_reward_mean = 83.82516479492188:  77%|███████▋  | 46/60 [01:22<00:24,  1.78s/it]\u001B[A\n",
      "episode_reward_mean = 84.31523132324219:  78%|███████▊  | 47/60 [01:24<00:22,  1.77s/it]\u001B[A\n",
      "episode_reward_mean = 84.433837890625:  80%|████████  | 48/60 [01:26<00:21,  1.76s/it]  \u001B[A\n",
      "episode_reward_mean = 82.34429168701172:  82%|████████▏ | 49/60 [01:27<00:19,  1.76s/it]\u001B[A\n",
      "episode_reward_mean = 74.61831665039062:  83%|████████▎ | 50/60 [01:29<00:17,  1.76s/it]\u001B[A\n",
      "episode_reward_mean = 95.05648040771484:  85%|████████▌ | 51/60 [01:31<00:15,  1.76s/it]\u001B[A\n",
      "episode_reward_mean = 84.32289123535156:  87%|████████▋ | 52/60 [01:33<00:14,  1.76s/it]\u001B[A\n",
      "episode_reward_mean = 97.84902954101562:  88%|████████▊ | 53/60 [01:34<00:12,  1.75s/it]\u001B[A\n",
      "episode_reward_mean = 85.87702941894531:  90%|█████████ | 54/60 [01:36<00:10,  1.75s/it]\u001B[A\n",
      "episode_reward_mean = 81.71233367919922:  92%|█████████▏| 55/60 [01:38<00:08,  1.75s/it]\u001B[A\n",
      "episode_reward_mean = 84.61601257324219:  93%|█████████▎| 56/60 [01:40<00:06,  1.75s/it]\u001B[A\n",
      "episode_reward_mean = 95.25917053222656:  95%|█████████▌| 57/60 [01:41<00:05,  1.75s/it]\u001B[A\n",
      "episode_reward_mean = 97.05370330810547:  97%|█████████▋| 58/60 [01:43<00:03,  1.75s/it]\u001B[A\n",
      "episode_reward_mean = 85.9438247680664:  98%|█████████▊| 59/60 [01:45<00:01,  1.82s/it] \u001B[A\n",
      "episode_reward_mean = 88.25228118896484: 100%|██████████| 60/60 [01:47<00:00,  1.79s/it]\u001B[A\n",
      "2025-08-20 20:14:22,076 [torchrl][INFO] Training time: 61.15 seconds\n",
      "2025-08-20 20:14:22,080 [torchrl][INFO] macs: 11.62 MMac  Params: 10.37 k\n",
      "2025-08-20 20:14:23,001 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:14:23,036 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -6.517302989959717:   2%|▏         | 1/60 [00:01<01:41,  1.72s/it]\u001B[A\n",
      "episode_reward_mean = -7.908013820648193:   3%|▎         | 2/60 [00:03<01:40,  1.73s/it]\u001B[A\n",
      "episode_reward_mean = -7.009063243865967:   5%|▌         | 3/60 [00:05<01:38,  1.72s/it]\u001B[A\n",
      "episode_reward_mean = -5.457328796386719:   7%|▋         | 4/60 [00:06<01:36,  1.72s/it]\u001B[A\n",
      "episode_reward_mean = -5.599403381347656:   8%|▊         | 5/60 [00:08<01:36,  1.75s/it]\u001B[A\n",
      "episode_reward_mean = -9.14654541015625:  10%|█         | 6/60 [00:10<01:34,  1.75s/it] \u001B[A\n",
      "episode_reward_mean = -6.968518257141113:  12%|█▏        | 7/60 [00:12<01:32,  1.75s/it]\u001B[A\n",
      "episode_reward_mean = -7.534301280975342:  13%|█▎        | 8/60 [00:13<01:30,  1.75s/it]\u001B[A\n",
      "episode_reward_mean = -6.349761009216309:  15%|█▌        | 9/60 [00:15<01:28,  1.74s/it]\u001B[A\n",
      "episode_reward_mean = -3.7008907794952393:  17%|█▋        | 10/60 [00:17<01:26,  1.74s/it]\u001B[A\n",
      "episode_reward_mean = 2.1171000003814697:  18%|█▊        | 11/60 [00:19<01:25,  1.74s/it] \u001B[A\n",
      "episode_reward_mean = -0.6328975558280945:  20%|██        | 12/60 [00:20<01:23,  1.74s/it]\u001B[A\n",
      "episode_reward_mean = 0.9405401945114136:  22%|██▏       | 13/60 [00:22<01:21,  1.74s/it] \u001B[A\n",
      "episode_reward_mean = 7.480544567108154:  23%|██▎       | 14/60 [00:24<01:19,  1.74s/it] \u001B[A\n",
      "episode_reward_mean = 9.320516586303711:  25%|██▌       | 15/60 [00:26<01:18,  1.74s/it]\u001B[A\n",
      "episode_reward_mean = 15.780035018920898:  27%|██▋       | 16/60 [00:27<01:16,  1.74s/it]\u001B[A\n",
      "episode_reward_mean = 15.846735000610352:  28%|██▊       | 17/60 [00:29<01:14,  1.74s/it]\u001B[A\n",
      "episode_reward_mean = 17.430824279785156:  30%|███       | 18/60 [00:31<01:12,  1.73s/it]\u001B[A\n",
      "episode_reward_mean = 28.304933547973633:  32%|███▏      | 19/60 [00:33<01:11,  1.73s/it]\u001B[A\n",
      "episode_reward_mean = 19.21086883544922:  33%|███▎      | 20/60 [00:34<01:09,  1.73s/it] \u001B[A\n",
      "episode_reward_mean = 23.91305160522461:  35%|███▌      | 21/60 [00:36<01:07,  1.74s/it]\u001B[A\n",
      "episode_reward_mean = 28.7825927734375:  37%|███▋      | 22/60 [00:38<01:06,  1.74s/it] \u001B[A\n",
      "episode_reward_mean = 32.246028900146484:  38%|███▊      | 23/60 [00:40<01:05,  1.76s/it]\u001B[A\n",
      "episode_reward_mean = 33.353126525878906:  40%|████      | 24/60 [00:41<01:03,  1.78s/it]\u001B[A\n",
      "episode_reward_mean = 28.86565399169922:  42%|████▏     | 25/60 [00:43<01:01,  1.76s/it] \u001B[A\n",
      "episode_reward_mean = 35.06970977783203:  43%|████▎     | 26/60 [00:45<01:04,  1.89s/it]\u001B[A\n",
      "episode_reward_mean = 30.63027000427246:  45%|████▌     | 27/60 [00:49<01:18,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 25.138994216918945:  47%|████▋     | 28/60 [00:52<01:22,  2.57s/it]\u001B[A\n",
      "episode_reward_mean = 21.716642379760742:  48%|████▊     | 29/60 [00:54<01:13,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 34.971778869628906:  50%|█████     | 30/60 [00:56<01:06,  2.21s/it]\u001B[A\n",
      "episode_reward_mean = 35.5575065612793:  52%|█████▏    | 31/60 [00:57<00:59,  2.06s/it]  \u001B[A\n",
      "episode_reward_mean = 34.586124420166016:  53%|█████▎    | 32/60 [00:59<00:55,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 43.08802795410156:  55%|█████▌    | 33/60 [01:01<00:51,  1.90s/it] \u001B[A\n",
      "episode_reward_mean = 41.34980773925781:  57%|█████▋    | 34/60 [01:02<00:47,  1.84s/it]\u001B[A\n",
      "episode_reward_mean = 44.96638488769531:  58%|█████▊    | 35/60 [01:04<00:45,  1.81s/it]\u001B[A\n",
      "episode_reward_mean = 45.01488494873047:  60%|██████    | 36/60 [01:06<00:42,  1.79s/it]\u001B[A\n",
      "episode_reward_mean = 47.6151123046875:  62%|██████▏   | 37/60 [01:08<00:40,  1.78s/it] \u001B[A\n",
      "episode_reward_mean = 55.73170471191406:  63%|██████▎   | 38/60 [01:09<00:39,  1.78s/it]\u001B[A\n",
      "episode_reward_mean = 55.399044036865234:  65%|██████▌   | 39/60 [01:11<00:37,  1.77s/it]\u001B[A\n",
      "episode_reward_mean = 62.70271301269531:  67%|██████▋   | 40/60 [01:13<00:35,  1.77s/it] \u001B[A\n",
      "episode_reward_mean = 59.572975158691406:  68%|██████▊   | 41/60 [01:15<00:33,  1.77s/it]\u001B[A\n",
      "episode_reward_mean = 68.52796173095703:  70%|███████   | 42/60 [01:17<00:31,  1.77s/it] \u001B[A\n",
      "episode_reward_mean = 61.845924377441406:  72%|███████▏  | 43/60 [01:18<00:30,  1.78s/it]\u001B[A\n",
      "episode_reward_mean = 61.605369567871094:  73%|███████▎  | 44/60 [01:20<00:28,  1.79s/it]\u001B[A\n",
      "episode_reward_mean = 74.46472930908203:  75%|███████▌  | 45/60 [01:22<00:26,  1.78s/it] \u001B[A\n",
      "episode_reward_mean = 76.82588958740234:  77%|███████▋  | 46/60 [01:24<00:25,  1.81s/it]\u001B[A\n",
      "episode_reward_mean = 82.07601928710938:  78%|███████▊  | 47/60 [01:26<00:23,  1.80s/it]\u001B[A\n",
      "episode_reward_mean = 68.3360595703125:  80%|████████  | 48/60 [01:27<00:21,  1.79s/it] \u001B[A\n",
      "episode_reward_mean = 74.38626098632812:  82%|████████▏ | 49/60 [01:29<00:19,  1.79s/it]\u001B[A\n",
      "episode_reward_mean = 72.66915130615234:  83%|████████▎ | 50/60 [01:31<00:17,  1.79s/it]\u001B[A\n",
      "episode_reward_mean = 83.16575622558594:  85%|████████▌ | 51/60 [01:33<00:16,  1.79s/it]\u001B[A\n",
      "episode_reward_mean = 85.4941635131836:  87%|████████▋ | 52/60 [01:35<00:14,  1.79s/it] \u001B[A\n",
      "episode_reward_mean = 86.71859741210938:  88%|████████▊ | 53/60 [01:36<00:12,  1.81s/it]\u001B[A\n",
      "episode_reward_mean = 94.23934936523438:  90%|█████████ | 54/60 [01:38<00:10,  1.80s/it]\u001B[A\n",
      "episode_reward_mean = 82.89276123046875:  92%|█████████▏| 55/60 [01:40<00:08,  1.80s/it]\u001B[A\n",
      "episode_reward_mean = 89.19901275634766:  93%|█████████▎| 56/60 [01:42<00:07,  1.80s/it]\u001B[A\n",
      "episode_reward_mean = 89.8737564086914:  95%|█████████▌| 57/60 [01:44<00:05,  1.80s/it] \u001B[A\n",
      "episode_reward_mean = 95.92906188964844:  97%|█████████▋| 58/60 [01:45<00:03,  1.81s/it]\u001B[A\n",
      "episode_reward_mean = 90.88401794433594:  98%|█████████▊| 59/60 [01:47<00:01,  1.81s/it]\u001B[A\n",
      "episode_reward_mean = 93.58654022216797: 100%|██████████| 60/60 [01:49<00:00,  1.82s/it]\u001B[A\n",
      "2025-08-20 20:16:12,514 [torchrl][INFO] Training time: 61.57 seconds\n",
      "2025-08-20 20:16:12,520 [torchrl][INFO] macs: 11.62 MMac  Params: 10.37 k\n",
      "2025-08-20 20:16:13,396 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:16:13,433 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -11.10625171661377:   2%|▏         | 1/60 [00:01<01:48,  1.83s/it]\u001B[A\n",
      "episode_reward_mean = -11.750258445739746:   3%|▎         | 2/60 [00:03<01:47,  1.86s/it]\u001B[A\n",
      "episode_reward_mean = -11.910469055175781:   5%|▌         | 3/60 [00:05<01:45,  1.86s/it]\u001B[A\n",
      "episode_reward_mean = -10.1957426071167:   7%|▋         | 4/60 [00:07<01:44,  1.86s/it]  \u001B[A\n",
      "episode_reward_mean = -8.23040771484375:   8%|▊         | 5/60 [00:09<01:42,  1.86s/it]\u001B[A\n",
      "episode_reward_mean = -8.611601829528809:  10%|█         | 6/60 [00:11<01:40,  1.86s/it]\u001B[A\n",
      "episode_reward_mean = -7.901339054107666:  12%|█▏        | 7/60 [00:13<01:40,  1.89s/it]\u001B[A\n",
      "episode_reward_mean = -7.925832271575928:  13%|█▎        | 8/60 [00:14<01:37,  1.88s/it]\u001B[A\n",
      "episode_reward_mean = -4.621575832366943:  15%|█▌        | 9/60 [00:16<01:35,  1.88s/it]\u001B[A\n",
      "episode_reward_mean = -4.011432647705078:  17%|█▋        | 10/60 [00:18<01:33,  1.88s/it]\u001B[A\n",
      "episode_reward_mean = -0.7773578763008118:  18%|█▊        | 11/60 [00:20<01:32,  1.88s/it]\u001B[A\n",
      "episode_reward_mean = 0.3535165786743164:  20%|██        | 12/60 [00:22<01:30,  1.88s/it] \u001B[A\n",
      "episode_reward_mean = 5.55348014831543:  22%|██▏       | 13/60 [00:24<01:28,  1.89s/it]  \u001B[A\n",
      "episode_reward_mean = 9.448793411254883:  23%|██▎       | 14/60 [00:26<01:27,  1.89s/it]\u001B[A\n",
      "episode_reward_mean = 10.285154342651367:  25%|██▌       | 15/60 [00:28<01:24,  1.88s/it]\u001B[A\n",
      "episode_reward_mean = 19.626445770263672:  27%|██▋       | 16/60 [00:30<01:23,  1.91s/it]\u001B[A\n",
      "episode_reward_mean = 21.125913619995117:  28%|██▊       | 17/60 [00:31<01:21,  1.90s/it]\u001B[A\n",
      "episode_reward_mean = 21.70955467224121:  30%|███       | 18/60 [00:33<01:19,  1.89s/it] \u001B[A\n",
      "episode_reward_mean = 18.248411178588867:  32%|███▏      | 19/60 [00:35<01:17,  1.88s/it]\u001B[A\n",
      "episode_reward_mean = 35.804046630859375:  33%|███▎      | 20/60 [00:37<01:15,  1.88s/it]\u001B[A\n",
      "episode_reward_mean = 26.909984588623047:  35%|███▌      | 21/60 [00:39<01:13,  1.89s/it]\u001B[A\n",
      "episode_reward_mean = 39.17704772949219:  37%|███▋      | 22/60 [00:41<01:12,  1.90s/it] \u001B[A\n",
      "episode_reward_mean = 34.75074768066406:  38%|███▊      | 23/60 [00:43<01:10,  1.90s/it]\u001B[A\n",
      "episode_reward_mean = 40.25725173950195:  40%|████      | 24/60 [00:45<01:08,  1.90s/it]\u001B[A\n",
      "episode_reward_mean = 45.39229965209961:  42%|████▏     | 25/60 [00:47<01:06,  1.90s/it]\u001B[A\n",
      "episode_reward_mean = 45.59473419189453:  43%|████▎     | 26/60 [00:49<01:04,  1.90s/it]\u001B[A\n",
      "episode_reward_mean = 43.60980987548828:  45%|████▌     | 27/60 [00:50<01:03,  1.91s/it]\u001B[A\n",
      "episode_reward_mean = 52.657859802246094:  47%|████▋     | 28/60 [00:52<01:01,  1.92s/it]\u001B[A\n",
      "episode_reward_mean = 54.10768127441406:  48%|████▊     | 29/60 [00:54<00:59,  1.92s/it] \u001B[A\n",
      "episode_reward_mean = 51.63998031616211:  50%|█████     | 30/60 [00:56<00:58,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 59.67946243286133:  52%|█████▏    | 31/60 [00:58<00:56,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 51.834571838378906:  53%|█████▎    | 32/60 [01:00<00:54,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 64.46562957763672:  55%|█████▌    | 33/60 [01:02<00:52,  1.94s/it] \u001B[A\n",
      "episode_reward_mean = 59.853607177734375:  57%|█████▋    | 34/60 [01:04<00:50,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 64.61406707763672:  58%|█████▊    | 35/60 [01:06<00:48,  1.94s/it] \u001B[A\n",
      "episode_reward_mean = 72.93621826171875:  60%|██████    | 36/60 [01:08<00:46,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 65.61712646484375:  62%|██████▏   | 37/60 [01:10<00:44,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 65.96099853515625:  63%|██████▎   | 38/60 [01:12<00:42,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 67.92449188232422:  65%|██████▌   | 39/60 [01:14<00:40,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 72.52166748046875:  67%|██████▋   | 40/60 [01:16<00:39,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 72.2845687866211:  68%|██████▊   | 41/60 [01:18<00:37,  1.96s/it] \u001B[A\n",
      "episode_reward_mean = 65.62653350830078:  70%|███████   | 42/60 [01:20<00:35,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 71.4601821899414:  72%|███████▏  | 43/60 [01:22<00:33,  1.95s/it] \u001B[A\n",
      "episode_reward_mean = 62.806854248046875:  73%|███████▎  | 44/60 [01:24<00:31,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 77.56452178955078:  75%|███████▌  | 45/60 [01:26<00:29,  1.96s/it] \u001B[A\n",
      "episode_reward_mean = 71.060302734375:  77%|███████▋  | 46/60 [01:28<00:27,  1.96s/it]  \u001B[A\n",
      "episode_reward_mean = 73.820556640625:  78%|███████▊  | 47/60 [01:30<00:25,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 79.37552642822266:  80%|████████  | 48/60 [01:31<00:23,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 74.63691711425781:  82%|████████▏ | 49/60 [01:33<00:21,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 72.5435562133789:  83%|████████▎ | 50/60 [01:35<00:19,  1.95s/it] \u001B[A\n",
      "episode_reward_mean = 76.23267364501953:  85%|████████▌ | 51/60 [01:37<00:17,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 73.01884460449219:  87%|████████▋ | 52/60 [01:39<00:15,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 80.1370620727539:  88%|████████▊ | 53/60 [01:41<00:13,  1.97s/it] \u001B[A\n",
      "episode_reward_mean = 82.36485290527344:  90%|█████████ | 54/60 [01:43<00:11,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 90.74036407470703:  92%|█████████▏| 55/60 [01:45<00:09,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 87.41255187988281:  93%|█████████▎| 56/60 [01:47<00:07,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 79.25914001464844:  95%|█████████▌| 57/60 [01:49<00:05,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 84.27730560302734:  97%|█████████▋| 58/60 [01:51<00:03,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 93.00206756591797:  98%|█████████▊| 59/60 [01:53<00:01,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 89.87177276611328: 100%|██████████| 60/60 [01:55<00:00,  1.93s/it]\u001B[A\n",
      "2025-08-20 20:18:08,980 [torchrl][INFO] Training time: 63.59 seconds\n",
      "2025-08-20 20:18:08,985 [torchrl][INFO] macs: 11.62 MMac  Params: 10.37 k\n",
      "2025-08-20 20:18:09,978 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:18:10,018 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -12.642582893371582:   2%|▏         | 1/60 [00:01<01:54,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = -11.957985877990723:   3%|▎         | 2/60 [00:03<01:52,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = -12.277290344238281:   5%|▌         | 3/60 [00:05<01:51,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = -12.40994644165039:   7%|▋         | 4/60 [00:07<01:51,  1.99s/it] \u001B[A\n",
      "episode_reward_mean = -11.142491340637207:   8%|▊         | 5/60 [00:09<01:48,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = -11.126306533813477:  10%|█         | 6/60 [00:11<01:45,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = -10.407939910888672:  12%|█▏        | 7/60 [00:13<01:43,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = -6.802412986755371:  13%|█▎        | 8/60 [00:15<01:40,  1.94s/it] \u001B[A\n",
      "episode_reward_mean = -6.491745471954346:  15%|█▌        | 9/60 [00:17<01:38,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = -6.085048675537109:  17%|█▋        | 10/60 [00:19<01:36,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = -6.013436317443848:  18%|█▊        | 11/60 [00:21<01:34,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = -6.250697135925293:  20%|██        | 12/60 [00:23<01:33,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = -4.385629177093506:  22%|██▏       | 13/60 [00:25<01:32,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = -3.331348419189453:  23%|██▎       | 14/60 [00:27<01:30,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 0.6157499551773071:  25%|██▌       | 15/60 [00:29<01:28,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 2.8742761611938477:  27%|██▋       | 16/60 [00:31<01:25,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 0.8789095282554626:  28%|██▊       | 17/60 [00:33<01:23,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 3.9956769943237305:  30%|███       | 18/60 [00:35<01:21,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 8.28792953491211:  32%|███▏      | 19/60 [00:37<01:20,  1.96s/it]  \u001B[A\n",
      "episode_reward_mean = 12.28410816192627:  33%|███▎      | 20/60 [00:39<01:18,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 15.789803504943848:  35%|███▌      | 21/60 [00:40<01:15,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 18.698001861572266:  37%|███▋      | 22/60 [00:42<01:14,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 20.333852767944336:  38%|███▊      | 23/60 [00:44<01:12,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 26.199541091918945:  40%|████      | 24/60 [00:46<01:10,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 30.22629165649414:  42%|████▏     | 25/60 [00:48<01:08,  1.96s/it] \u001B[A\n",
      "episode_reward_mean = 38.9101676940918:  43%|████▎     | 26/60 [00:50<01:06,  1.95s/it] \u001B[A\n",
      "episode_reward_mean = 36.496238708496094:  45%|████▌     | 27/60 [00:52<01:05,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 27.95977783203125:  47%|████▋     | 28/60 [00:54<01:02,  1.97s/it] \u001B[A\n",
      "episode_reward_mean = 37.91832733154297:  48%|████▊     | 29/60 [00:56<01:00,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 37.51070022583008:  50%|█████     | 30/60 [00:58<00:58,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 50.776588439941406:  52%|█████▏    | 31/60 [01:00<00:56,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 57.5227165222168:  53%|█████▎    | 32/60 [01:02<00:54,  1.96s/it]  \u001B[A\n",
      "episode_reward_mean = 58.13140106201172:  55%|█████▌    | 33/60 [01:04<00:52,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 64.82930755615234:  57%|█████▋    | 34/60 [01:06<00:50,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 48.4249267578125:  58%|█████▊    | 35/60 [01:08<00:48,  1.95s/it] \u001B[A\n",
      "episode_reward_mean = 66.0567398071289:  60%|██████    | 36/60 [01:10<00:47,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 68.60154724121094:  62%|██████▏   | 37/60 [01:12<00:45,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 70.19339752197266:  63%|██████▎   | 38/60 [01:14<00:43,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 69.4789810180664:  65%|██████▌   | 39/60 [01:16<00:41,  1.95s/it] \u001B[A\n",
      "episode_reward_mean = 72.5824966430664:  67%|██████▋   | 40/60 [01:18<00:38,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 70.72168731689453:  68%|██████▊   | 41/60 [01:20<00:36,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 66.46498107910156:  70%|███████   | 42/60 [01:22<00:35,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 76.85884094238281:  72%|███████▏  | 43/60 [01:24<00:33,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 72.9783706665039:  73%|███████▎  | 44/60 [01:26<00:31,  1.97s/it] \u001B[A\n",
      "episode_reward_mean = 76.11474609375:  75%|███████▌  | 45/60 [01:28<00:29,  1.98s/it]  \u001B[A\n",
      "episode_reward_mean = 82.34619903564453:  77%|███████▋  | 46/60 [01:30<00:27,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 76.83167266845703:  78%|███████▊  | 47/60 [01:32<00:25,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 78.95625305175781:  80%|████████  | 48/60 [01:34<00:23,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 80.14119720458984:  82%|████████▏ | 49/60 [01:36<00:21,  2.00s/it]\u001B[A\n",
      "episode_reward_mean = 86.53087615966797:  83%|████████▎ | 50/60 [01:38<00:19,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 83.88190460205078:  85%|████████▌ | 51/60 [01:40<00:17,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 84.51212310791016:  87%|████████▋ | 52/60 [01:42<00:15,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 94.10359191894531:  88%|████████▊ | 53/60 [01:43<00:13,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 85.58455657958984:  90%|█████████ | 54/60 [01:45<00:11,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 86.19690704345703:  92%|█████████▏| 55/60 [01:47<00:09,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = 99.80277252197266:  93%|█████████▎| 56/60 [01:49<00:07,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = 98.70875549316406:  95%|█████████▌| 57/60 [01:51<00:05,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = 98.38590240478516:  97%|█████████▋| 58/60 [01:53<00:03,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 95.10160064697266:  98%|█████████▊| 59/60 [01:55<00:01,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 84.0755844116211: 100%|██████████| 60/60 [01:57<00:00,  1.96s/it] \u001B[A\n",
      "2025-08-20 20:20:07,512 [torchrl][INFO] Training time: 64.47 seconds\n",
      "2025-08-20 20:20:07,517 [torchrl][INFO] macs: 11.62 MMac  Params: 10.37 k\n",
      "2025-08-20 20:20:08,490 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:20:08,530 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -10.140951156616211:   2%|▏         | 1/60 [00:01<01:54,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = -11.065966606140137:   3%|▎         | 2/60 [00:03<01:51,  1.92s/it]\u001B[A\n",
      "episode_reward_mean = -11.174388885498047:   5%|▌         | 3/60 [00:05<01:49,  1.92s/it]\u001B[A\n",
      "episode_reward_mean = -10.736160278320312:   7%|▋         | 4/60 [00:07<01:47,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = -10.782674789428711:   8%|▊         | 5/60 [00:09<01:45,  1.92s/it]\u001B[A\n",
      "episode_reward_mean = -10.383540153503418:  10%|█         | 6/60 [00:11<01:43,  1.91s/it]\u001B[A\n",
      "episode_reward_mean = -9.27962589263916:  12%|█▏        | 7/60 [00:13<01:41,  1.91s/it]  \u001B[A\n",
      "episode_reward_mean = -9.593548774719238:  13%|█▎        | 8/60 [00:15<01:39,  1.91s/it]\u001B[A\n",
      "episode_reward_mean = -8.546120643615723:  15%|█▌        | 9/60 [00:17<01:37,  1.90s/it]\u001B[A\n",
      "episode_reward_mean = -9.894536972045898:  17%|█▋        | 10/60 [00:19<01:36,  1.92s/it]\u001B[A\n",
      "episode_reward_mean = -9.036566734313965:  18%|█▊        | 11/60 [00:21<01:33,  1.92s/it]\u001B[A\n",
      "episode_reward_mean = -8.09730052947998:  20%|██        | 12/60 [00:22<01:32,  1.92s/it] \u001B[A\n",
      "episode_reward_mean = -6.693881511688232:  22%|██▏       | 13/60 [00:24<01:30,  1.92s/it]\u001B[A\n",
      "episode_reward_mean = -6.517094612121582:  23%|██▎       | 14/60 [00:26<01:28,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = -2.479933261871338:  25%|██▌       | 15/60 [00:28<01:28,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = -0.18561643362045288:  27%|██▋       | 16/60 [00:30<01:26,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 0.5221807956695557:  28%|██▊       | 17/60 [00:32<01:25,  1.99s/it]  \u001B[A\n",
      "episode_reward_mean = 7.923976898193359:  30%|███       | 18/60 [00:34<01:22,  1.98s/it] \u001B[A\n",
      "episode_reward_mean = 6.5911359786987305:  32%|███▏      | 19/60 [00:36<01:21,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = 11.203908920288086:  33%|███▎      | 20/60 [00:38<01:18,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 15.094657897949219:  35%|███▌      | 21/60 [00:40<01:15,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 16.698213577270508:  37%|███▋      | 22/60 [00:42<01:13,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 23.297183990478516:  38%|███▊      | 23/60 [00:44<01:11,  1.92s/it]\u001B[A\n",
      "episode_reward_mean = 24.851680755615234:  40%|████      | 24/60 [00:46<01:08,  1.91s/it]\u001B[A\n",
      "episode_reward_mean = 23.455678939819336:  42%|████▏     | 25/60 [00:48<01:06,  1.91s/it]\u001B[A\n",
      "episode_reward_mean = 34.08120346069336:  43%|████▎     | 26/60 [00:50<01:04,  1.90s/it] \u001B[A\n",
      "episode_reward_mean = 36.50145721435547:  45%|████▌     | 27/60 [00:52<01:02,  1.91s/it]\u001B[A\n",
      "episode_reward_mean = 33.786354064941406:  47%|████▋     | 28/60 [00:54<01:00,  1.90s/it]\u001B[A\n",
      "episode_reward_mean = 51.945213317871094:  48%|████▊     | 29/60 [00:55<00:59,  1.92s/it]\u001B[A\n",
      "episode_reward_mean = 49.114986419677734:  50%|█████     | 30/60 [00:57<00:58,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = 46.09284973144531:  52%|█████▏    | 31/60 [00:59<00:55,  1.93s/it] \u001B[A\n",
      "episode_reward_mean = 55.26752471923828:  53%|█████▎    | 32/60 [01:01<00:53,  1.92s/it]\u001B[A\n",
      "episode_reward_mean = 53.020408630371094:  55%|█████▌    | 33/60 [01:03<00:52,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 60.46614074707031:  57%|█████▋    | 34/60 [01:05<00:50,  1.93s/it] \u001B[A\n",
      "episode_reward_mean = 69.31509399414062:  58%|█████▊    | 35/60 [01:07<00:48,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = 80.49365234375:  60%|██████    | 36/60 [01:09<00:46,  1.92s/it]   \u001B[A\n",
      "episode_reward_mean = 78.60242462158203:  62%|██████▏   | 37/60 [01:11<00:44,  1.91s/it]\u001B[A\n",
      "episode_reward_mean = 77.59432220458984:  63%|██████▎   | 38/60 [01:13<00:42,  1.91s/it]\u001B[A\n",
      "episode_reward_mean = 81.01483917236328:  65%|██████▌   | 39/60 [01:15<00:40,  1.91s/it]\u001B[A\n",
      "episode_reward_mean = 70.67616271972656:  67%|██████▋   | 40/60 [01:17<00:38,  1.91s/it]\u001B[A\n",
      "episode_reward_mean = 71.44578552246094:  68%|██████▊   | 41/60 [01:19<00:36,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 76.1710205078125:  70%|███████   | 42/60 [01:21<00:34,  1.93s/it] \u001B[A\n",
      "episode_reward_mean = 90.36129760742188:  72%|███████▏  | 43/60 [01:22<00:32,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = 90.14948272705078:  73%|███████▎  | 44/60 [01:24<00:30,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = 93.78456115722656:  75%|███████▌  | 45/60 [01:26<00:28,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = 97.0184326171875:  77%|███████▋  | 46/60 [01:28<00:26,  1.93s/it] \u001B[A\n",
      "episode_reward_mean = 91.0222396850586:  78%|███████▊  | 47/60 [01:30<00:25,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = 90.44541931152344:  80%|████████  | 48/60 [01:32<00:23,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 90.26087951660156:  82%|████████▏ | 49/60 [01:34<00:21,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 96.1137466430664:  83%|████████▎ | 50/60 [01:36<00:19,  1.94s/it] \u001B[A\n",
      "episode_reward_mean = 93.26081085205078:  85%|████████▌ | 51/60 [01:38<00:17,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = 98.5005111694336:  87%|████████▋ | 52/60 [01:40<00:15,  1.92s/it] \u001B[A\n",
      "episode_reward_mean = 88.47429656982422:  88%|████████▊ | 53/60 [01:42<00:13,  1.91s/it]\u001B[A\n",
      "episode_reward_mean = 90.6552734375:  90%|█████████ | 54/60 [01:44<00:11,  1.91s/it]    \u001B[A\n",
      "episode_reward_mean = 90.66950988769531:  92%|█████████▏| 55/60 [01:46<00:09,  1.91s/it]\u001B[A\n",
      "episode_reward_mean = 97.76637268066406:  93%|█████████▎| 56/60 [01:47<00:07,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = 100.83723449707031:  95%|█████████▌| 57/60 [01:49<00:05,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 94.26480102539062:  97%|█████████▋| 58/60 [01:51<00:03,  1.97s/it] \u001B[A\n",
      "episode_reward_mean = 93.42517852783203:  98%|█████████▊| 59/60 [01:53<00:01,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 97.20512390136719: 100%|██████████| 60/60 [01:55<00:00,  1.93s/it]\u001B[A\n",
      "2025-08-20 20:22:04,420 [torchrl][INFO] Training time: 63.58 seconds\n",
      "2025-08-20 20:22:04,426 [torchrl][INFO] macs: 11.62 MMac  Params: 10.37 k\n",
      "2025-08-20 20:22:05,385 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:22:05,425 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -12.64248275756836:   2%|▏         | 1/60 [00:02<02:00,  2.04s/it]\u001B[A\n",
      "episode_reward_mean = -12.27196216583252:   3%|▎         | 2/60 [00:03<01:54,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = -10.09637451171875:   5%|▌         | 3/60 [00:05<01:52,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = -8.354325294494629:   7%|▋         | 4/60 [00:07<01:49,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = -6.2039475440979:   8%|▊         | 5/60 [00:09<01:47,  1.95s/it]  \u001B[A\n",
      "episode_reward_mean = -4.457566261291504:  10%|█         | 6/60 [00:11<01:45,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = -4.11956787109375:  12%|█▏        | 7/60 [00:13<01:42,  1.93s/it] \u001B[A\n",
      "episode_reward_mean = -5.829205513000488:  13%|█▎        | 8/60 [00:15<01:40,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = -5.164558410644531:  15%|█▌        | 9/60 [00:17<01:38,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = -2.2060444355010986:  17%|█▋        | 10/60 [00:19<01:36,  1.92s/it]\u001B[A\n",
      "episode_reward_mean = -4.217142105102539:  18%|█▊        | 11/60 [00:21<01:34,  1.92s/it] \u001B[A\n",
      "episode_reward_mean = -1.196218490600586:  20%|██        | 12/60 [00:23<01:32,  1.92s/it]\u001B[A\n",
      "episode_reward_mean = -0.8291208148002625:  22%|██▏       | 13/60 [00:25<01:30,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = -0.08787371963262558:  23%|██▎       | 14/60 [00:27<01:28,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = 4.339783668518066:  25%|██▌       | 15/60 [00:29<01:27,  1.94s/it]   \u001B[A\n",
      "episode_reward_mean = 12.395380973815918:  27%|██▋       | 16/60 [00:31<01:25,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 11.472387313842773:  28%|██▊       | 17/60 [00:32<01:23,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 19.239643096923828:  30%|███       | 18/60 [00:34<01:21,  1.93s/it]\u001B[A\n",
      "episode_reward_mean = 20.299556732177734:  32%|███▏      | 19/60 [00:36<01:18,  1.92s/it]\u001B[A\n",
      "episode_reward_mean = 30.260568618774414:  33%|███▎      | 20/60 [00:38<01:16,  1.92s/it]\u001B[A\n",
      "episode_reward_mean = 26.577878952026367:  35%|███▌      | 21/60 [00:40<01:14,  1.91s/it]\u001B[A\n",
      "episode_reward_mean = 27.612531661987305:  37%|███▋      | 22/60 [00:42<01:13,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 33.00339889526367:  38%|███▊      | 23/60 [00:44<01:12,  1.96s/it] \u001B[A\n",
      "episode_reward_mean = 37.020206451416016:  40%|████      | 24/60 [00:46<01:09,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 41.69638442993164:  42%|████▏     | 25/60 [00:48<01:07,  1.94s/it] \u001B[A\n",
      "episode_reward_mean = 46.0731086730957:  43%|████▎     | 26/60 [00:50<01:05,  1.93s/it] \u001B[A\n",
      "episode_reward_mean = 50.73629379272461:  45%|████▌     | 27/60 [00:52<01:03,  1.94s/it]\u001B[A\n",
      "episode_reward_mean = 55.16028594970703:  47%|████▋     | 28/60 [00:54<01:02,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 59.753536224365234:  48%|████▊     | 29/60 [00:56<01:01,  2.00s/it]\u001B[A\n",
      "episode_reward_mean = 63.987342834472656:  50%|█████     | 30/60 [00:58<01:00,  2.00s/it]\u001B[A\n",
      "episode_reward_mean = 65.87108612060547:  52%|█████▏    | 31/60 [01:00<00:58,  2.01s/it] \u001B[A\n",
      "episode_reward_mean = 64.26010131835938:  53%|█████▎    | 32/60 [01:02<00:56,  2.00s/it]\u001B[A\n",
      "episode_reward_mean = 66.98523712158203:  55%|█████▌    | 33/60 [01:04<00:54,  2.00s/it]\u001B[A\n",
      "episode_reward_mean = 60.836029052734375:  57%|█████▋    | 34/60 [01:06<00:52,  2.00s/it]\u001B[A\n",
      "episode_reward_mean = 72.01595306396484:  58%|█████▊    | 35/60 [01:08<00:49,  1.98s/it] \u001B[A\n",
      "episode_reward_mean = 83.13502502441406:  60%|██████    | 36/60 [01:10<00:47,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 73.59716796875:  62%|██████▏   | 37/60 [01:12<00:45,  1.97s/it]   \u001B[A\n",
      "episode_reward_mean = 71.2845458984375:  63%|██████▎   | 38/60 [01:14<00:43,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 81.9206314086914:  65%|██████▌   | 39/60 [01:16<00:41,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 78.25802612304688:  67%|██████▋   | 40/60 [01:18<00:39,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 77.67230987548828:  68%|██████▊   | 41/60 [01:20<00:37,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 75.9384536743164:  70%|███████   | 42/60 [01:22<00:35,  1.96s/it] \u001B[A\n",
      "episode_reward_mean = 76.43785095214844:  72%|███████▏  | 43/60 [01:23<00:33,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 84.9768295288086:  73%|███████▎  | 44/60 [01:25<00:31,  1.96s/it] \u001B[A\n",
      "episode_reward_mean = 84.50533294677734:  75%|███████▌  | 45/60 [01:28<00:29,  2.00s/it]\u001B[A\n",
      "episode_reward_mean = 86.68818664550781:  77%|███████▋  | 46/60 [01:29<00:27,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 87.24764251708984:  78%|███████▊  | 47/60 [01:31<00:25,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 91.3793716430664:  80%|████████  | 48/60 [01:33<00:23,  1.99s/it] \u001B[A\n",
      "episode_reward_mean = 84.01667785644531:  82%|████████▏ | 49/60 [01:35<00:21,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 86.49113464355469:  83%|████████▎ | 50/60 [01:37<00:19,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 89.12164306640625:  85%|████████▌ | 51/60 [01:39<00:17,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 87.2174301147461:  87%|████████▋ | 52/60 [01:41<00:15,  1.95s/it] \u001B[A\n",
      "episode_reward_mean = 86.74911499023438:  88%|████████▊ | 53/60 [01:43<00:13,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = 93.0283203125:  90%|█████████ | 54/60 [01:45<00:11,  1.96s/it]    \u001B[A\n",
      "episode_reward_mean = 87.96637725830078:  92%|█████████▏| 55/60 [01:47<00:09,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 94.8935546875:  93%|█████████▎| 56/60 [01:49<00:07,  1.96s/it]    \u001B[A\n",
      "episode_reward_mean = 81.67625427246094:  95%|█████████▌| 57/60 [01:51<00:05,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 89.53800964355469:  97%|█████████▋| 58/60 [01:53<00:03,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 91.73164367675781:  98%|█████████▊| 59/60 [01:55<00:01,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 87.13705444335938: 100%|██████████| 60/60 [01:57<00:00,  1.96s/it]\u001B[A\n",
      "2025-08-20 20:24:03,003 [torchrl][INFO] Training time: 64.43 seconds\n",
      "2025-08-20 20:24:03,008 [torchrl][INFO] macs: 11.62 MMac  Params: 10.37 k\n",
      "2025-08-20 20:24:04,002 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:24:04,046 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -7.758786201477051:   2%|▏         | 1/60 [00:02<01:58,  2.01s/it]\u001B[A\n",
      "episode_reward_mean = -3.914454698562622:   3%|▎         | 2/60 [00:03<01:55,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = -5.8482184410095215:   5%|▌         | 3/60 [00:05<01:53,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = -3.0538744926452637:   7%|▋         | 4/60 [00:07<01:51,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = -3.156352996826172:   8%|▊         | 5/60 [00:10<01:50,  2.01s/it] \u001B[A\n",
      "episode_reward_mean = -4.6127610206604:  10%|█         | 6/60 [00:12<01:48,  2.01s/it]  \u001B[A\n",
      "episode_reward_mean = -2.480006694793701:  12%|█▏        | 7/60 [00:13<01:45,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = -5.2560038566589355:  13%|█▎        | 8/60 [00:15<01:43,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = -4.516329288482666:  15%|█▌        | 9/60 [00:17<01:41,  1.99s/it] \u001B[A\n",
      "episode_reward_mean = 0.7925311923027039:  17%|█▋        | 10/60 [00:19<01:39,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = 1.0619325637817383:  18%|█▊        | 11/60 [00:21<01:38,  2.01s/it]\u001B[A\n",
      "episode_reward_mean = 4.621004581451416:  20%|██        | 12/60 [00:23<01:36,  2.00s/it] \u001B[A\n",
      "episode_reward_mean = 11.611495971679688:  22%|██▏       | 13/60 [00:25<01:33,  2.00s/it]\u001B[A\n",
      "episode_reward_mean = 9.562606811523438:  23%|██▎       | 14/60 [00:27<01:31,  2.00s/it] \u001B[A\n",
      "episode_reward_mean = 14.445263862609863:  25%|██▌       | 15/60 [00:29<01:29,  2.00s/it]\u001B[A\n",
      "episode_reward_mean = 20.83131980895996:  27%|██▋       | 16/60 [00:31<01:27,  1.99s/it] \u001B[A\n",
      "episode_reward_mean = 20.768991470336914:  28%|██▊       | 17/60 [00:33<01:25,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = 26.937284469604492:  30%|███       | 18/60 [00:35<01:23,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 37.537994384765625:  32%|███▏      | 19/60 [00:37<01:21,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 37.944435119628906:  33%|███▎      | 20/60 [00:39<01:19,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 38.01115417480469:  35%|███▌      | 21/60 [00:41<01:17,  1.99s/it] \u001B[A\n",
      "episode_reward_mean = 39.51277542114258:  37%|███▋      | 22/60 [00:44<01:18,  2.07s/it]\u001B[A\n",
      "episode_reward_mean = 44.81486892700195:  38%|███▊      | 23/60 [00:46<01:16,  2.06s/it]\u001B[A\n",
      "episode_reward_mean = 42.756961822509766:  40%|████      | 24/60 [00:48<01:13,  2.04s/it]\u001B[A\n",
      "episode_reward_mean = 58.566829681396484:  42%|████▏     | 25/60 [00:50<01:11,  2.04s/it]\u001B[A\n",
      "episode_reward_mean = 64.14644622802734:  43%|████▎     | 26/60 [00:52<01:10,  2.06s/it] \u001B[A\n",
      "episode_reward_mean = 55.15627670288086:  45%|████▌     | 27/60 [00:54<01:07,  2.04s/it]\u001B[A\n",
      "episode_reward_mean = 65.38835144042969:  47%|████▋     | 28/60 [00:56<01:04,  2.03s/it]\u001B[A\n",
      "episode_reward_mean = 55.487396240234375:  48%|████▊     | 29/60 [00:58<01:02,  2.02s/it]\u001B[A\n",
      "episode_reward_mean = 58.84953308105469:  50%|█████     | 30/60 [01:00<01:00,  2.02s/it] \u001B[A\n",
      "episode_reward_mean = 68.08480834960938:  52%|█████▏    | 31/60 [01:02<00:58,  2.03s/it]\u001B[A\n",
      "episode_reward_mean = 74.51354217529297:  53%|█████▎    | 32/60 [01:04<00:57,  2.06s/it]\u001B[A\n",
      "episode_reward_mean = 75.92771911621094:  55%|█████▌    | 33/60 [01:06<00:55,  2.05s/it]\u001B[A\n",
      "episode_reward_mean = 73.0323715209961:  57%|█████▋    | 34/60 [01:08<00:52,  2.03s/it] \u001B[A\n",
      "episode_reward_mean = 77.05309295654297:  58%|█████▊    | 35/60 [01:10<00:50,  2.03s/it]\u001B[A\n",
      "episode_reward_mean = 80.10946655273438:  60%|██████    | 36/60 [01:12<00:48,  2.01s/it]\u001B[A\n",
      "episode_reward_mean = 83.95255279541016:  62%|██████▏   | 37/60 [01:14<00:46,  2.01s/it]\u001B[A\n",
      "episode_reward_mean = 78.44001007080078:  63%|██████▎   | 38/60 [01:16<00:44,  2.02s/it]\u001B[A\n",
      "episode_reward_mean = 85.35863494873047:  65%|██████▌   | 39/60 [01:18<00:43,  2.05s/it]\u001B[A\n",
      "episode_reward_mean = 82.66480255126953:  67%|██████▋   | 40/60 [01:20<00:40,  2.05s/it]\u001B[A\n",
      "episode_reward_mean = 82.96660614013672:  68%|██████▊   | 41/60 [01:22<00:38,  2.02s/it]\u001B[A\n",
      "episode_reward_mean = 92.73511505126953:  70%|███████   | 42/60 [01:24<00:36,  2.01s/it]\u001B[A\n",
      "episode_reward_mean = 87.7018051147461:  72%|███████▏  | 43/60 [01:26<00:33,  2.00s/it] \u001B[A\n",
      "episode_reward_mean = 95.6878890991211:  73%|███████▎  | 44/60 [01:28<00:31,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = 89.57606506347656:  75%|███████▌  | 45/60 [01:30<00:29,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = 83.42279815673828:  77%|███████▋  | 46/60 [01:32<00:27,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = 90.70339965820312:  78%|███████▊  | 47/60 [01:34<00:25,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = 88.07142639160156:  80%|████████  | 48/60 [01:36<00:24,  2.03s/it]\u001B[A\n",
      "episode_reward_mean = 86.89498901367188:  82%|████████▏ | 49/60 [01:38<00:22,  2.01s/it]\u001B[A\n",
      "episode_reward_mean = 92.45899963378906:  83%|████████▎ | 50/60 [01:40<00:19,  2.00s/it]\u001B[A\n",
      "episode_reward_mean = 89.88299560546875:  85%|████████▌ | 51/60 [01:42<00:17,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = 84.45207977294922:  87%|████████▋ | 52/60 [01:44<00:15,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 96.47666931152344:  88%|████████▊ | 53/60 [01:46<00:13,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 86.97869110107422:  90%|█████████ | 54/60 [01:48<00:12,  2.00s/it]\u001B[A\n",
      "episode_reward_mean = 89.9788818359375:  92%|█████████▏| 55/60 [01:50<00:09,  1.99s/it] \u001B[A\n",
      "episode_reward_mean = 89.45377349853516:  93%|█████████▎| 56/60 [01:52<00:07,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = 98.53654479980469:  95%|█████████▌| 57/60 [01:54<00:05,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 98.80626678466797:  97%|█████████▋| 58/60 [01:56<00:03,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 97.33844757080078:  98%|█████████▊| 59/60 [01:58<00:01,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 100.64543151855469: 100%|██████████| 60/60 [02:00<00:00,  2.01s/it]\u001B[A\n",
      "2025-08-20 20:26:04,511 [torchrl][INFO] Training time: 66.31 seconds\n",
      "2025-08-20 20:26:04,517 [torchrl][INFO] macs: 11.62 MMac  Params: 10.37 k\n",
      "2025-08-20 20:26:05,496 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:26:05,536 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -8.309745788574219:   2%|▏         | 1/60 [00:01<01:55,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = -7.027248382568359:   3%|▎         | 2/60 [00:03<01:52,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = -7.37042760848999:   5%|▌         | 3/60 [00:05<01:51,  1.95s/it] \u001B[A\n",
      "episode_reward_mean = -8.269410133361816:   7%|▋         | 4/60 [00:07<01:49,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = -6.31089973449707:   8%|▊         | 5/60 [00:09<01:47,  1.95s/it] \u001B[A\n",
      "episode_reward_mean = -7.672882080078125:  10%|█         | 6/60 [00:11<01:45,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = -8.577737808227539:  12%|█▏        | 7/60 [00:13<01:43,  1.95s/it]\u001B[A\n",
      "episode_reward_mean = -7.281012535095215:  13%|█▎        | 8/60 [00:15<01:43,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = -7.0307817459106445:  15%|█▌        | 9/60 [00:17<01:40,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = -4.898807525634766:  17%|█▋        | 10/60 [00:19<01:38,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = -5.457610130310059:  18%|█▊        | 11/60 [00:21<01:36,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = -4.514604568481445:  20%|██        | 12/60 [00:23<01:34,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = -1.4740095138549805:  22%|██▏       | 13/60 [00:25<01:32,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 0.2548995912075043:  23%|██▎       | 14/60 [00:27<01:30,  1.97s/it] \u001B[A\n",
      "episode_reward_mean = 5.009618759155273:  25%|██▌       | 15/60 [00:29<01:29,  2.00s/it] \u001B[A\n",
      "episode_reward_mean = 8.99993896484375:  27%|██▋       | 16/60 [00:31<01:26,  1.97s/it] \u001B[A\n",
      "episode_reward_mean = 13.15317153930664:  28%|██▊       | 17/60 [00:33<01:24,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 15.332685470581055:  30%|███       | 18/60 [00:35<01:22,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 19.841388702392578:  32%|███▏      | 19/60 [00:37<01:20,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 18.717121124267578:  33%|███▎      | 20/60 [00:39<01:18,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 14.955768585205078:  35%|███▌      | 21/60 [00:41<01:16,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 19.725887298583984:  37%|███▋      | 22/60 [00:43<01:14,  1.97s/it]\u001B[A\n",
      "episode_reward_mean = 23.658342361450195:  38%|███▊      | 23/60 [00:45<01:12,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 41.67799377441406:  40%|████      | 24/60 [00:47<01:10,  1.96s/it] \u001B[A\n",
      "episode_reward_mean = 34.70376205444336:  42%|████▏     | 25/60 [00:49<01:08,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 48.42561721801758:  43%|████▎     | 26/60 [00:51<01:06,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 43.342742919921875:  45%|████▌     | 27/60 [00:53<01:04,  1.96s/it]\u001B[A\n",
      "episode_reward_mean = 56.46089172363281:  47%|████▋     | 28/60 [00:55<01:02,  1.97s/it] \u001B[A\n",
      "episode_reward_mean = 48.0829963684082:  48%|████▊     | 29/60 [00:56<01:01,  1.97s/it] \u001B[A\n",
      "episode_reward_mean = 59.31028747558594:  50%|█████     | 30/60 [00:59<00:59,  2.00s/it]\u001B[A\n",
      "episode_reward_mean = 59.430416107177734:  52%|█████▏    | 31/60 [01:01<00:57,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = 69.0696792602539:  53%|█████▎    | 32/60 [01:02<00:55,  1.98s/it]  \u001B[A\n",
      "episode_reward_mean = 70.76937103271484:  55%|█████▌    | 33/60 [01:04<00:53,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = 72.80735778808594:  57%|█████▋    | 34/60 [01:06<00:51,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 70.92759704589844:  58%|█████▊    | 35/60 [01:08<00:49,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 78.83306121826172:  60%|██████    | 36/60 [01:10<00:47,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 76.55581665039062:  62%|██████▏   | 37/60 [01:12<00:45,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 70.44844055175781:  63%|██████▎   | 38/60 [01:14<00:44,  2.01s/it]\u001B[A\n",
      "episode_reward_mean = 73.13427734375:  65%|██████▌   | 39/60 [01:16<00:41,  2.00s/it]   \u001B[A\n",
      "episode_reward_mean = 78.46984100341797:  67%|██████▋   | 40/60 [01:18<00:39,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = 78.74647521972656:  68%|██████▊   | 41/60 [01:20<00:37,  2.00s/it]\u001B[A\n",
      "episode_reward_mean = 87.22807312011719:  70%|███████   | 42/60 [01:22<00:35,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = 80.76773834228516:  72%|███████▏  | 43/60 [01:24<00:33,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 86.70869445800781:  73%|███████▎  | 44/60 [01:26<00:31,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 88.15547943115234:  75%|███████▌  | 45/60 [01:28<00:29,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = 87.88139343261719:  77%|███████▋  | 46/60 [01:30<00:27,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = 85.08487701416016:  78%|███████▊  | 47/60 [01:32<00:25,  1.98s/it]\u001B[A\n",
      "episode_reward_mean = 93.4346923828125:  80%|████████  | 48/60 [01:34<00:23,  1.98s/it] \u001B[A\n",
      "episode_reward_mean = 96.41305541992188:  82%|████████▏ | 49/60 [01:36<00:21,  1.99s/it]\u001B[A\n",
      "episode_reward_mean = 91.77204895019531:  83%|████████▎ | 50/60 [01:38<00:20,  2.01s/it]\u001B[A\n",
      "episode_reward_mean = 96.4840087890625:  85%|████████▌ | 51/60 [01:40<00:18,  2.01s/it] \u001B[A\n",
      "episode_reward_mean = 88.11695098876953:  87%|████████▋ | 52/60 [01:42<00:16,  2.05s/it]\u001B[A\n",
      "episode_reward_mean = 88.54218292236328:  88%|████████▊ | 53/60 [01:44<00:14,  2.03s/it]\u001B[A\n",
      "episode_reward_mean = 93.80462646484375:  90%|█████████ | 54/60 [01:46<00:12,  2.02s/it]\u001B[A\n",
      "episode_reward_mean = 93.42816162109375:  92%|█████████▏| 55/60 [01:48<00:10,  2.02s/it]\u001B[A\n",
      "episode_reward_mean = 87.52803039550781:  93%|█████████▎| 56/60 [01:50<00:08,  2.01s/it]\u001B[A\n",
      "episode_reward_mean = 95.7652587890625:  95%|█████████▌| 57/60 [01:52<00:06,  2.00s/it] \u001B[A\n",
      "episode_reward_mean = 95.133544921875:  97%|█████████▋| 58/60 [01:54<00:03,  2.00s/it] \u001B[A\n",
      "episode_reward_mean = 94.07913970947266:  98%|█████████▊| 59/60 [01:56<00:01,  2.00s/it]\u001B[A\n",
      "episode_reward_mean = 92.04593658447266: 100%|██████████| 60/60 [01:58<00:00,  1.98s/it]\u001B[A\n",
      "2025-08-20 20:28:04,500 [torchrl][INFO] Training time: 64.95 seconds\n",
      "2025-08-20 20:28:04,506 [torchrl][INFO] macs: 11.62 MMac  Params: 10.37 k\n",
      "2025-08-20 20:28:05,568 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:28:05,667 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 79.1729965209961:   5%|▌         | 1/20 [00:11<03:43, 11.76s/it]\u001B[A\n",
      "episode_reward_mean = 72.30915832519531:  10%|█         | 2/20 [00:23<03:28, 11.59s/it]\u001B[A\n",
      "episode_reward_mean = 81.83151245117188:  15%|█▌        | 3/20 [00:34<03:15, 11.53s/it]\u001B[A\n",
      "episode_reward_mean = 52.22481155395508:  20%|██        | 4/20 [00:46<03:04, 11.53s/it]\u001B[A\n",
      "episode_reward_mean = 85.79629516601562:  25%|██▌       | 5/20 [00:57<02:50, 11.36s/it]\u001B[A\n",
      "episode_reward_mean = 62.461647033691406:  30%|███       | 6/20 [01:09<02:41, 11.52s/it]\u001B[A\n",
      "episode_reward_mean = 82.46062469482422:  35%|███▌      | 7/20 [01:20<02:27, 11.37s/it] \u001B[A\n",
      "episode_reward_mean = 82.77632141113281:  40%|████      | 8/20 [01:31<02:17, 11.44s/it]\u001B[A\n",
      "episode_reward_mean = 53.3788948059082:  45%|████▌     | 9/20 [01:43<02:05, 11.42s/it] \u001B[A\n",
      "episode_reward_mean = 81.26052856445312:  50%|█████     | 10/20 [01:54<01:55, 11.52s/it]\u001B[A\n",
      "episode_reward_mean = 59.3150749206543:  55%|█████▌    | 11/20 [02:06<01:42, 11.42s/it] \u001B[A\n",
      "episode_reward_mean = 87.50161743164062:  60%|██████    | 12/20 [02:17<01:30, 11.31s/it]\u001B[A\n",
      "episode_reward_mean = 67.23340606689453:  65%|██████▌   | 13/20 [02:28<01:18, 11.27s/it]\u001B[A\n",
      "episode_reward_mean = 67.70397186279297:  70%|███████   | 14/20 [02:39<01:07, 11.25s/it]\u001B[A\n",
      "episode_reward_mean = 77.86173248291016:  75%|███████▌  | 15/20 [02:50<00:56, 11.28s/it]\u001B[A\n",
      "episode_reward_mean = 74.59941864013672:  80%|████████  | 16/20 [03:02<00:45, 11.26s/it]\u001B[A\n",
      "episode_reward_mean = 74.4427719116211:  85%|████████▌ | 17/20 [03:13<00:33, 11.23s/it] \u001B[A\n",
      "episode_reward_mean = 75.28983306884766:  90%|█████████ | 18/20 [03:24<00:22, 11.22s/it]\u001B[A\n",
      "episode_reward_mean = 75.12448120117188:  95%|█████████▌| 19/20 [03:35<00:11, 11.20s/it]\u001B[A\n",
      "episode_reward_mean = 99.74983978271484: 100%|██████████| 20/20 [03:46<00:00, 11.34s/it]\u001B[A\n",
      "2025-08-20 20:31:52,397 [torchrl][INFO] Training time: 175.36 seconds\n",
      "2025-08-20 20:31:52,429 [torchrl][INFO] macs: 46.47 MMac  Params: 10.37 k\n",
      "2025-08-20 20:31:55,091 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:31:55,195 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 64.23036193847656:   5%|▌         | 1/20 [00:10<03:25, 10.83s/it]\u001B[A\n",
      "episode_reward_mean = 55.64455795288086:  10%|█         | 2/20 [00:21<03:16, 10.89s/it]\u001B[A\n",
      "episode_reward_mean = 61.189544677734375:  15%|█▌        | 3/20 [00:32<03:05, 10.90s/it]\u001B[A\n",
      "episode_reward_mean = 80.58956146240234:  20%|██        | 4/20 [00:43<02:55, 10.94s/it] \u001B[A\n",
      "episode_reward_mean = 65.8595199584961:  25%|██▌       | 5/20 [00:54<02:44, 10.97s/it] \u001B[A\n",
      "episode_reward_mean = 80.55123901367188:  30%|███       | 6/20 [01:06<02:37, 11.25s/it]\u001B[A\n",
      "episode_reward_mean = 81.63207244873047:  35%|███▌      | 7/20 [01:18<02:27, 11.37s/it]\u001B[A\n",
      "episode_reward_mean = 67.7352523803711:  40%|████      | 8/20 [01:29<02:15, 11.31s/it] \u001B[A\n",
      "episode_reward_mean = 72.60110473632812:  45%|████▌     | 9/20 [01:42<02:12, 12.01s/it]\u001B[A\n",
      "episode_reward_mean = 54.2696418762207:  50%|█████     | 10/20 [01:54<01:59, 11.99s/it]\u001B[A\n",
      "episode_reward_mean = 64.63150024414062:  55%|█████▌    | 11/20 [02:06<01:47, 11.95s/it]\u001B[A\n",
      "episode_reward_mean = 67.36133575439453:  60%|██████    | 12/20 [02:17<01:34, 11.76s/it]\u001B[A\n",
      "episode_reward_mean = 65.52750396728516:  65%|██████▌   | 13/20 [02:29<01:21, 11.59s/it]\u001B[A\n",
      "episode_reward_mean = 49.14396667480469:  70%|███████   | 14/20 [02:40<01:08, 11.48s/it]\u001B[A\n",
      "episode_reward_mean = 51.035797119140625:  75%|███████▌  | 15/20 [02:51<00:57, 11.46s/it]\u001B[A\n",
      "episode_reward_mean = 63.349998474121094:  80%|████████  | 16/20 [03:03<00:46, 11.51s/it]\u001B[A\n",
      "episode_reward_mean = 67.62995910644531:  85%|████████▌ | 17/20 [03:15<00:34, 11.58s/it] \u001B[A\n",
      "episode_reward_mean = 99.82167053222656:  90%|█████████ | 18/20 [03:27<00:23, 11.66s/it]\u001B[A\n",
      "episode_reward_mean = 65.32316589355469:  95%|█████████▌| 19/20 [03:38<00:11, 11.73s/it]\u001B[A\n",
      "episode_reward_mean = 77.65424346923828: 100%|██████████| 20/20 [03:50<00:00, 11.54s/it]\u001B[A\n",
      "2025-08-20 20:35:45,993 [torchrl][INFO] Training time: 179.62 seconds\n",
      "2025-08-20 20:35:46,032 [torchrl][INFO] macs: 46.47 MMac  Params: 10.37 k\n",
      "2025-08-20 20:35:49,013 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:35:49,130 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 66.31598663330078:   5%|▌         | 1/20 [00:11<03:44, 11.81s/it]\u001B[A\n",
      "episode_reward_mean = 66.78482818603516:  10%|█         | 2/20 [00:23<03:34, 11.92s/it]\u001B[A\n",
      "episode_reward_mean = 59.17216110229492:  15%|█▌        | 3/20 [00:35<03:21, 11.84s/it]\u001B[A\n",
      "episode_reward_mean = 86.99727630615234:  20%|██        | 4/20 [00:47<03:08, 11.76s/it]\u001B[A\n",
      "episode_reward_mean = 63.4353141784668:  25%|██▌       | 5/20 [00:58<02:55, 11.70s/it] \u001B[A\n",
      "episode_reward_mean = 46.65070343017578:  30%|███       | 6/20 [01:10<02:42, 11.63s/it]\u001B[A\n",
      "episode_reward_mean = 64.03888702392578:  35%|███▌      | 7/20 [01:21<02:30, 11.59s/it]\u001B[A\n",
      "episode_reward_mean = 59.74736404418945:  40%|████      | 8/20 [01:33<02:18, 11.56s/it]\u001B[A\n",
      "episode_reward_mean = 78.05392456054688:  45%|████▌     | 9/20 [01:44<02:07, 11.56s/it]\u001B[A\n",
      "episode_reward_mean = 69.77059173583984:  50%|█████     | 10/20 [01:56<01:55, 11.56s/it]\u001B[A\n",
      "episode_reward_mean = 72.5154800415039:  55%|█████▌    | 11/20 [02:07<01:43, 11.55s/it] \u001B[A\n",
      "episode_reward_mean = 72.25300598144531:  60%|██████    | 12/20 [02:19<01:32, 11.62s/it]\u001B[A\n",
      "episode_reward_mean = 81.3096694946289:  65%|██████▌   | 13/20 [02:31<01:21, 11.61s/it] \u001B[A\n",
      "episode_reward_mean = 60.35696792602539:  70%|███████   | 14/20 [02:42<01:09, 11.60s/it]\u001B[A\n",
      "episode_reward_mean = 77.14014434814453:  75%|███████▌  | 15/20 [02:54<00:57, 11.59s/it]\u001B[A\n",
      "episode_reward_mean = 78.61443328857422:  80%|████████  | 16/20 [03:06<00:46, 11.58s/it]\u001B[A\n",
      "episode_reward_mean = 71.54762268066406:  85%|████████▌ | 17/20 [03:17<00:34, 11.61s/it]\u001B[A\n",
      "episode_reward_mean = 77.06695556640625:  90%|█████████ | 18/20 [03:29<00:23, 11.61s/it]\u001B[A\n",
      "episode_reward_mean = 80.93314361572266:  95%|█████████▌| 19/20 [03:40<00:11, 11.58s/it]\u001B[A\n",
      "episode_reward_mean = 60.37506103515625: 100%|██████████| 20/20 [03:52<00:00, 11.63s/it]\u001B[A\n",
      "2025-08-20 20:39:41,763 [torchrl][INFO] Training time: 179.71 seconds\n",
      "2025-08-20 20:39:41,793 [torchrl][INFO] macs: 46.47 MMac  Params: 10.37 k\n",
      "2025-08-20 20:39:44,610 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:39:44,720 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 52.02596664428711:   5%|▌         | 1/20 [00:11<03:33, 11.25s/it]\u001B[A\n",
      "episode_reward_mean = 59.9419059753418:  10%|█         | 2/20 [00:22<03:24, 11.35s/it] \u001B[A\n",
      "episode_reward_mean = 63.83776092529297:  15%|█▌        | 3/20 [00:34<03:14, 11.42s/it]\u001B[A\n",
      "episode_reward_mean = 55.334930419921875:  20%|██        | 4/20 [00:45<03:03, 11.45s/it]\u001B[A\n",
      "episode_reward_mean = 69.7677993774414:  25%|██▌       | 5/20 [00:57<02:51, 11.45s/it]  \u001B[A\n",
      "episode_reward_mean = 80.08207702636719:  30%|███       | 6/20 [01:08<02:40, 11.48s/it]\u001B[A\n",
      "episode_reward_mean = 66.31660461425781:  35%|███▌      | 7/20 [01:20<02:29, 11.52s/it]\u001B[A\n",
      "episode_reward_mean = 62.858367919921875:  40%|████      | 8/20 [01:31<02:18, 11.55s/it]\u001B[A\n",
      "episode_reward_mean = 51.476375579833984:  45%|████▌     | 9/20 [01:43<02:07, 11.60s/it]\u001B[A\n",
      "episode_reward_mean = 65.81798553466797:  50%|█████     | 10/20 [01:55<01:56, 11.65s/it]\u001B[A\n",
      "episode_reward_mean = 62.297279357910156:  55%|█████▌    | 11/20 [02:06<01:44, 11.61s/it]\u001B[A\n",
      "episode_reward_mean = 58.792457580566406:  60%|██████    | 12/20 [02:18<01:32, 11.59s/it]\u001B[A\n",
      "episode_reward_mean = 73.15443420410156:  65%|██████▌   | 13/20 [02:30<01:21, 11.59s/it] \u001B[A\n",
      "episode_reward_mean = 83.89107513427734:  70%|███████   | 14/20 [02:41<01:09, 11.59s/it]\u001B[A\n",
      "episode_reward_mean = 74.36302947998047:  75%|███████▌  | 15/20 [02:53<00:57, 11.60s/it]\u001B[A\n",
      "episode_reward_mean = 59.873291015625:  80%|████████  | 16/20 [03:05<00:46, 11.66s/it]  \u001B[A\n",
      "episode_reward_mean = 52.54275894165039:  85%|████████▌ | 17/20 [03:16<00:34, 11.65s/it]\u001B[A\n",
      "episode_reward_mean = 81.7037353515625:  90%|█████████ | 18/20 [03:28<00:23, 11.64s/it] \u001B[A\n",
      "episode_reward_mean = 76.08480072021484:  95%|█████████▌| 19/20 [03:40<00:11, 11.68s/it]\u001B[A\n",
      "episode_reward_mean = 71.23880004882812: 100%|██████████| 20/20 [03:51<00:00, 11.59s/it]\u001B[A\n",
      "2025-08-20 20:43:36,465 [torchrl][INFO] Training time: 179.39 seconds\n",
      "2025-08-20 20:43:36,497 [torchrl][INFO] macs: 46.47 MMac  Params: 10.37 k\n",
      "2025-08-20 20:43:39,332 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:43:39,446 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 95.93101501464844:   5%|▌         | 1/20 [00:12<04:02, 12.76s/it]\u001B[A\n",
      "episode_reward_mean = 102.07462310791016:  10%|█         | 2/20 [00:24<03:39, 12.20s/it]\u001B[A\n",
      "episode_reward_mean = 70.24178314208984:  15%|█▌        | 3/20 [00:36<03:24, 12.04s/it] \u001B[A\n",
      "episode_reward_mean = 108.15653228759766:  20%|██        | 4/20 [00:48<03:11, 11.94s/it]\u001B[A\n",
      "episode_reward_mean = 71.76329040527344:  25%|██▌       | 5/20 [00:59<02:57, 11.83s/it] \u001B[A\n",
      "episode_reward_mean = 79.30158996582031:  30%|███       | 6/20 [01:11<02:45, 11.81s/it]\u001B[A\n",
      "episode_reward_mean = 84.35597229003906:  35%|███▌      | 7/20 [01:23<02:33, 11.78s/it]\u001B[A\n",
      "episode_reward_mean = 64.43709564208984:  40%|████      | 8/20 [01:34<02:20, 11.74s/it]\u001B[A\n",
      "episode_reward_mean = 76.62281036376953:  45%|████▌     | 9/20 [01:46<02:09, 11.74s/it]\u001B[A\n",
      "episode_reward_mean = 86.73065185546875:  50%|█████     | 10/20 [01:58<01:57, 11.74s/it]\u001B[A\n",
      "episode_reward_mean = 88.90459442138672:  55%|█████▌    | 11/20 [02:10<01:46, 11.78s/it]\u001B[A\n",
      "episode_reward_mean = 72.33071899414062:  60%|██████    | 12/20 [02:22<01:35, 11.89s/it]\u001B[A\n",
      "episode_reward_mean = 92.10569763183594:  65%|██████▌   | 13/20 [02:34<01:23, 11.92s/it]\u001B[A\n",
      "episode_reward_mean = 90.1825180053711:  70%|███████   | 14/20 [02:46<01:11, 11.91s/it] \u001B[A\n",
      "episode_reward_mean = 103.66175079345703:  75%|███████▌  | 15/20 [02:58<00:59, 11.89s/it]\u001B[A\n",
      "episode_reward_mean = 93.75885009765625:  80%|████████  | 16/20 [03:10<00:47, 11.88s/it] \u001B[A\n",
      "episode_reward_mean = 83.11620330810547:  85%|████████▌ | 17/20 [03:22<00:35, 11.93s/it]\u001B[A\n",
      "episode_reward_mean = 79.91419219970703:  90%|█████████ | 18/20 [03:33<00:23, 11.89s/it]\u001B[A\n",
      "episode_reward_mean = 94.70380401611328:  95%|█████████▌| 19/20 [03:45<00:11, 11.87s/it]\u001B[A\n",
      "episode_reward_mean = 92.15974426269531: 100%|██████████| 20/20 [03:57<00:00, 11.87s/it]\u001B[A\n",
      "2025-08-20 20:47:36,912 [torchrl][INFO] Training time: 182.38 seconds\n",
      "2025-08-20 20:47:36,945 [torchrl][INFO] macs: 46.47 MMac  Params: 10.37 k\n",
      "2025-08-20 20:47:39,791 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:47:39,902 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 74.87232971191406:   5%|▌         | 1/20 [00:11<03:40, 11.62s/it]\u001B[A\n",
      "episode_reward_mean = 91.42479705810547:  10%|█         | 2/20 [00:23<03:30, 11.69s/it]\u001B[A\n",
      "episode_reward_mean = 84.2913589477539:  15%|█▌        | 3/20 [00:35<03:19, 11.73s/it] \u001B[A\n",
      "episode_reward_mean = 67.39069366455078:  20%|██        | 4/20 [00:46<03:07, 11.74s/it]\u001B[A\n",
      "episode_reward_mean = 75.66742706298828:  25%|██▌       | 5/20 [00:58<02:55, 11.72s/it]\u001B[A\n",
      "episode_reward_mean = 59.698211669921875:  30%|███       | 6/20 [01:10<02:44, 11.72s/it]\u001B[A\n",
      "episode_reward_mean = 88.86064147949219:  35%|███▌      | 7/20 [01:22<02:32, 11.76s/it] \u001B[A\n",
      "episode_reward_mean = 72.27594757080078:  40%|████      | 8/20 [01:33<02:21, 11.77s/it]\u001B[A\n",
      "episode_reward_mean = 77.11463928222656:  45%|████▌     | 9/20 [01:45<02:10, 11.83s/it]\u001B[A\n",
      "episode_reward_mean = 83.29914093017578:  50%|█████     | 10/20 [01:57<01:58, 11.81s/it]\u001B[A\n",
      "episode_reward_mean = 89.13077545166016:  55%|█████▌    | 11/20 [02:09<01:46, 11.81s/it]\u001B[A\n",
      "episode_reward_mean = 91.20377349853516:  60%|██████    | 12/20 [02:21<01:34, 11.82s/it]\u001B[A\n",
      "episode_reward_mean = 90.51126861572266:  65%|██████▌   | 13/20 [02:33<01:22, 11.81s/it]\u001B[A\n",
      "episode_reward_mean = 81.92263793945312:  70%|███████   | 14/20 [02:44<01:10, 11.81s/it]\u001B[A\n",
      "episode_reward_mean = 91.40116119384766:  75%|███████▌  | 15/20 [02:56<00:58, 11.77s/it]\u001B[A\n",
      "episode_reward_mean = 92.06538391113281:  80%|████████  | 16/20 [03:08<00:46, 11.71s/it]\u001B[A\n",
      "episode_reward_mean = 81.83167266845703:  85%|████████▌ | 17/20 [03:19<00:35, 11.67s/it]\u001B[A\n",
      "episode_reward_mean = 96.32022857666016:  90%|█████████ | 18/20 [03:31<00:23, 11.63s/it]\u001B[A\n",
      "episode_reward_mean = 88.06941223144531:  95%|█████████▌| 19/20 [03:42<00:11, 11.61s/it]\u001B[A\n",
      "episode_reward_mean = 84.73363494873047: 100%|██████████| 20/20 [03:54<00:00, 11.72s/it]\u001B[A\n",
      "2025-08-20 20:51:34,319 [torchrl][INFO] Training time: 179.42 seconds\n",
      "2025-08-20 20:51:34,351 [torchrl][INFO] macs: 46.47 MMac  Params: 10.37 k\n",
      "2025-08-20 20:51:37,188 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:51:37,295 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 84.12201690673828:   5%|▌         | 1/20 [00:11<03:37, 11.45s/it]\u001B[A\n",
      "episode_reward_mean = 84.93537902832031:  10%|█         | 2/20 [00:23<03:28, 11.56s/it]\u001B[A\n",
      "episode_reward_mean = 95.180908203125:  15%|█▌        | 3/20 [00:34<03:17, 11.61s/it]  \u001B[A\n",
      "episode_reward_mean = 92.97085571289062:  20%|██        | 4/20 [00:46<03:06, 11.68s/it]\u001B[A\n",
      "episode_reward_mean = 76.94591522216797:  25%|██▌       | 5/20 [00:58<02:56, 11.77s/it]\u001B[A\n",
      "episode_reward_mean = 89.96543884277344:  30%|███       | 6/20 [01:10<02:45, 11.83s/it]\u001B[A\n",
      "episode_reward_mean = 88.69470977783203:  35%|███▌      | 7/20 [01:22<02:36, 12.01s/it]\u001B[A\n",
      "episode_reward_mean = 91.84310913085938:  40%|████      | 8/20 [01:35<02:25, 12.15s/it]\u001B[A\n",
      "episode_reward_mean = 67.6212158203125:  45%|████▌     | 9/20 [01:47<02:14, 12.22s/it] \u001B[A\n",
      "episode_reward_mean = 73.02223205566406:  50%|█████     | 10/20 [01:59<02:00, 12.04s/it]\u001B[A\n",
      "episode_reward_mean = 96.97625732421875:  55%|█████▌    | 11/20 [02:10<01:47, 11.89s/it]\u001B[A\n",
      "episode_reward_mean = 79.09790802001953:  60%|██████    | 12/20 [02:22<01:34, 11.80s/it]\u001B[A\n",
      "episode_reward_mean = 80.65872192382812:  65%|██████▌   | 13/20 [02:34<01:22, 11.76s/it]\u001B[A\n",
      "episode_reward_mean = 97.21820831298828:  70%|███████   | 14/20 [02:45<01:10, 11.72s/it]\u001B[A\n",
      "episode_reward_mean = 81.53246307373047:  75%|███████▌  | 15/20 [02:57<00:58, 11.70s/it]\u001B[A\n",
      "episode_reward_mean = 94.93804931640625:  80%|████████  | 16/20 [03:08<00:46, 11.65s/it]\u001B[A\n",
      "episode_reward_mean = 95.04073333740234:  85%|████████▌ | 17/20 [03:20<00:34, 11.64s/it]\u001B[A\n",
      "episode_reward_mean = 70.46920013427734:  90%|█████████ | 18/20 [03:32<00:23, 11.63s/it]\u001B[A\n",
      "episode_reward_mean = 89.33030700683594:  95%|█████████▌| 19/20 [03:43<00:11, 11.63s/it]\u001B[A\n",
      "episode_reward_mean = 73.95648193359375: 100%|██████████| 20/20 [03:55<00:00, 11.76s/it]\u001B[A\n",
      "2025-08-20 20:55:32,521 [torchrl][INFO] Training time: 181.51 seconds\n",
      "2025-08-20 20:55:32,552 [torchrl][INFO] macs: 46.47 MMac  Params: 10.37 k\n",
      "2025-08-20 20:55:35,306 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:55:35,410 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 87.8949203491211:   5%|▌         | 1/20 [00:11<03:41, 11.64s/it]\u001B[A\n",
      "episode_reward_mean = 62.65707015991211:  10%|█         | 2/20 [00:23<03:28, 11.59s/it]\u001B[A\n",
      "episode_reward_mean = 61.553890228271484:  15%|█▌        | 3/20 [00:34<03:16, 11.59s/it]\u001B[A\n",
      "episode_reward_mean = 81.32415771484375:  20%|██        | 4/20 [00:46<03:04, 11.56s/it] \u001B[A\n",
      "episode_reward_mean = 67.154296875:  25%|██▌       | 5/20 [00:57<02:53, 11.54s/it]     \u001B[A\n",
      "episode_reward_mean = 71.24081420898438:  30%|███       | 6/20 [01:09<02:41, 11.56s/it]\u001B[A\n",
      "episode_reward_mean = 67.06778717041016:  35%|███▌      | 7/20 [01:21<02:31, 11.62s/it]\u001B[A\n",
      "episode_reward_mean = 81.75961303710938:  40%|████      | 8/20 [01:33<02:21, 11.81s/it]\u001B[A\n",
      "episode_reward_mean = 83.01339721679688:  45%|████▌     | 9/20 [01:45<02:10, 11.85s/it]\u001B[A\n",
      "episode_reward_mean = 80.1579818725586:  50%|█████     | 10/20 [01:57<01:58, 11.83s/it]\u001B[A\n",
      "episode_reward_mean = 59.017921447753906:  55%|█████▌    | 11/20 [02:08<01:46, 11.84s/it]\u001B[A\n",
      "episode_reward_mean = 69.99862670898438:  60%|██████    | 12/20 [02:21<01:35, 11.91s/it] \u001B[A\n",
      "episode_reward_mean = 87.12765502929688:  65%|██████▌   | 13/20 [02:32<01:23, 11.87s/it]\u001B[A\n",
      "episode_reward_mean = 78.9853286743164:  70%|███████   | 14/20 [02:44<01:11, 11.85s/it] \u001B[A\n",
      "episode_reward_mean = 78.96871948242188:  75%|███████▌  | 15/20 [02:56<00:59, 11.83s/it]\u001B[A\n",
      "episode_reward_mean = 93.28487396240234:  80%|████████  | 16/20 [03:08<00:47, 11.81s/it]\u001B[A\n",
      "episode_reward_mean = 91.68560791015625:  85%|████████▌ | 17/20 [03:19<00:35, 11.81s/it]\u001B[A\n",
      "episode_reward_mean = 81.3076400756836:  90%|█████████ | 18/20 [03:31<00:23, 11.84s/it] \u001B[A\n",
      "episode_reward_mean = 74.90167236328125:  95%|█████████▌| 19/20 [03:43<00:11, 11.81s/it]\u001B[A\n",
      "episode_reward_mean = 70.24468994140625: 100%|██████████| 20/20 [03:56<00:00, 11.80s/it]\u001B[A\n",
      "2025-08-20 20:59:31,507 [torchrl][INFO] Training time: 180.56 seconds\n",
      "2025-08-20 20:59:31,538 [torchrl][INFO] macs: 46.47 MMac  Params: 10.37 k\n",
      "2025-08-20 20:59:34,490 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 20:59:34,594 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 72.70986938476562:   5%|▌         | 1/20 [00:11<03:39, 11.54s/it]\u001B[A\n",
      "episode_reward_mean = 71.22985076904297:  10%|█         | 2/20 [00:23<03:33, 11.85s/it]\u001B[A\n",
      "episode_reward_mean = 80.54658508300781:  15%|█▌        | 3/20 [00:35<03:18, 11.69s/it]\u001B[A\n",
      "episode_reward_mean = 80.51959991455078:  20%|██        | 4/20 [00:46<03:05, 11.58s/it]\u001B[A\n",
      "episode_reward_mean = 64.83684539794922:  25%|██▌       | 5/20 [00:58<02:53, 11.59s/it]\u001B[A\n",
      "episode_reward_mean = 77.68030548095703:  30%|███       | 6/20 [01:09<02:41, 11.54s/it]\u001B[A\n",
      "episode_reward_mean = 81.73353576660156:  35%|███▌      | 7/20 [01:21<02:29, 11.53s/it]\u001B[A\n",
      "episode_reward_mean = 79.85440826416016:  40%|████      | 8/20 [01:32<02:18, 11.52s/it]\u001B[A\n",
      "episode_reward_mean = 96.18002319335938:  45%|████▌     | 9/20 [01:44<02:06, 11.53s/it]\u001B[A\n",
      "episode_reward_mean = 73.82402038574219:  50%|█████     | 10/20 [01:55<01:55, 11.53s/it]\u001B[A\n",
      "episode_reward_mean = 69.8187026977539:  55%|█████▌    | 11/20 [02:07<01:43, 11.52s/it] \u001B[A\n",
      "episode_reward_mean = 75.02129364013672:  60%|██████    | 12/20 [02:18<01:32, 11.56s/it]\u001B[A\n",
      "episode_reward_mean = 81.45305633544922:  65%|██████▌   | 13/20 [02:30<01:20, 11.56s/it]\u001B[A\n",
      "episode_reward_mean = 91.82730865478516:  70%|███████   | 14/20 [02:41<01:09, 11.56s/it]\u001B[A\n",
      "episode_reward_mean = 94.11998748779297:  75%|███████▌  | 15/20 [02:53<00:57, 11.55s/it]\u001B[A\n",
      "episode_reward_mean = 79.1317138671875:  80%|████████  | 16/20 [03:05<00:46, 11.57s/it] \u001B[A\n",
      "episode_reward_mean = 83.0992660522461:  85%|████████▌ | 17/20 [03:16<00:34, 11.58s/it]\u001B[A\n",
      "episode_reward_mean = 54.861663818359375:  90%|█████████ | 18/20 [03:28<00:23, 11.61s/it]\u001B[A\n",
      "episode_reward_mean = 69.57872009277344:  95%|█████████▌| 19/20 [03:40<00:11, 11.64s/it] \u001B[A\n",
      "episode_reward_mean = 97.95714569091797: 100%|██████████| 20/20 [03:51<00:00, 11.58s/it]\u001B[A\n",
      "2025-08-20 21:03:26,245 [torchrl][INFO] Training time: 178.74 seconds\n",
      "2025-08-20 21:03:26,275 [torchrl][INFO] macs: 46.47 MMac  Params: 10.37 k\n",
      "2025-08-20 21:03:29,112 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:03:29,217 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 93.93177032470703:   5%|▌         | 1/20 [00:11<03:35, 11.32s/it]\u001B[A\n",
      "episode_reward_mean = 85.64327239990234:  10%|█         | 2/20 [00:22<03:24, 11.36s/it]\u001B[A\n",
      "episode_reward_mean = 83.17656707763672:  15%|█▌        | 3/20 [00:34<03:14, 11.42s/it]\u001B[A\n",
      "episode_reward_mean = 84.26634216308594:  20%|██        | 4/20 [00:45<03:03, 11.47s/it]\u001B[A\n",
      "episode_reward_mean = 95.99357604980469:  25%|██▌       | 5/20 [00:57<02:52, 11.49s/it]\u001B[A\n",
      "episode_reward_mean = 71.81523132324219:  30%|███       | 6/20 [01:08<02:40, 11.48s/it]\u001B[A\n",
      "episode_reward_mean = 80.38243103027344:  35%|███▌      | 7/20 [01:20<02:28, 11.46s/it]\u001B[A\n",
      "episode_reward_mean = 100.32796478271484:  40%|████      | 8/20 [01:31<02:17, 11.42s/it]\u001B[A\n",
      "episode_reward_mean = 94.4442367553711:  45%|████▌     | 9/20 [01:42<02:05, 11.44s/it]  \u001B[A\n",
      "episode_reward_mean = 70.9618911743164:  50%|█████     | 10/20 [01:54<01:54, 11.50s/it]\u001B[A\n",
      "episode_reward_mean = 91.4150161743164:  55%|█████▌    | 11/20 [02:06<01:43, 11.55s/it]\u001B[A\n",
      "episode_reward_mean = 65.46573638916016:  60%|██████    | 12/20 [02:18<01:32, 11.62s/it]\u001B[A\n",
      "episode_reward_mean = 70.0301742553711:  65%|██████▌   | 13/20 [02:29<01:21, 11.65s/it] \u001B[A\n",
      "episode_reward_mean = 87.59986114501953:  70%|███████   | 14/20 [02:42<01:11, 11.86s/it]\u001B[A\n",
      "episode_reward_mean = 80.45660400390625:  75%|███████▌  | 15/20 [02:53<00:59, 11.81s/it]\u001B[A\n",
      "episode_reward_mean = 70.03690338134766:  80%|████████  | 16/20 [03:05<00:47, 11.75s/it]\u001B[A\n",
      "episode_reward_mean = 77.02444458007812:  85%|████████▌ | 17/20 [03:17<00:35, 11.74s/it]\u001B[A\n",
      "episode_reward_mean = 81.92529296875:  90%|█████████ | 18/20 [03:28<00:23, 11.70s/it]   \u001B[A\n",
      "episode_reward_mean = 70.84426879882812:  95%|█████████▌| 19/20 [03:40<00:11, 11.63s/it]\u001B[A\n",
      "episode_reward_mean = 76.38522338867188: 100%|██████████| 20/20 [03:51<00:00, 11.59s/it]\u001B[A\n",
      "2025-08-20 21:07:20,970 [torchrl][INFO] Training time: 179.50 seconds\n",
      "2025-08-20 21:07:21,001 [torchrl][INFO] macs: 46.47 MMac  Params: 10.37 k\n",
      "2025-08-20 21:07:23,770 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:07:23,818 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -10.440092086791992:   2%|▏         | 1/60 [00:02<02:26,  2.49s/it]\u001B[A\n",
      "episode_reward_mean = -8.488885879516602:   3%|▎         | 2/60 [00:04<02:22,  2.45s/it] \u001B[A\n",
      "episode_reward_mean = -7.057815074920654:   5%|▌         | 3/60 [00:07<02:19,  2.44s/it]\u001B[A\n",
      "episode_reward_mean = -5.197174072265625:   7%|▋         | 4/60 [00:09<02:15,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = -4.71345853805542:   8%|▊         | 5/60 [00:12<02:12,  2.40s/it] \u001B[A\n",
      "episode_reward_mean = -4.81691837310791:  10%|█         | 6/60 [00:14<02:09,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = -4.528216361999512:  12%|█▏        | 7/60 [00:16<02:06,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = -2.9186031818389893:  13%|█▎        | 8/60 [00:19<02:03,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 3.0835628509521484:  15%|█▌        | 9/60 [00:21<02:01,  2.39s/it] \u001B[A\n",
      "episode_reward_mean = 5.21523380279541:  17%|█▋        | 10/60 [00:24<02:01,  2.43s/it] \u001B[A\n",
      "episode_reward_mean = 11.497339248657227:  18%|█▊        | 11/60 [00:26<01:58,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 24.81124496459961:  20%|██        | 12/60 [00:28<01:54,  2.40s/it] \u001B[A\n",
      "episode_reward_mean = 26.81244468688965:  22%|██▏       | 13/60 [00:31<01:52,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 24.358327865600586:  23%|██▎       | 14/60 [00:33<01:49,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 24.008560180664062:  25%|██▌       | 15/60 [00:35<01:46,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 34.58156204223633:  27%|██▋       | 16/60 [00:38<01:43,  2.36s/it] \u001B[A\n",
      "episode_reward_mean = 39.256446838378906:  28%|██▊       | 17/60 [00:40<01:42,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 39.88832092285156:  30%|███       | 18/60 [00:43<01:39,  2.36s/it] \u001B[A\n",
      "episode_reward_mean = 48.2017707824707:  32%|███▏      | 19/60 [00:45<01:37,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 58.10808181762695:  33%|███▎      | 20/60 [00:47<01:34,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 58.34852981567383:  35%|███▌      | 21/60 [00:50<01:32,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 56.48604965209961:  37%|███▋      | 22/60 [00:52<01:29,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 57.333805084228516:  38%|███▊      | 23/60 [00:54<01:27,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 62.34663391113281:  40%|████      | 24/60 [00:57<01:24,  2.35s/it] \u001B[A\n",
      "episode_reward_mean = 58.165950775146484:  42%|████▏     | 25/60 [00:59<01:22,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 76.12738800048828:  43%|████▎     | 26/60 [01:02<01:21,  2.39s/it] \u001B[A\n",
      "episode_reward_mean = 72.39353942871094:  45%|████▌     | 27/60 [01:04<01:18,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 76.33374786376953:  47%|████▋     | 28/60 [01:06<01:16,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 82.26714324951172:  48%|████▊     | 29/60 [01:09<01:13,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 82.52552032470703:  50%|█████     | 30/60 [01:11<01:11,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 93.7596435546875:  52%|█████▏    | 31/60 [01:14<01:12,  2.49s/it] \u001B[A\n",
      "episode_reward_mean = 86.10592651367188:  53%|█████▎    | 32/60 [01:16<01:09,  2.47s/it]\u001B[A\n",
      "episode_reward_mean = 84.14620971679688:  55%|█████▌    | 33/60 [01:19<01:06,  2.46s/it]\u001B[A\n",
      "episode_reward_mean = 79.49111938476562:  57%|█████▋    | 34/60 [01:21<01:03,  2.45s/it]\u001B[A\n",
      "episode_reward_mean = 89.45420837402344:  58%|█████▊    | 35/60 [01:23<01:00,  2.43s/it]\u001B[A\n",
      "episode_reward_mean = 87.99964904785156:  60%|██████    | 36/60 [01:26<00:57,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = 95.14348602294922:  62%|██████▏   | 37/60 [01:28<00:55,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 87.69662475585938:  63%|██████▎   | 38/60 [01:31<00:52,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 90.85047912597656:  65%|██████▌   | 39/60 [01:33<00:50,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 84.58635711669922:  67%|██████▋   | 40/60 [01:35<00:48,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 97.09061431884766:  68%|██████▊   | 41/60 [01:38<00:45,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 85.72888946533203:  70%|███████   | 42/60 [01:40<00:42,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 88.18035888671875:  72%|███████▏  | 43/60 [01:42<00:40,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 89.39024353027344:  73%|███████▎  | 44/60 [01:45<00:37,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 87.2043228149414:  75%|███████▌  | 45/60 [01:47<00:35,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 92.10725402832031:  77%|███████▋  | 46/60 [01:50<00:33,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 95.2138900756836:  78%|███████▊  | 47/60 [01:52<00:31,  2.40s/it] \u001B[A\n",
      "episode_reward_mean = 83.78364562988281:  80%|████████  | 48/60 [01:55<00:29,  2.44s/it]\u001B[A\n",
      "episode_reward_mean = 96.8210678100586:  82%|████████▏ | 49/60 [01:57<00:26,  2.43s/it] \u001B[A\n",
      "episode_reward_mean = 87.3613510131836:  83%|████████▎ | 50/60 [01:59<00:24,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = 96.81890869140625:  85%|████████▌ | 51/60 [02:02<00:21,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 98.71736907958984:  87%|████████▋ | 52/60 [02:04<00:19,  2.47s/it]\u001B[A\n",
      "episode_reward_mean = 94.48450469970703:  88%|████████▊ | 53/60 [02:07<00:17,  2.44s/it]\u001B[A\n",
      "episode_reward_mean = 98.24903869628906:  90%|█████████ | 54/60 [02:09<00:14,  2.43s/it]\u001B[A\n",
      "episode_reward_mean = 89.75515747070312:  92%|█████████▏| 55/60 [02:12<00:12,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = 96.5991439819336:  93%|█████████▎| 56/60 [02:14<00:09,  2.41s/it] \u001B[A\n",
      "episode_reward_mean = 92.95713806152344:  95%|█████████▌| 57/60 [02:16<00:07,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 97.13629150390625:  97%|█████████▋| 58/60 [02:19<00:04,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 101.0206298828125:  98%|█████████▊| 59/60 [02:21<00:02,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 101.34992218017578: 100%|██████████| 60/60 [02:24<00:00,  2.40s/it]\u001B[A\n",
      "2025-08-20 21:09:47,830 [torchrl][INFO] Training time: 87.53 seconds\n",
      "2025-08-20 21:09:47,838 [torchrl][INFO] macs: 49.17 MMac  Params: 35.47 k\n",
      "2025-08-20 21:09:48,868 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:09:48,911 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -8.239269256591797:   2%|▏         | 1/60 [00:02<02:20,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = -9.69066047668457:   3%|▎         | 2/60 [00:04<02:18,  2.40s/it] \u001B[A\n",
      "episode_reward_mean = -8.700521469116211:   5%|▌         | 3/60 [00:07<02:16,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = -8.129488945007324:   7%|▋         | 4/60 [00:09<02:14,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = -4.785980701446533:   8%|▊         | 5/60 [00:11<02:11,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 2.1178934574127197:  10%|█         | 6/60 [00:14<02:09,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = -0.056863147765398026:  12%|█▏        | 7/60 [00:16<02:06,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = -0.9585573673248291:  13%|█▎        | 8/60 [00:19<02:04,  2.39s/it]  \u001B[A\n",
      "episode_reward_mean = 8.575212478637695:  15%|█▌        | 9/60 [00:21<02:01,  2.37s/it]  \u001B[A\n",
      "episode_reward_mean = 9.062591552734375:  17%|█▋        | 10/60 [00:23<01:59,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 10.845474243164062:  18%|█▊        | 11/60 [00:26<01:56,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 21.333942413330078:  20%|██        | 12/60 [00:28<01:54,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 30.3321475982666:  22%|██▏       | 13/60 [00:31<01:53,  2.41s/it]  \u001B[A\n",
      "episode_reward_mean = 31.100780487060547:  23%|██▎       | 14/60 [00:33<01:50,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 34.589717864990234:  25%|██▌       | 15/60 [00:35<01:47,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 34.969173431396484:  27%|██▋       | 16/60 [00:38<01:44,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 37.38642120361328:  28%|██▊       | 17/60 [00:40<01:42,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 34.01542663574219:  30%|███       | 18/60 [00:42<01:39,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 51.55986404418945:  32%|███▏      | 19/60 [00:45<01:36,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 43.89828872680664:  33%|███▎      | 20/60 [00:47<01:33,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 45.37966537475586:  35%|███▌      | 21/60 [00:49<01:31,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 57.06075668334961:  37%|███▋      | 22/60 [00:52<01:29,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 62.529014587402344:  38%|███▊      | 23/60 [00:54<01:26,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 62.1671142578125:  40%|████      | 24/60 [00:57<01:24,  2.36s/it]  \u001B[A\n",
      "episode_reward_mean = 63.85347366333008:  42%|████▏     | 25/60 [00:59<01:22,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 67.13262939453125:  43%|████▎     | 26/60 [01:01<01:20,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 76.89138793945312:  45%|████▌     | 27/60 [01:04<01:18,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 76.01778411865234:  47%|████▋     | 28/60 [01:06<01:15,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 83.85652923583984:  48%|████▊     | 29/60 [01:08<01:13,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 83.37652587890625:  50%|█████     | 30/60 [01:11<01:11,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 83.69579315185547:  52%|█████▏    | 31/60 [01:13<01:09,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 85.47996520996094:  53%|█████▎    | 32/60 [01:16<01:07,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 85.98783874511719:  55%|█████▌    | 33/60 [01:18<01:04,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 84.83873748779297:  57%|█████▋    | 34/60 [01:20<01:01,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 78.60774993896484:  58%|█████▊    | 35/60 [01:23<00:59,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 89.65569305419922:  60%|██████    | 36/60 [01:25<00:57,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 91.79833984375:  62%|██████▏   | 37/60 [01:27<00:54,  2.37s/it]   \u001B[A\n",
      "episode_reward_mean = 89.40239715576172:  63%|██████▎   | 38/60 [01:30<00:52,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 82.30553436279297:  65%|██████▌   | 39/60 [01:32<00:49,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 97.98670196533203:  67%|██████▋   | 40/60 [01:35<00:47,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 79.47550964355469:  68%|██████▊   | 41/60 [01:37<00:45,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 89.86833953857422:  70%|███████   | 42/60 [01:39<00:42,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 94.19824981689453:  72%|███████▏  | 43/60 [01:42<00:40,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 88.98719787597656:  73%|███████▎  | 44/60 [01:44<00:38,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 91.51441955566406:  75%|███████▌  | 45/60 [01:47<00:35,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 96.44470977783203:  77%|███████▋  | 46/60 [01:49<00:33,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 95.37926483154297:  78%|███████▊  | 47/60 [01:51<00:30,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 85.21949005126953:  80%|████████  | 48/60 [01:54<00:28,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 88.36937713623047:  82%|████████▏ | 49/60 [01:56<00:26,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 97.9857177734375:  83%|████████▎ | 50/60 [01:58<00:23,  2.39s/it] \u001B[A\n",
      "episode_reward_mean = 100.64909362792969:  85%|████████▌ | 51/60 [02:01<00:21,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = 102.59669494628906:  87%|████████▋ | 52/60 [02:03<00:19,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 89.3794174194336:  88%|████████▊ | 53/60 [02:06<00:16,  2.39s/it]  \u001B[A\n",
      "episode_reward_mean = 99.03099060058594:  90%|█████████ | 54/60 [02:08<00:14,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 98.71874237060547:  92%|█████████▏| 55/60 [02:10<00:11,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 91.42645263671875:  93%|█████████▎| 56/60 [02:13<00:09,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 97.30728149414062:  95%|█████████▌| 57/60 [02:15<00:07,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 98.59870910644531:  97%|█████████▋| 58/60 [02:18<00:04,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 98.73006439208984:  98%|█████████▊| 59/60 [02:20<00:02,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 97.476806640625: 100%|██████████| 60/60 [02:22<00:00,  2.38s/it]  \u001B[A\n",
      "2025-08-20 21:12:11,829 [torchrl][INFO] Training time: 86.48 seconds\n",
      "2025-08-20 21:12:11,837 [torchrl][INFO] macs: 49.17 MMac  Params: 35.47 k\n",
      "2025-08-20 21:12:12,913 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:12:12,957 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -10.670211791992188:   2%|▏         | 1/60 [00:02<02:20,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = -9.418357849121094:   3%|▎         | 2/60 [00:04<02:16,  2.36s/it] \u001B[A\n",
      "episode_reward_mean = -9.259900093078613:   5%|▌         | 3/60 [00:07<02:15,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = -7.2909836769104:   7%|▋         | 4/60 [00:09<02:13,  2.38s/it]  \u001B[A\n",
      "episode_reward_mean = -5.39157247543335:   8%|▊         | 5/60 [00:11<02:10,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = -5.51809024810791:  10%|█         | 6/60 [00:14<02:08,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = -6.280462741851807:  12%|█▏        | 7/60 [00:16<02:05,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = -1.580206036567688:  13%|█▎        | 8/60 [00:19<02:04,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = -4.96276330947876:  15%|█▌        | 9/60 [00:21<02:01,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 4.129414081573486:  17%|█▋        | 10/60 [00:23<01:59,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 2.989783525466919:  18%|█▊        | 11/60 [00:26<01:56,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 9.31346607208252:  20%|██        | 12/60 [00:28<01:53,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 13.529024124145508:  22%|██▏       | 13/60 [00:30<01:51,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 15.86274242401123:  23%|██▎       | 14/60 [00:33<01:49,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 10.988154411315918:  25%|██▌       | 15/60 [00:35<01:46,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 12.637870788574219:  27%|██▋       | 16/60 [00:38<01:44,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 15.99720287322998:  28%|██▊       | 17/60 [00:40<01:41,  2.36s/it] \u001B[A\n",
      "episode_reward_mean = 24.38890838623047:  30%|███       | 18/60 [00:42<01:39,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 33.0990104675293:  32%|███▏      | 19/60 [00:45<01:37,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 37.487403869628906:  33%|███▎      | 20/60 [00:47<01:35,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 45.06883239746094:  35%|███▌      | 21/60 [00:49<01:32,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 41.84573745727539:  37%|███▋      | 22/60 [00:52<01:30,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 44.2322883605957:  38%|███▊      | 23/60 [00:54<01:27,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 54.767913818359375:  40%|████      | 24/60 [00:56<01:25,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 45.3007926940918:  42%|████▏     | 25/60 [00:59<01:22,  2.37s/it]  \u001B[A\n",
      "episode_reward_mean = 63.75850296020508:  43%|████▎     | 26/60 [01:01<01:20,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 61.3818359375:  45%|████▌     | 27/60 [01:04<01:18,  2.38s/it]    \u001B[A\n",
      "episode_reward_mean = 60.19131088256836:  47%|████▋     | 28/60 [01:06<01:16,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 69.11186218261719:  48%|████▊     | 29/60 [01:08<01:13,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 74.99620056152344:  50%|█████     | 30/60 [01:11<01:11,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 70.57787322998047:  52%|█████▏    | 31/60 [01:13<01:09,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 79.79635620117188:  53%|█████▎    | 32/60 [01:16<01:06,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 79.55940246582031:  55%|█████▌    | 33/60 [01:18<01:03,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 77.82398986816406:  57%|█████▋    | 34/60 [01:20<01:01,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 86.42488861083984:  58%|█████▊    | 35/60 [01:23<00:59,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 90.75496673583984:  60%|██████    | 36/60 [01:25<00:57,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 83.32493591308594:  62%|██████▏   | 37/60 [01:27<00:54,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 88.45548248291016:  63%|██████▎   | 38/60 [01:30<00:52,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 83.53631591796875:  65%|██████▌   | 39/60 [01:32<00:50,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 87.96920776367188:  67%|██████▋   | 40/60 [01:35<00:48,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 86.96158599853516:  68%|██████▊   | 41/60 [01:37<00:45,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 91.9917221069336:  70%|███████   | 42/60 [01:39<00:43,  2.40s/it] \u001B[A\n",
      "episode_reward_mean = 91.16234588623047:  72%|███████▏  | 43/60 [01:42<00:41,  2.43s/it]\u001B[A\n",
      "episode_reward_mean = 97.1578140258789:  73%|███████▎  | 44/60 [01:44<00:38,  2.42s/it] \u001B[A\n",
      "episode_reward_mean = 87.69374084472656:  75%|███████▌  | 45/60 [01:47<00:36,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = 90.04351806640625:  77%|███████▋  | 46/60 [01:49<00:33,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 94.69468688964844:  78%|███████▊  | 47/60 [01:52<00:31,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 94.58626556396484:  80%|████████  | 48/60 [01:54<00:28,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 97.27957916259766:  82%|████████▏ | 49/60 [01:56<00:26,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 94.07274627685547:  83%|████████▎ | 50/60 [01:59<00:24,  2.46s/it]\u001B[A\n",
      "episode_reward_mean = 95.08635711669922:  85%|████████▌ | 51/60 [02:01<00:22,  2.45s/it]\u001B[A\n",
      "episode_reward_mean = 96.44892883300781:  87%|████████▋ | 52/60 [02:04<00:19,  2.43s/it]\u001B[A\n",
      "episode_reward_mean = 96.11368560791016:  88%|████████▊ | 53/60 [02:06<00:16,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 97.1190185546875:  90%|█████████ | 54/60 [02:08<00:14,  2.40s/it] \u001B[A\n",
      "episode_reward_mean = 98.96629333496094:  92%|█████████▏| 55/60 [02:11<00:12,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 94.62135314941406:  93%|█████████▎| 56/60 [02:13<00:09,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 94.68968963623047:  95%|█████████▌| 57/60 [02:16<00:07,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 91.45863342285156:  97%|█████████▋| 58/60 [02:18<00:04,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 99.47672271728516:  98%|█████████▊| 59/60 [02:20<00:02,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 83.07980346679688: 100%|██████████| 60/60 [02:23<00:00,  2.39s/it]\u001B[A\n",
      "2025-08-20 21:14:36,302 [torchrl][INFO] Training time: 86.89 seconds\n",
      "2025-08-20 21:14:36,309 [torchrl][INFO] macs: 49.17 MMac  Params: 35.47 k\n",
      "2025-08-20 21:14:37,442 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:14:37,484 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -7.536374568939209:   2%|▏         | 1/60 [00:02<02:19,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = -5.462067127227783:   3%|▎         | 2/60 [00:04<02:15,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = -6.315023422241211:   5%|▌         | 3/60 [00:07<02:13,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = -8.375170707702637:   7%|▋         | 4/60 [00:09<02:11,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = -9.017931938171387:   8%|▊         | 5/60 [00:11<02:09,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = -7.296924591064453:  10%|█         | 6/60 [00:14<02:07,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = -6.457808971405029:  12%|█▏        | 7/60 [00:16<02:05,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = -5.481818199157715:  13%|█▎        | 8/60 [00:18<02:04,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 1.3591879606246948:  15%|█▌        | 9/60 [00:21<02:01,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 4.343045711517334:  17%|█▋        | 10/60 [00:23<01:57,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 9.718316078186035:  18%|█▊        | 11/60 [00:25<01:54,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 15.498476028442383:  20%|██        | 12/60 [00:28<01:52,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 18.09011459350586:  22%|██▏       | 13/60 [00:30<01:50,  2.35s/it] \u001B[A\n",
      "episode_reward_mean = 15.2322998046875:  23%|██▎       | 14/60 [00:32<01:48,  2.36s/it] \u001B[A\n",
      "episode_reward_mean = 22.64630889892578:  25%|██▌       | 15/60 [00:35<01:45,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 26.86468505859375:  27%|██▋       | 16/60 [00:37<01:43,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 36.744564056396484:  28%|██▊       | 17/60 [00:39<01:40,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 31.00775909423828:  30%|███       | 18/60 [00:42<01:38,  2.35s/it] \u001B[A\n",
      "episode_reward_mean = 34.72414779663086:  32%|███▏      | 19/60 [00:44<01:36,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 31.909772872924805:  33%|███▎      | 20/60 [00:47<01:34,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 32.58442687988281:  35%|███▌      | 21/60 [00:49<01:33,  2.40s/it] \u001B[A\n",
      "episode_reward_mean = 39.811832427978516:  37%|███▋      | 22/60 [00:51<01:29,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 39.65208053588867:  38%|███▊      | 23/60 [00:54<01:27,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 42.073509216308594:  40%|████      | 24/60 [00:56<01:25,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 45.12400817871094:  42%|████▏     | 25/60 [00:58<01:22,  2.35s/it] \u001B[A\n",
      "episode_reward_mean = 55.830352783203125:  43%|████▎     | 26/60 [01:01<01:19,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 54.736385345458984:  45%|████▌     | 27/60 [01:03<01:17,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 58.54865646362305:  47%|████▋     | 28/60 [01:06<01:15,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 56.325443267822266:  48%|████▊     | 29/60 [01:08<01:13,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 67.47868347167969:  50%|█████     | 30/60 [01:10<01:10,  2.36s/it] \u001B[A\n",
      "episode_reward_mean = 77.85997772216797:  52%|█████▏    | 31/60 [01:13<01:08,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 68.67733001708984:  53%|█████▎    | 32/60 [01:15<01:06,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 77.02734375:  55%|█████▌    | 33/60 [01:17<01:03,  2.37s/it]      \u001B[A\n",
      "episode_reward_mean = 75.41771697998047:  57%|█████▋    | 34/60 [01:20<01:01,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 81.7190170288086:  58%|█████▊    | 35/60 [01:22<00:59,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 86.01374816894531:  60%|██████    | 36/60 [01:25<00:57,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 82.07633209228516:  62%|██████▏   | 37/60 [01:27<00:55,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 90.35926055908203:  63%|██████▎   | 38/60 [01:29<00:52,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 80.26139068603516:  65%|██████▌   | 39/60 [01:32<00:50,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 83.502685546875:  67%|██████▋   | 40/60 [01:34<00:47,  2.40s/it]  \u001B[A\n",
      "episode_reward_mean = 91.09295654296875:  68%|██████▊   | 41/60 [01:37<00:45,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 96.8084487915039:  70%|███████   | 42/60 [01:39<00:43,  2.42s/it] \u001B[A\n",
      "episode_reward_mean = 88.11212158203125:  72%|███████▏  | 43/60 [01:41<00:41,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = 94.06739044189453:  73%|███████▎  | 44/60 [01:44<00:38,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 97.06520080566406:  75%|███████▌  | 45/60 [01:46<00:35,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 96.30723571777344:  77%|███████▋  | 46/60 [01:48<00:33,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 89.4382095336914:  78%|███████▊  | 47/60 [01:51<00:30,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 90.41470336914062:  80%|████████  | 48/60 [01:53<00:28,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 97.05545043945312:  82%|████████▏ | 49/60 [01:56<00:26,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 98.93948364257812:  83%|████████▎ | 50/60 [01:58<00:23,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 97.87972259521484:  85%|████████▌ | 51/60 [02:00<00:21,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 93.85326385498047:  87%|████████▋ | 52/60 [02:03<00:19,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 100.36673736572266:  88%|████████▊ | 53/60 [02:05<00:16,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 101.41490936279297:  90%|█████████ | 54/60 [02:08<00:14,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 93.73514556884766:  92%|█████████▏| 55/60 [02:10<00:11,  2.39s/it] \u001B[A\n",
      "episode_reward_mean = 94.16655731201172:  93%|█████████▎| 56/60 [02:12<00:09,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 99.34517669677734:  95%|█████████▌| 57/60 [02:15<00:07,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 94.08360290527344:  97%|█████████▋| 58/60 [02:17<00:04,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 100.71198272705078:  98%|█████████▊| 59/60 [02:20<00:02,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 106.90059661865234: 100%|██████████| 60/60 [02:22<00:00,  2.37s/it]\u001B[A\n",
      "2025-08-20 21:16:59,899 [torchrl][INFO] Training time: 86.08 seconds\n",
      "2025-08-20 21:16:59,906 [torchrl][INFO] macs: 49.17 MMac  Params: 35.47 k\n",
      "2025-08-20 21:17:00,957 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:17:01,000 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -10.667377471923828:   2%|▏         | 1/60 [00:02<02:22,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = -10.98647689819336:   3%|▎         | 2/60 [00:04<02:22,  2.46s/it] \u001B[A\n",
      "episode_reward_mean = -11.218174934387207:   5%|▌         | 3/60 [00:07<02:17,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = -8.982012748718262:   7%|▋         | 4/60 [00:09<02:14,  2.41s/it] \u001B[A\n",
      "episode_reward_mean = -6.721872329711914:   8%|▊         | 5/60 [00:12<02:12,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = -6.34881067276001:  10%|█         | 6/60 [00:14<02:11,  2.44s/it] \u001B[A\n",
      "episode_reward_mean = -5.622243404388428:  12%|█▏        | 7/60 [00:16<02:07,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = -3.843628168106079:  13%|█▎        | 8/60 [00:19<02:04,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = -2.894314765930176:  15%|█▌        | 9/60 [00:21<02:01,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 0.889472246170044:  17%|█▋        | 10/60 [00:24<01:58,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 3.8781778812408447:  18%|█▊        | 11/60 [00:26<01:55,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 8.335015296936035:  20%|██        | 12/60 [00:28<01:53,  2.36s/it] \u001B[A\n",
      "episode_reward_mean = 9.724576950073242:  22%|██▏       | 13/60 [00:31<01:52,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 13.581687927246094:  23%|██▎       | 14/60 [00:33<01:50,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 14.707720756530762:  25%|██▌       | 15/60 [00:35<01:47,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 24.901456832885742:  27%|██▋       | 16/60 [00:38<01:46,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 23.849285125732422:  28%|██▊       | 17/60 [00:40<01:43,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 25.008188247680664:  30%|███       | 18/60 [00:43<01:41,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 34.02748489379883:  32%|███▏      | 19/60 [00:45<01:39,  2.42s/it] \u001B[A\n",
      "episode_reward_mean = 34.99388885498047:  33%|███▎      | 20/60 [00:48<01:36,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 35.579681396484375:  35%|███▌      | 21/60 [00:50<01:33,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 40.315589904785156:  37%|███▋      | 22/60 [00:52<01:31,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 44.98823547363281:  38%|███▊      | 23/60 [00:55<01:29,  2.41s/it] \u001B[A\n",
      "episode_reward_mean = 39.067474365234375:  40%|████      | 24/60 [00:57<01:27,  2.43s/it]\u001B[A\n",
      "episode_reward_mean = 52.0714111328125:  42%|████▏     | 25/60 [01:00<01:24,  2.41s/it]  \u001B[A\n",
      "episode_reward_mean = 58.43747329711914:  43%|████▎     | 26/60 [01:02<01:23,  2.46s/it]\u001B[A\n",
      "episode_reward_mean = 65.69982147216797:  45%|████▌     | 27/60 [01:05<01:20,  2.44s/it]\u001B[A\n",
      "episode_reward_mean = 68.59501647949219:  47%|████▋     | 28/60 [01:07<01:18,  2.44s/it]\u001B[A\n",
      "episode_reward_mean = 74.72249603271484:  48%|████▊     | 29/60 [01:09<01:15,  2.44s/it]\u001B[A\n",
      "episode_reward_mean = 81.16812133789062:  50%|█████     | 30/60 [01:12<01:13,  2.44s/it]\u001B[A\n",
      "episode_reward_mean = 83.53305053710938:  52%|█████▏    | 31/60 [01:14<01:10,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = 80.83827209472656:  53%|█████▎    | 32/60 [01:17<01:07,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 77.28303527832031:  55%|█████▌    | 33/60 [01:19<01:05,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = 80.72409057617188:  57%|█████▋    | 34/60 [01:22<01:02,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = 82.4569091796875:  58%|█████▊    | 35/60 [01:24<01:00,  2.41s/it] \u001B[A\n",
      "episode_reward_mean = 93.17497253417969:  60%|██████    | 36/60 [01:26<00:57,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 90.70997619628906:  62%|██████▏   | 37/60 [01:29<00:55,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 93.47731018066406:  63%|██████▎   | 38/60 [01:31<00:53,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 90.63870239257812:  65%|██████▌   | 39/60 [01:34<00:50,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 87.48789978027344:  67%|██████▋   | 40/60 [01:36<00:48,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 90.0263900756836:  68%|██████▊   | 41/60 [01:38<00:45,  2.41s/it] \u001B[A\n",
      "episode_reward_mean = 91.65460968017578:  70%|███████   | 42/60 [01:41<00:43,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 93.32759094238281:  72%|███████▏  | 43/60 [01:43<00:40,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 102.04643249511719:  73%|███████▎  | 44/60 [01:46<00:38,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 95.34398651123047:  75%|███████▌  | 45/60 [01:48<00:36,  2.42s/it] \u001B[A\n",
      "episode_reward_mean = 92.07182312011719:  77%|███████▋  | 46/60 [01:51<00:34,  2.43s/it]\u001B[A\n",
      "episode_reward_mean = 89.75650024414062:  78%|███████▊  | 47/60 [01:53<00:31,  2.43s/it]\u001B[A\n",
      "episode_reward_mean = 103.98888397216797:  80%|████████  | 48/60 [01:55<00:29,  2.43s/it]\u001B[A\n",
      "episode_reward_mean = 104.57250213623047:  82%|████████▏ | 49/60 [01:58<00:26,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 93.19940185546875:  83%|████████▎ | 50/60 [02:00<00:24,  2.41s/it] \u001B[A\n",
      "episode_reward_mean = 92.64791107177734:  85%|████████▌ | 51/60 [02:03<00:21,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 108.23762512207031:  87%|████████▋ | 52/60 [02:05<00:19,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 97.22696685791016:  88%|████████▊ | 53/60 [02:07<00:16,  2.41s/it] \u001B[A\n",
      "episode_reward_mean = 94.51315307617188:  90%|█████████ | 54/60 [02:10<00:14,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 100.64765167236328:  92%|█████████▏| 55/60 [02:12<00:12,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = 98.8416519165039:  93%|█████████▎| 56/60 [02:15<00:09,  2.46s/it]  \u001B[A\n",
      "episode_reward_mean = 98.08158874511719:  95%|█████████▌| 57/60 [02:17<00:07,  2.45s/it]\u001B[A\n",
      "episode_reward_mean = 103.57047271728516:  97%|█████████▋| 58/60 [02:20<00:04,  2.44s/it]\u001B[A\n",
      "episode_reward_mean = 92.60414123535156:  98%|█████████▊| 59/60 [02:22<00:02,  2.43s/it] \u001B[A\n",
      "episode_reward_mean = 98.15936279296875: 100%|██████████| 60/60 [02:24<00:00,  2.42s/it]\u001B[A\n",
      "2025-08-20 21:19:25,940 [torchrl][INFO] Training time: 87.69 seconds\n",
      "2025-08-20 21:19:25,947 [torchrl][INFO] macs: 49.17 MMac  Params: 35.47 k\n",
      "2025-08-20 21:19:27,026 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:19:27,070 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -11.698554039001465:   2%|▏         | 1/60 [00:02<02:22,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = -12.06899642944336:   3%|▎         | 2/60 [00:05<02:28,  2.56s/it] \u001B[A\n",
      "episode_reward_mean = -11.279878616333008:   5%|▌         | 3/60 [00:07<02:20,  2.47s/it]\u001B[A\n",
      "episode_reward_mean = -10.624759674072266:   7%|▋         | 4/60 [00:09<02:17,  2.45s/it]\u001B[A\n",
      "episode_reward_mean = -11.143845558166504:   8%|▊         | 5/60 [00:12<02:12,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = -8.968962669372559:  10%|█         | 6/60 [00:14<02:10,  2.42s/it] \u001B[A\n",
      "episode_reward_mean = -8.7359037399292:  12%|█▏        | 7/60 [00:17<02:07,  2.41s/it]  \u001B[A\n",
      "episode_reward_mean = -8.191530227661133:  13%|█▎        | 8/60 [00:19<02:05,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = -8.685627937316895:  15%|█▌        | 9/60 [00:21<02:02,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = -8.578569412231445:  17%|█▋        | 10/60 [00:24<01:59,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = -8.812201499938965:  18%|█▊        | 11/60 [00:26<01:57,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = -8.63463020324707:  20%|██        | 12/60 [00:28<01:54,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = -7.940996170043945:  22%|██▏       | 13/60 [00:31<01:51,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = -7.741209030151367:  23%|██▎       | 14/60 [00:33<01:49,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = -5.869670867919922:  25%|██▌       | 15/60 [00:36<01:47,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = -3.296809434890747:  27%|██▋       | 16/60 [00:38<01:45,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = -0.6636946797370911:  28%|██▊       | 17/60 [00:40<01:42,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 1.8169597387313843:  30%|███       | 18/60 [00:43<01:39,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 6.420278549194336:  32%|███▏      | 19/60 [00:45<01:37,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 12.566810607910156:  33%|███▎      | 20/60 [00:47<01:35,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 13.148920059204102:  35%|███▌      | 21/60 [00:50<01:31,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 19.511972427368164:  37%|███▋      | 22/60 [00:52<01:30,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 15.407017707824707:  38%|███▊      | 23/60 [00:55<01:28,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 18.601293563842773:  40%|████      | 24/60 [00:57<01:25,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 25.145580291748047:  42%|████▏     | 25/60 [00:59<01:22,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 21.523284912109375:  43%|████▎     | 26/60 [01:02<01:20,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 21.432519912719727:  45%|████▌     | 27/60 [01:04<01:17,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 27.332115173339844:  47%|████▋     | 28/60 [01:06<01:15,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 32.48713302612305:  48%|████▊     | 29/60 [01:09<01:13,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 30.284574508666992:  50%|█████     | 30/60 [01:11<01:11,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 34.8541259765625:  52%|█████▏    | 31/60 [01:14<01:09,  2.40s/it]  \u001B[A\n",
      "episode_reward_mean = 29.717016220092773:  53%|█████▎    | 32/60 [01:16<01:07,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 30.902658462524414:  55%|█████▌    | 33/60 [01:18<01:04,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 37.925899505615234:  57%|█████▋    | 34/60 [01:21<01:01,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 43.97496032714844:  58%|█████▊    | 35/60 [01:23<00:59,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 41.7283821105957:  60%|██████    | 36/60 [01:25<00:56,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 43.15818786621094:  62%|██████▏   | 37/60 [01:28<00:54,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 46.620033264160156:  63%|██████▎   | 38/60 [01:30<00:52,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 51.710296630859375:  65%|██████▌   | 39/60 [01:33<00:49,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 53.79524230957031:  67%|██████▋   | 40/60 [01:35<00:47,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 51.507694244384766:  68%|██████▊   | 41/60 [01:38<00:46,  2.44s/it]\u001B[A\n",
      "episode_reward_mean = 49.21794509887695:  70%|███████   | 42/60 [01:40<00:43,  2.43s/it] \u001B[A\n",
      "episode_reward_mean = 55.58742904663086:  72%|███████▏  | 43/60 [01:42<00:41,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = 59.943504333496094:  73%|███████▎  | 44/60 [01:45<00:38,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 59.15217971801758:  75%|███████▌  | 45/60 [01:47<00:36,  2.41s/it] \u001B[A\n",
      "episode_reward_mean = 71.08319091796875:  77%|███████▋  | 46/60 [01:50<00:33,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 63.21186065673828:  78%|███████▊  | 47/60 [01:52<00:31,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 69.26528930664062:  80%|████████  | 48/60 [01:54<00:28,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 70.964111328125:  82%|████████▏ | 49/60 [01:57<00:26,  2.38s/it]  \u001B[A\n",
      "episode_reward_mean = 77.32611083984375:  83%|████████▎ | 50/60 [01:59<00:23,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 75.59400177001953:  85%|████████▌ | 51/60 [02:01<00:21,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 84.38159942626953:  87%|████████▋ | 52/60 [02:04<00:19,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 82.17974090576172:  88%|████████▊ | 53/60 [02:06<00:16,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 81.89122772216797:  90%|█████████ | 54/60 [02:09<00:14,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 80.40518188476562:  92%|█████████▏| 55/60 [02:11<00:12,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 83.60186767578125:  93%|█████████▎| 56/60 [02:13<00:09,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 89.04293060302734:  95%|█████████▌| 57/60 [02:16<00:07,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 86.26905059814453:  97%|█████████▋| 58/60 [02:18<00:04,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 96.9696273803711:  98%|█████████▊| 59/60 [02:21<00:02,  2.40s/it] \u001B[A\n",
      "episode_reward_mean = 88.63905334472656: 100%|██████████| 60/60 [02:23<00:00,  2.39s/it]\u001B[A\n",
      "2025-08-20 21:21:50,623 [torchrl][INFO] Training time: 87.33 seconds\n",
      "2025-08-20 21:21:50,631 [torchrl][INFO] macs: 49.17 MMac  Params: 35.47 k\n",
      "2025-08-20 21:21:51,829 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:21:51,873 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -12.083422660827637:   2%|▏         | 1/60 [00:02<02:20,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = -9.808134078979492:   3%|▎         | 2/60 [00:04<02:19,  2.41s/it] \u001B[A\n",
      "episode_reward_mean = -8.394536018371582:   5%|▌         | 3/60 [00:07<02:17,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = -8.467168807983398:   7%|▋         | 4/60 [00:09<02:14,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = -8.104509353637695:   8%|▊         | 5/60 [00:11<02:11,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = -6.0460405349731445:  10%|█         | 6/60 [00:14<02:08,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = -6.3856682777404785:  12%|█▏        | 7/60 [00:16<02:06,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = -6.058525562286377:  13%|█▎        | 8/60 [00:19<02:03,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = -5.08981466293335:  15%|█▌        | 9/60 [00:21<02:00,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 0.3112702965736389:  17%|█▋        | 10/60 [00:23<01:59,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 4.00459098815918:  18%|█▊        | 11/60 [00:26<01:57,  2.39s/it]  \u001B[A\n",
      "episode_reward_mean = 9.368474960327148:  20%|██        | 12/60 [00:28<01:55,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 11.607309341430664:  22%|██▏       | 13/60 [00:31<01:52,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 13.802779197692871:  23%|██▎       | 14/60 [00:33<01:49,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 25.35347557067871:  25%|██▌       | 15/60 [00:35<01:47,  2.39s/it] \u001B[A\n",
      "episode_reward_mean = 27.583520889282227:  27%|██▋       | 16/60 [00:38<01:44,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 21.650657653808594:  28%|██▊       | 17/60 [00:40<01:41,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 32.782875061035156:  30%|███       | 18/60 [00:43<01:41,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 36.68520736694336:  32%|███▏      | 19/60 [00:45<01:38,  2.39s/it] \u001B[A\n",
      "episode_reward_mean = 44.527183532714844:  33%|███▎      | 20/60 [00:47<01:35,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 51.363739013671875:  35%|███▌      | 21/60 [00:50<01:33,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 51.21383285522461:  37%|███▋      | 22/60 [00:52<01:30,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 53.83285903930664:  38%|███▊      | 23/60 [00:54<01:27,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 57.505977630615234:  40%|████      | 24/60 [00:57<01:25,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 62.749446868896484:  42%|████▏     | 25/60 [00:59<01:22,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 60.83599090576172:  43%|████▎     | 26/60 [01:02<01:20,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 64.29651641845703:  45%|████▌     | 27/60 [01:04<01:18,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 68.6543960571289:  47%|████▋     | 28/60 [01:06<01:16,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 78.95062255859375:  48%|████▊     | 29/60 [01:09<01:13,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 87.39281463623047:  50%|█████     | 30/60 [01:11<01:11,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 71.2632827758789:  52%|█████▏    | 31/60 [01:13<01:09,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 83.53255462646484:  53%|█████▎    | 32/60 [01:16<01:06,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 84.91805267333984:  55%|█████▌    | 33/60 [01:18<01:04,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 88.75416564941406:  57%|█████▋    | 34/60 [01:21<01:02,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 86.3671875:  58%|█████▊    | 35/60 [01:23<00:59,  2.40s/it]       \u001B[A\n",
      "episode_reward_mean = 80.48393249511719:  60%|██████    | 36/60 [01:25<00:57,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 87.86053466796875:  62%|██████▏   | 37/60 [01:28<00:55,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 89.2484359741211:  63%|██████▎   | 38/60 [01:30<00:52,  2.40s/it] \u001B[A\n",
      "episode_reward_mean = 80.76380920410156:  65%|██████▌   | 39/60 [01:33<00:50,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 87.053466796875:  67%|██████▋   | 40/60 [01:35<00:47,  2.39s/it]  \u001B[A\n",
      "episode_reward_mean = 86.69206237792969:  68%|██████▊   | 41/60 [01:37<00:45,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 91.28683471679688:  70%|███████   | 42/60 [01:40<00:43,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 87.30810546875:  72%|███████▏  | 43/60 [01:42<00:40,  2.38s/it]   \u001B[A\n",
      "episode_reward_mean = 92.22785186767578:  73%|███████▎  | 44/60 [01:45<00:38,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 92.74531555175781:  75%|███████▌  | 45/60 [01:47<00:35,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 89.7633056640625:  77%|███████▋  | 46/60 [01:49<00:33,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 92.41212463378906:  78%|███████▊  | 47/60 [01:52<00:30,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 95.06254577636719:  80%|████████  | 48/60 [01:54<00:28,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 83.83904266357422:  82%|████████▏ | 49/60 [01:56<00:26,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 92.107177734375:  83%|████████▎ | 50/60 [01:59<00:23,  2.38s/it]  \u001B[A\n",
      "episode_reward_mean = 96.46797180175781:  85%|████████▌ | 51/60 [02:01<00:21,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 86.34764862060547:  87%|████████▋ | 52/60 [02:04<00:19,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 92.56298828125:  88%|████████▊ | 53/60 [02:06<00:16,  2.38s/it]   \u001B[A\n",
      "episode_reward_mean = 89.54346466064453:  90%|█████████ | 54/60 [02:08<00:14,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 90.22307586669922:  92%|█████████▏| 55/60 [02:11<00:11,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 84.2213363647461:  93%|█████████▎| 56/60 [02:13<00:09,  2.45s/it] \u001B[A\n",
      "episode_reward_mean = 82.51068878173828:  95%|█████████▌| 57/60 [02:16<00:07,  2.44s/it]\u001B[A\n",
      "episode_reward_mean = 97.1402816772461:  97%|█████████▋| 58/60 [02:18<00:04,  2.42s/it] \u001B[A\n",
      "episode_reward_mean = 93.10021209716797:  98%|█████████▊| 59/60 [02:20<00:02,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 97.17877960205078: 100%|██████████| 60/60 [02:23<00:00,  2.39s/it]\u001B[A\n",
      "2025-08-20 21:24:15,302 [torchrl][INFO] Training time: 86.97 seconds\n",
      "2025-08-20 21:24:15,311 [torchrl][INFO] macs: 49.17 MMac  Params: 35.47 k\n",
      "2025-08-20 21:24:16,373 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:24:16,417 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -13.117353439331055:   2%|▏         | 1/60 [00:02<02:24,  2.44s/it]\u001B[A\n",
      "episode_reward_mean = -12.815777778625488:   3%|▎         | 2/60 [00:04<02:20,  2.43s/it]\u001B[A\n",
      "episode_reward_mean = -11.118404388427734:   5%|▌         | 3/60 [00:07<02:18,  2.43s/it]\u001B[A\n",
      "episode_reward_mean = -8.784100532531738:   7%|▋         | 4/60 [00:09<02:16,  2.45s/it] \u001B[A\n",
      "episode_reward_mean = -6.027981281280518:   8%|▊         | 5/60 [00:12<02:13,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = -4.131792068481445:  10%|█         | 6/60 [00:14<02:10,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = -7.002857208251953:  12%|█▏        | 7/60 [00:16<02:07,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = -6.18337869644165:  13%|█▎        | 8/60 [00:19<02:03,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = -4.597236156463623:  15%|█▌        | 9/60 [00:21<02:00,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = -5.005834102630615:  17%|█▋        | 10/60 [00:23<01:58,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = -6.448346138000488:  18%|█▊        | 11/60 [00:26<01:56,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = -1.5590689182281494:  20%|██        | 12/60 [00:28<01:54,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 2.1188905239105225:  22%|██▏       | 13/60 [00:31<01:52,  2.40s/it] \u001B[A\n",
      "episode_reward_mean = 5.635992050170898:  23%|██▎       | 14/60 [00:33<01:50,  2.39s/it] \u001B[A\n",
      "episode_reward_mean = 9.924379348754883:  25%|██▌       | 15/60 [00:35<01:47,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 11.781349182128906:  27%|██▋       | 16/60 [00:38<01:44,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 14.993528366088867:  28%|██▊       | 17/60 [00:40<01:41,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 18.923358917236328:  30%|███       | 18/60 [00:43<01:40,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 23.0692195892334:  32%|███▏      | 19/60 [00:45<01:37,  2.37s/it]  \u001B[A\n",
      "episode_reward_mean = 23.359716415405273:  33%|███▎      | 20/60 [00:47<01:35,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 26.26276397705078:  35%|███▌      | 21/60 [00:50<01:32,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 38.819515228271484:  37%|███▋      | 22/60 [00:52<01:30,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 31.21491050720215:  38%|███▊      | 23/60 [00:54<01:27,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 40.88712692260742:  40%|████      | 24/60 [00:57<01:25,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 35.516990661621094:  42%|████▏     | 25/60 [00:59<01:23,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 48.40397644042969:  43%|████▎     | 26/60 [01:02<01:21,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 57.71681594848633:  45%|████▌     | 27/60 [01:04<01:18,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 60.08979034423828:  47%|████▋     | 28/60 [01:06<01:16,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 58.572845458984375:  48%|████▊     | 29/60 [01:09<01:13,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 72.4868392944336:  50%|█████     | 30/60 [01:11<01:11,  2.38s/it]  \u001B[A\n",
      "episode_reward_mean = 67.24725341796875:  52%|█████▏    | 31/60 [01:13<01:08,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 74.00392150878906:  53%|█████▎    | 32/60 [01:16<01:07,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 70.34732818603516:  55%|█████▌    | 33/60 [01:18<01:04,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 78.5931396484375:  57%|█████▋    | 34/60 [01:21<01:02,  2.39s/it] \u001B[A\n",
      "episode_reward_mean = 81.16259002685547:  58%|█████▊    | 35/60 [01:23<00:59,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 80.1850357055664:  60%|██████    | 36/60 [01:25<00:57,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 82.55154418945312:  62%|██████▏   | 37/60 [01:28<00:54,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 89.58748626708984:  63%|██████▎   | 38/60 [01:30<00:51,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 85.73230743408203:  65%|██████▌   | 39/60 [01:33<00:49,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 86.20073699951172:  67%|██████▋   | 40/60 [01:35<00:47,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 87.23108673095703:  68%|██████▊   | 41/60 [01:37<00:45,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 85.54143524169922:  70%|███████   | 42/60 [01:40<00:42,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 92.24109649658203:  72%|███████▏  | 43/60 [01:42<00:40,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 92.61307525634766:  73%|███████▎  | 44/60 [01:44<00:37,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 87.27486419677734:  75%|███████▌  | 45/60 [01:47<00:35,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 88.02704620361328:  77%|███████▋  | 46/60 [01:49<00:33,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 93.43477630615234:  78%|███████▊  | 47/60 [01:52<00:30,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 90.93827056884766:  80%|████████  | 48/60 [01:54<00:28,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 91.3691635131836:  82%|████████▏ | 49/60 [01:56<00:26,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 99.98043823242188:  83%|████████▎ | 50/60 [01:59<00:23,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 90.86097717285156:  85%|████████▌ | 51/60 [02:01<00:21,  2.43s/it]\u001B[A\n",
      "episode_reward_mean = 95.65450286865234:  87%|████████▋ | 52/60 [02:04<00:19,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 89.81864166259766:  88%|████████▊ | 53/60 [02:06<00:16,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 94.03595733642578:  90%|█████████ | 54/60 [02:08<00:14,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 96.55601501464844:  92%|█████████▏| 55/60 [02:11<00:11,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 93.30517578125:  93%|█████████▎| 56/60 [02:13<00:09,  2.36s/it]   \u001B[A\n",
      "episode_reward_mean = 104.20893096923828:  95%|█████████▌| 57/60 [02:15<00:07,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 98.78492736816406:  97%|█████████▋| 58/60 [02:18<00:04,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 96.00164794921875:  98%|█████████▊| 59/60 [02:20<00:02,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 100.73185729980469: 100%|██████████| 60/60 [02:23<00:00,  2.38s/it]\u001B[A\n",
      "2025-08-20 21:26:39,448 [torchrl][INFO] Training time: 86.60 seconds\n",
      "2025-08-20 21:26:39,455 [torchrl][INFO] macs: 49.17 MMac  Params: 35.47 k\n",
      "2025-08-20 21:26:40,488 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:26:40,532 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -5.168745517730713:   2%|▏         | 1/60 [00:02<02:25,  2.47s/it]\u001B[A\n",
      "episode_reward_mean = -5.982687473297119:   3%|▎         | 2/60 [00:04<02:19,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = -5.231297969818115:   5%|▌         | 3/60 [00:07<02:15,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = -6.521865367889404:   7%|▋         | 4/60 [00:09<02:12,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = -7.46065092086792:   8%|▊         | 5/60 [00:11<02:10,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = -5.937981128692627:  10%|█         | 6/60 [00:14<02:08,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = -2.9481399059295654:  12%|█▏        | 7/60 [00:16<02:08,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = -5.717981815338135:  13%|█▎        | 8/60 [00:19<02:05,  2.41s/it] \u001B[A\n",
      "episode_reward_mean = -4.102100372314453:  15%|█▌        | 9/60 [00:21<02:01,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = -1.4452584981918335:  17%|█▋        | 10/60 [00:23<01:59,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 6.285439968109131:  18%|█▊        | 11/60 [00:26<01:56,  2.38s/it]  \u001B[A\n",
      "episode_reward_mean = 13.780047416687012:  20%|██        | 12/60 [00:28<01:54,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 19.189006805419922:  22%|██▏       | 13/60 [00:31<01:51,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 18.594701766967773:  23%|██▎       | 14/60 [00:33<01:49,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 29.392608642578125:  25%|██▌       | 15/60 [00:35<01:46,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 34.308837890625:  27%|██▋       | 16/60 [00:38<01:44,  2.38s/it]   \u001B[A\n",
      "episode_reward_mean = 34.56858825683594:  28%|██▊       | 17/60 [00:40<01:42,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 43.07644271850586:  30%|███       | 18/60 [00:42<01:39,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 45.54718780517578:  32%|███▏      | 19/60 [00:45<01:37,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 55.44222640991211:  33%|███▎      | 20/60 [00:47<01:35,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 42.63175964355469:  35%|███▌      | 21/60 [00:50<01:33,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 61.41924285888672:  37%|███▋      | 22/60 [00:52<01:31,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 64.07722473144531:  38%|███▊      | 23/60 [00:54<01:28,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 71.3373794555664:  40%|████      | 24/60 [00:57<01:26,  2.40s/it] \u001B[A\n",
      "episode_reward_mean = 67.80535888671875:  42%|████▏     | 25/60 [00:59<01:23,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 65.82584381103516:  43%|████▎     | 26/60 [01:02<01:22,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 77.32733917236328:  45%|████▌     | 27/60 [01:04<01:19,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 75.17256164550781:  47%|████▋     | 28/60 [01:06<01:16,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 80.47576141357422:  48%|████▊     | 29/60 [01:09<01:14,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 88.23033142089844:  50%|█████     | 30/60 [01:11<01:12,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 87.45299530029297:  52%|█████▏    | 31/60 [01:14<01:09,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 87.40690612792969:  53%|█████▎    | 32/60 [01:16<01:07,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 87.9349136352539:  55%|█████▌    | 33/60 [01:18<01:04,  2.39s/it] \u001B[A\n",
      "episode_reward_mean = 87.2955322265625:  57%|█████▋    | 34/60 [01:21<01:02,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 87.60319519042969:  58%|█████▊    | 35/60 [01:23<00:59,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 90.85698699951172:  60%|██████    | 36/60 [01:26<00:57,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 91.18031311035156:  62%|██████▏   | 37/60 [01:28<00:55,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 86.90823364257812:  63%|██████▎   | 38/60 [01:30<00:52,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 93.23696899414062:  65%|██████▌   | 39/60 [01:33<00:50,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 91.27362060546875:  67%|██████▋   | 40/60 [01:35<00:47,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 90.22061157226562:  68%|██████▊   | 41/60 [01:38<00:45,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 98.20767211914062:  70%|███████   | 42/60 [01:40<00:43,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 95.05677795410156:  72%|███████▏  | 43/60 [01:42<00:40,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 92.0258560180664:  73%|███████▎  | 44/60 [01:45<00:38,  2.43s/it] \u001B[A\n",
      "episode_reward_mean = 94.36524963378906:  75%|███████▌  | 45/60 [01:47<00:36,  2.46s/it]\u001B[A\n",
      "episode_reward_mean = 94.62549591064453:  77%|███████▋  | 46/60 [01:50<00:34,  2.44s/it]\u001B[A\n",
      "episode_reward_mean = 102.69619750976562:  78%|███████▊  | 47/60 [01:52<00:31,  2.43s/it]\u001B[A\n",
      "episode_reward_mean = 95.72515106201172:  80%|████████  | 48/60 [01:55<00:28,  2.41s/it] \u001B[A\n",
      "episode_reward_mean = 92.81234741210938:  82%|████████▏ | 49/60 [01:57<00:26,  2.42s/it]\u001B[A\n",
      "episode_reward_mean = 106.80201721191406:  83%|████████▎ | 50/60 [01:59<00:24,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 91.99642181396484:  85%|████████▌ | 51/60 [02:02<00:21,  2.40s/it] \u001B[A\n",
      "episode_reward_mean = 96.39998626708984:  87%|████████▋ | 52/60 [02:04<00:19,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 101.74917602539062:  88%|████████▊ | 53/60 [02:07<00:16,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 100.75273132324219:  90%|█████████ | 54/60 [02:09<00:14,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 92.99832153320312:  92%|█████████▏| 55/60 [02:11<00:11,  2.39s/it] \u001B[A\n",
      "episode_reward_mean = 110.06535339355469:  93%|█████████▎| 56/60 [02:14<00:09,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 94.814453125:  95%|█████████▌| 57/60 [02:16<00:07,  2.40s/it]      \u001B[A\n",
      "episode_reward_mean = 106.7917251586914:  97%|█████████▋| 58/60 [02:19<00:04,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 96.67112731933594:  98%|█████████▊| 59/60 [02:21<00:02,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 100.9277572631836: 100%|██████████| 60/60 [02:23<00:00,  2.40s/it]\u001B[A\n",
      "2025-08-20 21:29:04,309 [torchrl][INFO] Training time: 87.25 seconds\n",
      "2025-08-20 21:29:04,314 [torchrl][INFO] macs: 49.17 MMac  Params: 35.47 k\n",
      "2025-08-20 21:29:05,347 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:29:05,389 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -8.014008522033691:   2%|▏         | 1/60 [00:02<02:23,  2.43s/it]\u001B[A\n",
      "episode_reward_mean = -9.880657196044922:   3%|▎         | 2/60 [00:04<02:19,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = -7.638850212097168:   5%|▌         | 3/60 [00:07<02:18,  2.43s/it]\u001B[A\n",
      "episode_reward_mean = -7.281825065612793:   7%|▋         | 4/60 [00:09<02:14,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = -6.48098087310791:   8%|▊         | 5/60 [00:11<02:10,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = -6.93903112411499:  10%|█         | 6/60 [00:14<02:07,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = -8.158106803894043:  12%|█▏        | 7/60 [00:16<02:05,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = -7.32822322845459:  13%|█▎        | 8/60 [00:19<02:02,  2.36s/it] \u001B[A\n",
      "episode_reward_mean = -3.5847551822662354:  15%|█▌        | 9/60 [00:21<02:00,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = -2.7423338890075684:  17%|█▋        | 10/60 [00:23<01:58,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 0.9486349821090698:  18%|█▊        | 11/60 [00:26<01:55,  2.36s/it] \u001B[A\n",
      "episode_reward_mean = 1.3326400518417358:  20%|██        | 12/60 [00:28<01:52,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 8.225603103637695:  22%|██▏       | 13/60 [00:30<01:51,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 13.181652069091797:  23%|██▎       | 14/60 [00:33<01:49,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 14.660481452941895:  25%|██▌       | 15/60 [00:35<01:47,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 19.64560317993164:  27%|██▋       | 16/60 [00:38<01:45,  2.39s/it] \u001B[A\n",
      "episode_reward_mean = 20.8765811920166:  28%|██▊       | 17/60 [00:40<01:42,  2.39s/it] \u001B[A\n",
      "episode_reward_mean = 26.102088928222656:  30%|███       | 18/60 [00:42<01:39,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 29.21186065673828:  32%|███▏      | 19/60 [00:45<01:38,  2.40s/it] \u001B[A\n",
      "episode_reward_mean = 38.279510498046875:  33%|███▎      | 20/60 [00:47<01:35,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 33.29285430908203:  35%|███▌      | 21/60 [00:49<01:32,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 30.085582733154297:  37%|███▋      | 22/60 [00:52<01:31,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 30.83259391784668:  38%|███▊      | 23/60 [00:54<01:28,  2.40s/it] \u001B[A\n",
      "episode_reward_mean = 36.70022201538086:  40%|████      | 24/60 [00:57<01:26,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 55.29655456542969:  42%|████▏     | 25/60 [00:59<01:24,  2.43s/it]\u001B[A\n",
      "episode_reward_mean = 41.2291374206543:  43%|████▎     | 26/60 [01:02<01:22,  2.41s/it] \u001B[A\n",
      "episode_reward_mean = 61.31553268432617:  45%|████▌     | 27/60 [01:04<01:18,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 50.68254470825195:  47%|████▋     | 28/60 [01:06<01:15,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 56.77484130859375:  48%|████▊     | 29/60 [01:09<01:13,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 78.86286926269531:  50%|█████     | 30/60 [01:11<01:10,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 71.51136016845703:  52%|█████▏    | 31/60 [01:13<01:08,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 82.49889373779297:  53%|█████▎    | 32/60 [01:16<01:05,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 81.8397216796875:  55%|█████▌    | 33/60 [01:18<01:03,  2.36s/it] \u001B[A\n",
      "episode_reward_mean = 84.7131576538086:  57%|█████▋    | 34/60 [01:20<01:01,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 68.82109069824219:  58%|█████▊    | 35/60 [01:23<00:59,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 79.9969482421875:  60%|██████    | 36/60 [01:25<00:56,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 82.27781677246094:  62%|██████▏   | 37/60 [01:27<00:54,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 79.61236572265625:  63%|██████▎   | 38/60 [01:30<00:52,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 80.94414520263672:  65%|██████▌   | 39/60 [01:32<00:50,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 92.11257934570312:  67%|██████▋   | 40/60 [01:35<00:47,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 99.6537857055664:  68%|██████▊   | 41/60 [01:37<00:45,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 97.2601547241211:  70%|███████   | 42/60 [01:39<00:42,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 95.85450744628906:  72%|███████▏  | 43/60 [01:42<00:40,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 96.62920379638672:  73%|███████▎  | 44/60 [01:44<00:38,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 95.02909088134766:  75%|███████▌  | 45/60 [01:47<00:35,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 90.20515441894531:  77%|███████▋  | 46/60 [01:49<00:33,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 94.66757202148438:  78%|███████▊  | 47/60 [01:52<00:31,  2.43s/it]\u001B[A\n",
      "episode_reward_mean = 102.42449951171875:  80%|████████  | 48/60 [01:54<00:28,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 93.26258850097656:  82%|████████▏ | 49/60 [01:56<00:26,  2.41s/it] \u001B[A\n",
      "episode_reward_mean = 102.28184509277344:  83%|████████▎ | 50/60 [01:59<00:24,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = 97.26370239257812:  85%|████████▌ | 51/60 [02:01<00:21,  2.40s/it] \u001B[A\n",
      "episode_reward_mean = 98.33136749267578:  87%|████████▋ | 52/60 [02:04<00:19,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 96.01190948486328:  88%|████████▊ | 53/60 [02:06<00:16,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 99.38356018066406:  90%|█████████ | 54/60 [02:08<00:14,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 92.59085083007812:  92%|█████████▏| 55/60 [02:11<00:11,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 97.54147338867188:  93%|█████████▎| 56/60 [02:13<00:09,  2.45s/it]\u001B[A\n",
      "episode_reward_mean = 97.9110336303711:  95%|█████████▌| 57/60 [02:16<00:07,  2.41s/it] \u001B[A\n",
      "episode_reward_mean = 98.96855926513672:  97%|█████████▋| 58/60 [02:18<00:04,  2.41s/it]\u001B[A\n",
      "episode_reward_mean = 98.977783203125:  98%|█████████▊| 59/60 [02:20<00:02,  2.41s/it]  \u001B[A\n",
      "episode_reward_mean = 92.78546905517578: 100%|██████████| 60/60 [02:23<00:00,  2.39s/it]\u001B[A\n",
      "2025-08-20 21:31:28,710 [torchrl][INFO] Training time: 86.79 seconds\n",
      "2025-08-20 21:31:28,716 [torchrl][INFO] macs: 49.17 MMac  Params: 35.47 k\n",
      "2025-08-20 21:31:29,827 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:31:29,930 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 81.98717498779297:   5%|▌         | 1/20 [00:07<02:21,  7.46s/it]\u001B[A\n",
      "episode_reward_mean = 83.34395599365234:  10%|█         | 2/20 [00:14<02:12,  7.36s/it]\u001B[A\n",
      "episode_reward_mean = 91.2018051147461:  15%|█▌        | 3/20 [00:21<02:04,  7.31s/it] \u001B[A\n",
      "episode_reward_mean = 57.373046875:  20%|██        | 4/20 [00:29<01:57,  7.33s/it]    \u001B[A\n",
      "episode_reward_mean = 80.20445251464844:  25%|██▌       | 5/20 [00:36<01:50,  7.39s/it]\u001B[A\n",
      "episode_reward_mean = 82.23242950439453:  30%|███       | 6/20 [00:44<01:43,  7.39s/it]\u001B[A\n",
      "episode_reward_mean = 82.4937744140625:  35%|███▌      | 7/20 [00:51<01:36,  7.41s/it] \u001B[A\n",
      "episode_reward_mean = 91.45160675048828:  40%|████      | 8/20 [00:59<01:28,  7.41s/it]\u001B[A\n",
      "episode_reward_mean = 83.62690734863281:  45%|████▌     | 9/20 [01:06<01:21,  7.40s/it]\u001B[A\n",
      "episode_reward_mean = 89.55332946777344:  50%|█████     | 10/20 [01:13<01:14,  7.41s/it]\u001B[A\n",
      "episode_reward_mean = 97.28553771972656:  55%|█████▌    | 11/20 [01:21<01:06,  7.43s/it]\u001B[A\n",
      "episode_reward_mean = 82.62797546386719:  60%|██████    | 12/20 [01:28<00:59,  7.47s/it]\u001B[A\n",
      "episode_reward_mean = 75.19215393066406:  65%|██████▌   | 13/20 [01:36<00:52,  7.48s/it]\u001B[A\n",
      "episode_reward_mean = 86.37224578857422:  70%|███████   | 14/20 [01:43<00:44,  7.49s/it]\u001B[A\n",
      "episode_reward_mean = 95.41107940673828:  75%|███████▌  | 15/20 [01:51<00:37,  7.51s/it]\u001B[A\n",
      "episode_reward_mean = 79.74791717529297:  80%|████████  | 16/20 [01:58<00:29,  7.49s/it]\u001B[A\n",
      "episode_reward_mean = 77.49508666992188:  85%|████████▌ | 17/20 [02:06<00:22,  7.50s/it]\u001B[A\n",
      "episode_reward_mean = 87.53498077392578:  90%|█████████ | 18/20 [02:14<00:15,  7.52s/it]\u001B[A\n",
      "episode_reward_mean = 87.7239990234375:  95%|█████████▌| 19/20 [02:21<00:07,  7.54s/it] \u001B[A\n",
      "episode_reward_mean = 95.53069305419922: 100%|██████████| 20/20 [02:29<00:00,  7.46s/it]\u001B[A\n",
      "2025-08-20 21:33:59,058 [torchrl][INFO] Training time: 98.92 seconds\n",
      "2025-08-20 21:33:59,074 [torchrl][INFO] macs: 179.64 MMac  Params: 35.47 k\n",
      "2025-08-20 21:34:01,873 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:34:01,975 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 94.50712585449219:   5%|▌         | 1/20 [00:07<02:22,  7.48s/it]\u001B[A\n",
      "episode_reward_mean = 77.23673248291016:  10%|█         | 2/20 [00:14<02:14,  7.47s/it]\u001B[A\n",
      "episode_reward_mean = 85.21337127685547:  15%|█▌        | 3/20 [00:22<02:07,  7.48s/it]\u001B[A\n",
      "episode_reward_mean = 101.88813781738281:  20%|██        | 4/20 [00:29<01:59,  7.48s/it]\u001B[A\n",
      "episode_reward_mean = 89.69014739990234:  25%|██▌       | 5/20 [00:37<01:51,  7.46s/it] \u001B[A\n",
      "episode_reward_mean = 72.23025512695312:  30%|███       | 6/20 [00:45<01:45,  7.55s/it]\u001B[A\n",
      "episode_reward_mean = 102.47230529785156:  35%|███▌      | 7/20 [00:52<01:37,  7.50s/it]\u001B[A\n",
      "episode_reward_mean = 84.15863037109375:  40%|████      | 8/20 [00:59<01:29,  7.49s/it] \u001B[A\n",
      "episode_reward_mean = 73.89985656738281:  45%|████▌     | 9/20 [01:07<01:22,  7.52s/it]\u001B[A\n",
      "episode_reward_mean = 84.30762481689453:  50%|█████     | 10/20 [01:15<01:15,  7.51s/it]\u001B[A\n",
      "episode_reward_mean = 75.9063720703125:  55%|█████▌    | 11/20 [01:22<01:07,  7.53s/it] \u001B[A\n",
      "episode_reward_mean = 110.35033416748047:  60%|██████    | 12/20 [01:30<01:00,  7.51s/it]\u001B[A\n",
      "episode_reward_mean = 78.74052429199219:  65%|██████▌   | 13/20 [01:37<00:52,  7.51s/it] \u001B[A\n",
      "episode_reward_mean = 82.09679412841797:  70%|███████   | 14/20 [01:45<00:45,  7.53s/it]\u001B[A\n",
      "episode_reward_mean = 100.240478515625:  75%|███████▌  | 15/20 [01:52<00:37,  7.50s/it] \u001B[A\n",
      "episode_reward_mean = 109.61370849609375:  80%|████████  | 16/20 [02:00<00:29,  7.50s/it]\u001B[A\n",
      "episode_reward_mean = 86.35945129394531:  85%|████████▌ | 17/20 [02:07<00:22,  7.51s/it] \u001B[A\n",
      "episode_reward_mean = 90.92005920410156:  90%|█████████ | 18/20 [02:15<00:15,  7.54s/it]\u001B[A\n",
      "episode_reward_mean = 90.71199035644531:  95%|█████████▌| 19/20 [02:22<00:07,  7.50s/it]\u001B[A\n",
      "episode_reward_mean = 81.16862487792969: 100%|██████████| 20/20 [02:30<00:00,  7.50s/it]\u001B[A\n",
      "2025-08-20 21:36:32,015 [torchrl][INFO] Training time: 98.49 seconds\n",
      "2025-08-20 21:36:32,031 [torchrl][INFO] macs: 179.64 MMac  Params: 35.47 k\n",
      "2025-08-20 21:36:34,737 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:36:34,837 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 74.84307861328125:   5%|▌         | 1/20 [00:07<02:24,  7.63s/it]\u001B[A\n",
      "episode_reward_mean = 79.50645446777344:  10%|█         | 2/20 [00:15<02:16,  7.61s/it]\u001B[A\n",
      "episode_reward_mean = 70.44776916503906:  15%|█▌        | 3/20 [00:22<02:09,  7.61s/it]\u001B[A\n",
      "episode_reward_mean = 78.86922454833984:  20%|██        | 4/20 [00:30<02:01,  7.59s/it]\u001B[A\n",
      "episode_reward_mean = 89.08419036865234:  25%|██▌       | 5/20 [00:37<01:53,  7.57s/it]\u001B[A\n",
      "episode_reward_mean = 69.67107391357422:  30%|███       | 6/20 [00:45<01:46,  7.58s/it]\u001B[A\n",
      "episode_reward_mean = 76.21015930175781:  35%|███▌      | 7/20 [00:52<01:37,  7.53s/it]\u001B[A\n",
      "episode_reward_mean = 63.66271209716797:  40%|████      | 8/20 [01:00<01:30,  7.55s/it]\u001B[A\n",
      "episode_reward_mean = 70.28850555419922:  45%|████▌     | 9/20 [01:08<01:22,  7.54s/it]\u001B[A\n",
      "episode_reward_mean = 93.33660888671875:  50%|█████     | 10/20 [01:15<01:15,  7.52s/it]\u001B[A\n",
      "episode_reward_mean = 59.968292236328125:  55%|█████▌    | 11/20 [01:23<01:07,  7.52s/it]\u001B[A\n",
      "episode_reward_mean = 74.6524887084961:  60%|██████    | 12/20 [01:30<00:59,  7.50s/it]  \u001B[A\n",
      "episode_reward_mean = 81.71678161621094:  65%|██████▌   | 13/20 [01:38<00:52,  7.50s/it]\u001B[A\n",
      "episode_reward_mean = 77.1280517578125:  70%|███████   | 14/20 [01:45<00:45,  7.50s/it] \u001B[A\n",
      "episode_reward_mean = 71.6651382446289:  75%|███████▌  | 15/20 [01:52<00:37,  7.49s/it]\u001B[A\n",
      "episode_reward_mean = 71.66329956054688:  80%|████████  | 16/20 [02:00<00:29,  7.47s/it]\u001B[A\n",
      "episode_reward_mean = 83.46074676513672:  85%|████████▌ | 17/20 [02:07<00:22,  7.47s/it]\u001B[A\n",
      "episode_reward_mean = 74.55101776123047:  90%|█████████ | 18/20 [02:15<00:14,  7.49s/it]\u001B[A\n",
      "episode_reward_mean = 76.05522918701172:  95%|█████████▌| 19/20 [02:23<00:07,  7.53s/it]\u001B[A\n",
      "episode_reward_mean = 73.80623626708984: 100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\u001B[A\n",
      "2025-08-20 21:39:05,302 [torchrl][INFO] Training time: 99.24 seconds\n",
      "2025-08-20 21:39:05,319 [torchrl][INFO] macs: 179.64 MMac  Params: 35.47 k\n",
      "2025-08-20 21:39:07,960 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:39:08,061 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 68.8485336303711:   5%|▌         | 1/20 [00:07<02:20,  7.40s/it]\u001B[A\n",
      "episode_reward_mean = 70.63992309570312:  10%|█         | 2/20 [00:14<02:13,  7.43s/it]\u001B[A\n",
      "episode_reward_mean = 73.15843200683594:  15%|█▌        | 3/20 [00:22<02:07,  7.50s/it]\u001B[A\n",
      "episode_reward_mean = 86.47039031982422:  20%|██        | 4/20 [00:29<01:59,  7.46s/it]\u001B[A\n",
      "episode_reward_mean = 68.71500396728516:  25%|██▌       | 5/20 [00:37<01:52,  7.47s/it]\u001B[A\n",
      "episode_reward_mean = 76.23236083984375:  30%|███       | 6/20 [00:44<01:44,  7.48s/it]\u001B[A\n",
      "episode_reward_mean = 74.1227035522461:  35%|███▌      | 7/20 [00:52<01:37,  7.48s/it] \u001B[A\n",
      "episode_reward_mean = 81.16641998291016:  40%|████      | 8/20 [00:59<01:29,  7.47s/it]\u001B[A\n",
      "episode_reward_mean = 89.58939361572266:  45%|████▌     | 9/20 [01:07<01:22,  7.51s/it]\u001B[A\n",
      "episode_reward_mean = 98.41989135742188:  50%|█████     | 10/20 [01:14<01:15,  7.50s/it]\u001B[A\n",
      "episode_reward_mean = 89.66360473632812:  55%|█████▌    | 11/20 [01:22<01:07,  7.51s/it]\u001B[A\n",
      "episode_reward_mean = 87.05583953857422:  60%|██████    | 12/20 [01:29<00:59,  7.49s/it]\u001B[A\n",
      "episode_reward_mean = 91.45623016357422:  65%|██████▌   | 13/20 [01:37<00:52,  7.51s/it]\u001B[A\n",
      "episode_reward_mean = 65.72386169433594:  70%|███████   | 14/20 [01:44<00:44,  7.50s/it]\u001B[A\n",
      "episode_reward_mean = 103.52640533447266:  75%|███████▌  | 15/20 [01:52<00:37,  7.50s/it]\u001B[A\n",
      "episode_reward_mean = 73.60254669189453:  80%|████████  | 16/20 [01:59<00:29,  7.49s/it] \u001B[A\n",
      "episode_reward_mean = 89.57208251953125:  85%|████████▌ | 17/20 [02:07<00:22,  7.47s/it]\u001B[A\n",
      "episode_reward_mean = 86.21862030029297:  90%|█████████ | 18/20 [02:14<00:14,  7.44s/it]\u001B[A\n",
      "episode_reward_mean = 84.17173767089844:  95%|█████████▌| 19/20 [02:21<00:07,  7.41s/it]\u001B[A\n",
      "episode_reward_mean = 91.8458023071289: 100%|██████████| 20/20 [02:29<00:00,  7.47s/it] \u001B[A\n",
      "2025-08-20 21:41:37,402 [torchrl][INFO] Training time: 98.52 seconds\n",
      "2025-08-20 21:41:37,418 [torchrl][INFO] macs: 179.64 MMac  Params: 35.47 k\n",
      "2025-08-20 21:41:40,088 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:41:40,189 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 78.84829711914062:   5%|▌         | 1/20 [00:07<02:18,  7.28s/it]\u001B[A\n",
      "episode_reward_mean = 88.7951431274414:  10%|█         | 2/20 [00:14<02:11,  7.32s/it] \u001B[A\n",
      "episode_reward_mean = 77.17525482177734:  15%|█▌        | 3/20 [00:21<02:04,  7.34s/it]\u001B[A\n",
      "episode_reward_mean = 74.519287109375:  20%|██        | 4/20 [00:29<01:58,  7.39s/it]  \u001B[A\n",
      "episode_reward_mean = 95.96208190917969:  25%|██▌       | 5/20 [00:36<01:50,  7.39s/it]\u001B[A\n",
      "episode_reward_mean = 74.69303131103516:  30%|███       | 6/20 [00:44<01:43,  7.37s/it]\u001B[A\n",
      "episode_reward_mean = 75.66048431396484:  35%|███▌      | 7/20 [00:51<01:35,  7.37s/it]\u001B[A\n",
      "episode_reward_mean = 92.11974334716797:  40%|████      | 8/20 [00:59<01:28,  7.40s/it]\u001B[A\n",
      "episode_reward_mean = 83.82347869873047:  45%|████▌     | 9/20 [01:06<01:21,  7.41s/it]\u001B[A\n",
      "episode_reward_mean = 86.56663513183594:  50%|█████     | 10/20 [01:13<01:13,  7.39s/it]\u001B[A\n",
      "episode_reward_mean = 105.72621154785156:  55%|█████▌    | 11/20 [01:21<01:06,  7.42s/it]\u001B[A\n",
      "episode_reward_mean = 93.40534210205078:  60%|██████    | 12/20 [01:28<00:59,  7.40s/it] \u001B[A\n",
      "episode_reward_mean = 70.93003845214844:  65%|██████▌   | 13/20 [01:36<00:51,  7.40s/it]\u001B[A\n",
      "episode_reward_mean = 102.59452819824219:  70%|███████   | 14/20 [01:43<00:44,  7.37s/it]\u001B[A\n",
      "episode_reward_mean = 65.30315399169922:  75%|███████▌  | 15/20 [01:50<00:36,  7.38s/it] \u001B[A\n",
      "episode_reward_mean = 75.91781616210938:  80%|████████  | 16/20 [01:58<00:29,  7.43s/it]\u001B[A\n",
      "episode_reward_mean = 84.93612670898438:  85%|████████▌ | 17/20 [02:05<00:22,  7.45s/it]\u001B[A\n",
      "episode_reward_mean = 75.50790405273438:  90%|█████████ | 18/20 [02:13<00:14,  7.47s/it]\u001B[A\n",
      "episode_reward_mean = 73.78306579589844:  95%|█████████▌| 19/20 [02:20<00:07,  7.47s/it]\u001B[A\n",
      "episode_reward_mean = 78.11604309082031: 100%|██████████| 20/20 [02:28<00:00,  7.42s/it]\u001B[A\n",
      "2025-08-20 21:44:08,497 [torchrl][INFO] Training time: 97.66 seconds\n",
      "2025-08-20 21:44:08,513 [torchrl][INFO] macs: 179.64 MMac  Params: 35.47 k\n",
      "2025-08-20 21:44:11,207 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:44:11,309 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 70.0401840209961:   5%|▌         | 1/20 [00:07<02:19,  7.35s/it]\u001B[A\n",
      "episode_reward_mean = 63.43031311035156:  10%|█         | 2/20 [00:14<02:12,  7.35s/it]\u001B[A\n",
      "episode_reward_mean = 41.25890350341797:  15%|█▌        | 3/20 [00:22<02:06,  7.43s/it]\u001B[A\n",
      "episode_reward_mean = 71.66804504394531:  20%|██        | 4/20 [00:29<01:58,  7.39s/it]\u001B[A\n",
      "episode_reward_mean = 64.38211059570312:  25%|██▌       | 5/20 [00:36<01:50,  7.40s/it]\u001B[A\n",
      "episode_reward_mean = 62.2567138671875:  30%|███       | 6/20 [00:44<01:43,  7.42s/it] \u001B[A\n",
      "episode_reward_mean = 73.19734191894531:  35%|███▌      | 7/20 [00:51<01:36,  7.41s/it]\u001B[A\n",
      "episode_reward_mean = 58.086814880371094:  40%|████      | 8/20 [00:59<01:28,  7.42s/it]\u001B[A\n",
      "episode_reward_mean = 52.82776641845703:  45%|████▌     | 9/20 [01:06<01:21,  7.42s/it] \u001B[A\n",
      "episode_reward_mean = 62.478515625:  50%|█████     | 10/20 [01:14<01:13,  7.39s/it]    \u001B[A\n",
      "episode_reward_mean = 74.4498519897461:  55%|█████▌    | 11/20 [01:21<01:06,  7.40s/it]\u001B[A\n",
      "episode_reward_mean = 62.44447708129883:  60%|██████    | 12/20 [01:28<00:59,  7.45s/it]\u001B[A\n",
      "episode_reward_mean = 69.22893524169922:  65%|██████▌   | 13/20 [01:36<00:52,  7.44s/it]\u001B[A\n",
      "episode_reward_mean = 69.66911315917969:  70%|███████   | 14/20 [01:43<00:44,  7.47s/it]\u001B[A\n",
      "episode_reward_mean = 79.95939636230469:  75%|███████▌  | 15/20 [01:51<00:37,  7.47s/it]\u001B[A\n",
      "episode_reward_mean = 72.29192352294922:  80%|████████  | 16/20 [01:58<00:29,  7.43s/it]\u001B[A\n",
      "episode_reward_mean = 73.0489730834961:  85%|████████▌ | 17/20 [02:06<00:22,  7.41s/it] \u001B[A\n",
      "episode_reward_mean = 62.69184494018555:  90%|█████████ | 18/20 [02:13<00:14,  7.41s/it]\u001B[A\n",
      "episode_reward_mean = 69.93031311035156:  95%|█████████▌| 19/20 [02:21<00:07,  7.44s/it]\u001B[A\n",
      "episode_reward_mean = 57.4598274230957: 100%|██████████| 20/20 [02:28<00:00,  7.43s/it] \u001B[A\n",
      "2025-08-20 21:46:39,944 [torchrl][INFO] Training time: 98.64 seconds\n",
      "2025-08-20 21:46:39,960 [torchrl][INFO] macs: 179.64 MMac  Params: 35.47 k\n",
      "2025-08-20 21:46:42,574 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:46:42,676 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 76.30602264404297:   5%|▌         | 1/20 [00:07<02:22,  7.49s/it]\u001B[A\n",
      "episode_reward_mean = 60.475162506103516:  10%|█         | 2/20 [00:14<02:13,  7.40s/it]\u001B[A\n",
      "episode_reward_mean = 79.20849609375:  15%|█▌        | 3/20 [00:22<02:06,  7.43s/it]    \u001B[A\n",
      "episode_reward_mean = 67.2968521118164:  20%|██        | 4/20 [00:29<01:58,  7.38s/it]\u001B[A\n",
      "episode_reward_mean = 59.5982780456543:  25%|██▌       | 5/20 [00:36<01:50,  7.38s/it]\u001B[A\n",
      "episode_reward_mean = 57.92565155029297:  30%|███       | 6/20 [00:44<01:43,  7.38s/it]\u001B[A\n",
      "episode_reward_mean = 76.40149688720703:  35%|███▌      | 7/20 [00:51<01:36,  7.39s/it]\u001B[A\n",
      "episode_reward_mean = 71.28624725341797:  40%|████      | 8/20 [00:59<01:29,  7.44s/it]\u001B[A\n",
      "episode_reward_mean = 65.691650390625:  45%|████▌     | 9/20 [01:06<01:22,  7.46s/it]  \u001B[A\n",
      "episode_reward_mean = 68.11631774902344:  50%|█████     | 10/20 [01:14<01:14,  7.45s/it]\u001B[A\n",
      "episode_reward_mean = 65.44204711914062:  55%|█████▌    | 11/20 [01:21<01:07,  7.48s/it]\u001B[A\n",
      "episode_reward_mean = 76.73120880126953:  60%|██████    | 12/20 [01:29<00:59,  7.48s/it]\u001B[A\n",
      "episode_reward_mean = 65.51708221435547:  65%|██████▌   | 13/20 [01:36<00:52,  7.45s/it]\u001B[A\n",
      "episode_reward_mean = 71.05462646484375:  70%|███████   | 14/20 [01:44<00:45,  7.51s/it]\u001B[A\n",
      "episode_reward_mean = 79.50679016113281:  75%|███████▌  | 15/20 [01:51<00:37,  7.49s/it]\u001B[A\n",
      "episode_reward_mean = 73.76805114746094:  80%|████████  | 16/20 [01:59<00:29,  7.48s/it]\u001B[A\n",
      "episode_reward_mean = 72.95518493652344:  85%|████████▌ | 17/20 [02:06<00:22,  7.43s/it]\u001B[A\n",
      "episode_reward_mean = 84.81747436523438:  90%|█████████ | 18/20 [02:13<00:14,  7.44s/it]\u001B[A\n",
      "episode_reward_mean = 63.649269104003906:  95%|█████████▌| 19/20 [02:21<00:07,  7.41s/it]\u001B[A\n",
      "episode_reward_mean = 64.5285415649414: 100%|██████████| 20/20 [02:28<00:00,  7.44s/it]  \u001B[A\n",
      "2025-08-20 21:49:11,395 [torchrl][INFO] Training time: 99.26 seconds\n",
      "2025-08-20 21:49:11,413 [torchrl][INFO] macs: 179.64 MMac  Params: 35.47 k\n",
      "2025-08-20 21:49:14,036 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:49:14,139 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 76.7072982788086:   5%|▌         | 1/20 [00:07<02:21,  7.43s/it]\u001B[A\n",
      "episode_reward_mean = 81.69934844970703:  10%|█         | 2/20 [00:14<02:14,  7.47s/it]\u001B[A\n",
      "episode_reward_mean = 78.75895690917969:  15%|█▌        | 3/20 [00:22<02:07,  7.49s/it]\u001B[A\n",
      "episode_reward_mean = 77.86640167236328:  20%|██        | 4/20 [00:29<02:00,  7.51s/it]\u001B[A\n",
      "episode_reward_mean = 68.39855194091797:  25%|██▌       | 5/20 [00:37<01:52,  7.51s/it]\u001B[A\n",
      "episode_reward_mean = 83.70687866210938:  30%|███       | 6/20 [00:44<01:45,  7.50s/it]\u001B[A\n",
      "episode_reward_mean = 78.40895080566406:  35%|███▌      | 7/20 [00:52<01:37,  7.54s/it]\u001B[A\n",
      "episode_reward_mean = 93.33284759521484:  40%|████      | 8/20 [01:00<01:30,  7.51s/it]\u001B[A\n",
      "episode_reward_mean = 80.66625213623047:  45%|████▌     | 9/20 [01:07<01:23,  7.56s/it]\u001B[A\n",
      "episode_reward_mean = 76.60598754882812:  50%|█████     | 10/20 [01:15<01:15,  7.54s/it]\u001B[A\n",
      "episode_reward_mean = 92.87533569335938:  55%|█████▌    | 11/20 [01:22<01:07,  7.54s/it]\u001B[A\n",
      "episode_reward_mean = 80.48500061035156:  60%|██████    | 12/20 [01:30<01:00,  7.52s/it]\u001B[A\n",
      "episode_reward_mean = 86.93358612060547:  65%|██████▌   | 13/20 [01:37<00:52,  7.49s/it]\u001B[A\n",
      "episode_reward_mean = 88.27285766601562:  70%|███████   | 14/20 [01:45<00:44,  7.48s/it]\u001B[A\n",
      "episode_reward_mean = 89.63665008544922:  75%|███████▌  | 15/20 [01:52<00:37,  7.49s/it]\u001B[A\n",
      "episode_reward_mean = 89.31612396240234:  80%|████████  | 16/20 [02:00<00:29,  7.50s/it]\u001B[A\n",
      "episode_reward_mean = 78.2385482788086:  85%|████████▌ | 17/20 [02:07<00:22,  7.50s/it] \u001B[A\n",
      "episode_reward_mean = 74.6286849975586:  90%|█████████ | 18/20 [02:15<00:15,  7.54s/it]\u001B[A\n",
      "episode_reward_mean = 86.17806243896484:  95%|█████████▌| 19/20 [02:22<00:07,  7.53s/it]\u001B[A\n",
      "episode_reward_mean = 94.91265106201172: 100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\u001B[A\n",
      "2025-08-20 21:51:44,567 [torchrl][INFO] Training time: 99.27 seconds\n",
      "2025-08-20 21:51:44,584 [torchrl][INFO] macs: 179.64 MMac  Params: 35.47 k\n",
      "2025-08-20 21:51:47,290 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:51:47,393 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 101.67887115478516:   5%|▌         | 1/20 [00:07<02:19,  7.33s/it]\u001B[A\n",
      "episode_reward_mean = 84.66382598876953:  10%|█         | 2/20 [00:14<02:12,  7.36s/it] \u001B[A\n",
      "episode_reward_mean = 77.3796157836914:  15%|█▌        | 3/20 [00:22<02:05,  7.38s/it] \u001B[A\n",
      "episode_reward_mean = 67.1916732788086:  20%|██        | 4/20 [00:29<01:59,  7.46s/it]\u001B[A\n",
      "episode_reward_mean = 51.05015563964844:  25%|██▌       | 5/20 [00:37<01:52,  7.50s/it]\u001B[A\n",
      "episode_reward_mean = 67.27103424072266:  30%|███       | 6/20 [00:44<01:45,  7.52s/it]\u001B[A\n",
      "episode_reward_mean = 87.1541519165039:  35%|███▌      | 7/20 [00:52<01:38,  7.56s/it] \u001B[A\n",
      "episode_reward_mean = 81.93199920654297:  40%|████      | 8/20 [00:59<01:30,  7.51s/it]\u001B[A\n",
      "episode_reward_mean = 74.6067123413086:  45%|████▌     | 9/20 [01:07<01:22,  7.49s/it] \u001B[A\n",
      "episode_reward_mean = 76.18160247802734:  50%|█████     | 10/20 [01:14<01:14,  7.49s/it]\u001B[A\n",
      "episode_reward_mean = 98.87020111083984:  55%|█████▌    | 11/20 [01:22<01:07,  7.47s/it]\u001B[A\n",
      "episode_reward_mean = 98.12276458740234:  60%|██████    | 12/20 [01:29<01:00,  7.53s/it]\u001B[A\n",
      "episode_reward_mean = 56.079586029052734:  65%|██████▌   | 13/20 [01:37<00:52,  7.52s/it]\u001B[A\n",
      "episode_reward_mean = 65.08277130126953:  70%|███████   | 14/20 [01:44<00:45,  7.51s/it] \u001B[A\n",
      "episode_reward_mean = 86.38739776611328:  75%|███████▌  | 15/20 [01:52<00:37,  7.49s/it]\u001B[A\n",
      "episode_reward_mean = 70.4389877319336:  80%|████████  | 16/20 [01:59<00:29,  7.47s/it] \u001B[A\n",
      "episode_reward_mean = 79.99049377441406:  85%|████████▌ | 17/20 [02:07<00:22,  7.45s/it]\u001B[A\n",
      "episode_reward_mean = 72.11138153076172:  90%|█████████ | 18/20 [02:14<00:14,  7.45s/it]\u001B[A\n",
      "episode_reward_mean = 75.70912170410156:  95%|█████████▌| 19/20 [02:22<00:07,  7.46s/it]\u001B[A\n",
      "episode_reward_mean = 74.911376953125: 100%|██████████| 20/20 [02:29<00:00,  7.49s/it]  \u001B[A\n",
      "2025-08-20 21:54:17,154 [torchrl][INFO] Training time: 98.31 seconds\n",
      "2025-08-20 21:54:17,170 [torchrl][INFO] macs: 179.64 MMac  Params: 35.47 k\n",
      "2025-08-20 21:54:19,853 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:54:19,955 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 96.1875228881836:   5%|▌         | 1/20 [00:07<02:19,  7.33s/it]\u001B[A\n",
      "episode_reward_mean = 79.41569519042969:  10%|█         | 2/20 [00:14<02:12,  7.35s/it]\u001B[A\n",
      "episode_reward_mean = 77.92109680175781:  15%|█▌        | 3/20 [00:22<02:05,  7.39s/it]\u001B[A\n",
      "episode_reward_mean = 91.10746002197266:  20%|██        | 4/20 [00:29<01:58,  7.41s/it]\u001B[A\n",
      "episode_reward_mean = 76.43183135986328:  25%|██▌       | 5/20 [00:36<01:51,  7.40s/it]\u001B[A\n",
      "episode_reward_mean = 72.3892593383789:  30%|███       | 6/20 [00:44<01:44,  7.44s/it] \u001B[A\n",
      "episode_reward_mean = 76.03833770751953:  35%|███▌      | 7/20 [00:51<01:36,  7.43s/it]\u001B[A\n",
      "episode_reward_mean = 66.06974029541016:  40%|████      | 8/20 [00:59<01:29,  7.45s/it]\u001B[A\n",
      "episode_reward_mean = 72.53241729736328:  45%|████▌     | 9/20 [01:06<01:22,  7.49s/it]\u001B[A\n",
      "episode_reward_mean = 80.8869400024414:  50%|█████     | 10/20 [01:14<01:15,  7.51s/it]\u001B[A\n",
      "episode_reward_mean = 78.71077728271484:  55%|█████▌    | 11/20 [01:21<01:07,  7.50s/it]\u001B[A\n",
      "episode_reward_mean = 81.72486877441406:  60%|██████    | 12/20 [01:29<00:59,  7.50s/it]\u001B[A\n",
      "episode_reward_mean = 74.98878479003906:  65%|██████▌   | 13/20 [01:36<00:52,  7.47s/it]\u001B[A\n",
      "episode_reward_mean = 90.04853820800781:  70%|███████   | 14/20 [01:44<00:44,  7.46s/it]\u001B[A\n",
      "episode_reward_mean = 101.54253387451172:  75%|███████▌  | 15/20 [01:51<00:37,  7.52s/it]\u001B[A\n",
      "episode_reward_mean = 101.48512268066406:  80%|████████  | 16/20 [01:59<00:30,  7.53s/it]\u001B[A\n",
      "episode_reward_mean = 102.82539367675781:  85%|████████▌ | 17/20 [02:07<00:22,  7.60s/it]\u001B[A\n",
      "episode_reward_mean = 80.98799133300781:  90%|█████████ | 18/20 [02:14<00:15,  7.59s/it] \u001B[A\n",
      "episode_reward_mean = 54.83839416503906:  95%|█████████▌| 19/20 [02:22<00:07,  7.55s/it]\u001B[A\n",
      "episode_reward_mean = 98.8362808227539: 100%|██████████| 20/20 [02:29<00:00,  7.49s/it] \u001B[A\n",
      "2025-08-20 21:56:49,681 [torchrl][INFO] Training time: 99.05 seconds\n",
      "2025-08-20 21:56:49,697 [torchrl][INFO] macs: 179.64 MMac  Params: 35.47 k\n",
      "2025-08-20 21:56:52,297 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:56:52,340 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -8.156330108642578:   2%|▏         | 1/60 [00:02<02:51,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = -10.902863502502441:   3%|▎         | 2/60 [00:05<02:48,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = -10.579898834228516:   5%|▌         | 3/60 [00:08<02:44,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = -9.565985679626465:   7%|▋         | 4/60 [00:11<02:42,  2.90s/it] \u001B[A\n",
      "episode_reward_mean = -6.055057525634766:   8%|▊         | 5/60 [00:14<02:38,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = -4.749241828918457:  10%|█         | 6/60 [00:17<02:35,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = -0.3149988055229187:  12%|█▏        | 7/60 [00:20<02:36,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 0.35897761583328247:  13%|█▎        | 8/60 [00:23<02:32,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 7.312254428863525:  15%|█▌        | 9/60 [00:26<02:27,  2.90s/it]  \u001B[A\n",
      "episode_reward_mean = 12.392821311950684:  17%|█▋        | 10/60 [00:29<02:25,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 25.583419799804688:  18%|█▊        | 11/60 [00:32<02:23,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 28.661224365234375:  20%|██        | 12/60 [00:34<02:20,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 33.069252014160156:  22%|██▏       | 13/60 [00:37<02:16,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 41.47028350830078:  23%|██▎       | 14/60 [00:40<02:12,  2.89s/it] \u001B[A\n",
      "episode_reward_mean = 54.85882568359375:  25%|██▌       | 15/60 [00:43<02:10,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 57.90309524536133:  27%|██▋       | 16/60 [00:46<02:07,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 59.868064880371094:  28%|██▊       | 17/60 [00:49<02:04,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 64.6060791015625:  30%|███       | 18/60 [00:52<02:01,  2.89s/it]  \u001B[A\n",
      "episode_reward_mean = 60.71088409423828:  32%|███▏      | 19/60 [00:55<01:58,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 71.3992919921875:  33%|███▎      | 20/60 [00:58<01:55,  2.90s/it] \u001B[A\n",
      "episode_reward_mean = 68.50493621826172:  35%|███▌      | 21/60 [01:00<01:52,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 73.74913024902344:  37%|███▋      | 22/60 [01:03<01:49,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 73.6673812866211:  38%|███▊      | 23/60 [01:06<01:46,  2.88s/it] \u001B[A\n",
      "episode_reward_mean = 69.40689086914062:  40%|████      | 24/60 [01:09<01:43,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 83.49552917480469:  42%|████▏     | 25/60 [01:12<01:40,  2.87s/it]\u001B[A\n",
      "episode_reward_mean = 85.86470031738281:  43%|████▎     | 26/60 [01:15<01:37,  2.87s/it]\u001B[A\n",
      "episode_reward_mean = 86.8035888671875:  45%|████▌     | 27/60 [01:18<01:34,  2.87s/it] \u001B[A\n",
      "episode_reward_mean = 87.56818389892578:  47%|████▋     | 28/60 [01:21<01:33,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 81.21853637695312:  48%|████▊     | 29/60 [01:24<01:30,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 86.58145904541016:  50%|█████     | 30/60 [01:26<01:27,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 97.73530578613281:  52%|█████▏    | 31/60 [01:29<01:24,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 88.28175354003906:  53%|█████▎    | 32/60 [01:32<01:21,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 96.474365234375:  55%|█████▌    | 33/60 [01:35<01:18,  2.90s/it]  \u001B[A\n",
      "episode_reward_mean = 96.52407836914062:  57%|█████▋    | 34/60 [01:38<01:15,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 98.24317169189453:  58%|█████▊    | 35/60 [01:41<01:12,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 94.08424377441406:  60%|██████    | 36/60 [01:44<01:10,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 91.7713623046875:  62%|██████▏   | 37/60 [01:47<01:07,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 91.84908294677734:  63%|██████▎   | 38/60 [01:50<01:03,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 94.09716796875:  65%|██████▌   | 39/60 [01:53<01:00,  2.89s/it]   \u001B[A\n",
      "episode_reward_mean = 91.79605865478516:  67%|██████▋   | 40/60 [01:55<00:57,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 95.61654663085938:  68%|██████▊   | 41/60 [01:58<00:54,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 101.04666137695312:  70%|███████   | 42/60 [02:01<00:51,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 96.7697525024414:  72%|███████▏  | 43/60 [02:04<00:49,  2.89s/it]  \u001B[A\n",
      "episode_reward_mean = 92.85493469238281:  73%|███████▎  | 44/60 [02:07<00:46,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 102.79457092285156:  75%|███████▌  | 45/60 [02:10<00:43,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 98.5998306274414:  77%|███████▋  | 46/60 [02:13<00:40,  2.89s/it]  \u001B[A\n",
      "episode_reward_mean = 95.60520935058594:  78%|███████▊  | 47/60 [02:16<00:37,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 100.43096160888672:  80%|████████  | 48/60 [02:19<00:34,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 100.75762939453125:  82%|████████▏ | 49/60 [02:21<00:31,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 103.64006042480469:  83%|████████▎ | 50/60 [02:24<00:28,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 95.36540222167969:  85%|████████▌ | 51/60 [02:27<00:25,  2.89s/it] \u001B[A\n",
      "episode_reward_mean = 101.72859191894531:  87%|████████▋ | 52/60 [02:30<00:23,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 100.90470886230469:  88%|████████▊ | 53/60 [02:33<00:20,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 105.2191390991211:  90%|█████████ | 54/60 [02:36<00:17,  2.89s/it] \u001B[A\n",
      "episode_reward_mean = 96.79306030273438:  92%|█████████▏| 55/60 [02:39<00:14,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 96.58976745605469:  93%|█████████▎| 56/60 [02:42<00:11,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 109.4036865234375:  95%|█████████▌| 57/60 [02:45<00:08,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 94.32178497314453:  97%|█████████▋| 58/60 [02:47<00:05,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 100.11253356933594:  98%|█████████▊| 59/60 [02:50<00:02,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 98.89561462402344: 100%|██████████| 60/60 [02:53<00:00,  2.90s/it] \u001B[A\n",
      "2025-08-20 21:59:46,096 [torchrl][INFO] Training time: 116.50 seconds\n",
      "2025-08-20 21:59:46,101 [torchrl][INFO] macs: 51.07 MMac  Params: 24.13 k\n",
      "2025-08-20 21:59:47,237 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 21:59:47,279 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -10.098090171813965:   2%|▏         | 1/60 [00:03<02:59,  3.05s/it]\u001B[A\n",
      "episode_reward_mean = -8.876382827758789:   3%|▎         | 2/60 [00:05<02:51,  2.95s/it] \u001B[A\n",
      "episode_reward_mean = -3.514399528503418:   5%|▌         | 3/60 [00:08<02:45,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = -3.3083107471466064:   7%|▋         | 4/60 [00:11<02:40,  2.87s/it]\u001B[A\n",
      "episode_reward_mean = -2.3485891819000244:   8%|▊         | 5/60 [00:14<02:38,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = -3.5970969200134277:  10%|█         | 6/60 [00:17<02:36,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = -1.6713964939117432:  12%|█▏        | 7/60 [00:20<02:32,  2.87s/it]\u001B[A\n",
      "episode_reward_mean = 2.1519317626953125:  13%|█▎        | 8/60 [00:23<02:28,  2.86s/it] \u001B[A\n",
      "episode_reward_mean = 4.777367115020752:  15%|█▌        | 9/60 [00:25<02:25,  2.86s/it] \u001B[A\n",
      "episode_reward_mean = 8.79265308380127:  17%|█▋        | 10/60 [00:28<02:22,  2.86s/it]\u001B[A\n",
      "episode_reward_mean = 19.275339126586914:  18%|█▊        | 11/60 [00:31<02:20,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 23.666715621948242:  20%|██        | 12/60 [00:34<02:18,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 26.611434936523438:  22%|██▏       | 13/60 [00:37<02:15,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 29.6312198638916:  23%|██▎       | 14/60 [00:40<02:12,  2.87s/it]  \u001B[A\n",
      "episode_reward_mean = 39.32921600341797:  25%|██▌       | 15/60 [00:43<02:09,  2.87s/it]\u001B[A\n",
      "episode_reward_mean = 36.66064453125:  27%|██▋       | 16/60 [00:46<02:05,  2.86s/it]   \u001B[A\n",
      "episode_reward_mean = 48.380088806152344:  28%|██▊       | 17/60 [00:48<02:03,  2.87s/it]\u001B[A\n",
      "episode_reward_mean = 49.8965950012207:  30%|███       | 18/60 [00:51<02:02,  2.92s/it]  \u001B[A\n",
      "episode_reward_mean = 52.21946334838867:  32%|███▏      | 19/60 [00:54<01:58,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 62.27387237548828:  33%|███▎      | 20/60 [00:57<01:55,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 66.56251525878906:  35%|███▌      | 21/60 [01:00<01:51,  2.87s/it]\u001B[A\n",
      "episode_reward_mean = 60.25261306762695:  37%|███▋      | 22/60 [01:03<01:49,  2.87s/it]\u001B[A\n",
      "episode_reward_mean = 67.15321350097656:  38%|███▊      | 23/60 [01:06<01:46,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 72.28221893310547:  40%|████      | 24/60 [01:09<01:44,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 74.23686981201172:  42%|████▏     | 25/60 [01:12<01:41,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 79.6004409790039:  43%|████▎     | 26/60 [01:15<01:38,  2.91s/it] \u001B[A\n",
      "episode_reward_mean = 82.08311462402344:  45%|████▌     | 27/60 [01:17<01:35,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 88.33049011230469:  47%|████▋     | 28/60 [01:20<01:33,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 82.05876159667969:  48%|████▊     | 29/60 [01:23<01:29,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 87.98164367675781:  50%|█████     | 30/60 [01:26<01:27,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 92.20021057128906:  52%|█████▏    | 31/60 [01:29<01:23,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 83.19457244873047:  53%|█████▎    | 32/60 [01:32<01:21,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 82.30293273925781:  55%|█████▌    | 33/60 [01:35<01:18,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 81.18154907226562:  57%|█████▋    | 34/60 [01:38<01:15,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 94.72991180419922:  58%|█████▊    | 35/60 [01:41<01:13,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 93.04823303222656:  60%|██████    | 36/60 [01:44<01:10,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 89.74970245361328:  62%|██████▏   | 37/60 [01:47<01:07,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 93.5881576538086:  63%|██████▎   | 38/60 [01:50<01:04,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 89.8958740234375:  65%|██████▌   | 39/60 [01:52<01:01,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 84.86698913574219:  67%|██████▋   | 40/60 [01:55<00:58,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 80.5270004272461:  68%|██████▊   | 41/60 [01:58<00:55,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 91.8446273803711:  70%|███████   | 42/60 [02:01<00:52,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 93.24187469482422:  72%|███████▏  | 43/60 [02:04<00:49,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 89.97174072265625:  73%|███████▎  | 44/60 [02:07<00:46,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 88.77440643310547:  75%|███████▌  | 45/60 [02:10<00:43,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 94.24024963378906:  77%|███████▋  | 46/60 [02:13<00:40,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 94.3687515258789:  78%|███████▊  | 47/60 [02:16<00:37,  2.91s/it] \u001B[A\n",
      "episode_reward_mean = 84.72994232177734:  80%|████████  | 48/60 [02:19<00:35,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 97.30523681640625:  82%|████████▏ | 49/60 [02:22<00:32,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 97.72908782958984:  83%|████████▎ | 50/60 [02:24<00:28,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 91.31925201416016:  85%|████████▌ | 51/60 [02:27<00:26,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 93.80764770507812:  87%|████████▋ | 52/60 [02:30<00:23,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 101.71623992919922:  88%|████████▊ | 53/60 [02:33<00:20,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 92.09981536865234:  90%|█████████ | 54/60 [02:36<00:17,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 96.10739135742188:  92%|█████████▏| 55/60 [02:39<00:14,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 97.38170623779297:  93%|█████████▎| 56/60 [02:42<00:11,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 99.02892303466797:  95%|█████████▌| 57/60 [02:45<00:08,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 96.9036865234375:  97%|█████████▋| 58/60 [02:48<00:05,  2.91s/it] \u001B[A\n",
      "episode_reward_mean = 91.38002014160156:  98%|█████████▊| 59/60 [02:51<00:02,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 103.02002716064453: 100%|██████████| 60/60 [02:54<00:00,  2.90s/it]\u001B[A\n",
      "2025-08-20 22:02:41,286 [torchrl][INFO] Training time: 116.49 seconds\n",
      "2025-08-20 22:02:41,293 [torchrl][INFO] macs: 51.07 MMac  Params: 24.13 k\n",
      "2025-08-20 22:02:42,349 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:02:42,392 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -10.464217185974121:   2%|▏         | 1/60 [00:02<02:49,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = -9.397476196289062:   3%|▎         | 2/60 [00:05<02:48,  2.90s/it] \u001B[A\n",
      "episode_reward_mean = -9.904007911682129:   5%|▌         | 3/60 [00:08<02:46,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = -6.518011569976807:   7%|▋         | 4/60 [00:11<02:42,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = -1.3363755941390991:   8%|▊         | 5/60 [00:14<02:39,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = -1.415478229522705:  10%|█         | 6/60 [00:17<02:36,  2.90s/it] \u001B[A\n",
      "episode_reward_mean = -0.15884637832641602:  12%|█▏        | 7/60 [00:20<02:35,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 2.96358323097229:  13%|█▎        | 8/60 [00:23<02:33,  2.94s/it]    \u001B[A\n",
      "episode_reward_mean = 8.595572471618652:  15%|█▌        | 9/60 [00:26<02:29,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 16.3150634765625:  17%|█▋        | 10/60 [00:29<02:26,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 22.65462875366211:  18%|█▊        | 11/60 [00:32<02:23,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 21.305919647216797:  20%|██        | 12/60 [00:35<02:20,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 30.133331298828125:  22%|██▏       | 13/60 [00:37<02:16,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 34.401336669921875:  23%|██▎       | 14/60 [00:40<02:13,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 42.59440231323242:  25%|██▌       | 15/60 [00:43<02:11,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 44.34939193725586:  27%|██▋       | 16/60 [00:46<02:09,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 48.05730438232422:  28%|██▊       | 17/60 [00:49<02:06,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 51.843318939208984:  30%|███       | 18/60 [00:52<02:02,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 63.01490783691406:  32%|███▏      | 19/60 [00:55<02:00,  2.93s/it] \u001B[A\n",
      "episode_reward_mean = 69.40338134765625:  33%|███▎      | 20/60 [00:58<01:56,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 63.29171371459961:  35%|███▌      | 21/60 [01:01<01:53,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 68.1300277709961:  37%|███▋      | 22/60 [01:04<01:51,  2.93s/it] \u001B[A\n",
      "episode_reward_mean = 73.33305358886719:  38%|███▊      | 23/60 [01:07<01:48,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 77.87928771972656:  40%|████      | 24/60 [01:10<01:45,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 69.77532958984375:  42%|████▏     | 25/60 [01:13<01:44,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 76.70751953125:  43%|████▎     | 26/60 [01:16<01:41,  2.98s/it]   \u001B[A\n",
      "episode_reward_mean = 80.80660247802734:  45%|████▌     | 27/60 [01:19<01:38,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 82.31988525390625:  47%|████▋     | 28/60 [01:22<01:34,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 78.83993530273438:  48%|████▊     | 29/60 [01:25<01:31,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 87.0882568359375:  50%|█████     | 30/60 [01:28<01:29,  2.98s/it] \u001B[A\n",
      "episode_reward_mean = 87.75801849365234:  52%|█████▏    | 31/60 [01:32<01:38,  3.40s/it]\u001B[A\n",
      "episode_reward_mean = 83.97306823730469:  53%|█████▎    | 32/60 [01:35<01:31,  3.28s/it]\u001B[A\n",
      "episode_reward_mean = 91.89521026611328:  55%|█████▌    | 33/60 [01:38<01:26,  3.21s/it]\u001B[A\n",
      "episode_reward_mean = 89.36490631103516:  57%|█████▋    | 34/60 [01:41<01:21,  3.14s/it]\u001B[A\n",
      "episode_reward_mean = 92.39574432373047:  58%|█████▊    | 35/60 [01:44<01:16,  3.08s/it]\u001B[A\n",
      "episode_reward_mean = 89.54743957519531:  60%|██████    | 36/60 [01:47<01:13,  3.04s/it]\u001B[A\n",
      "episode_reward_mean = 90.3126449584961:  62%|██████▏   | 37/60 [01:50<01:09,  3.03s/it] \u001B[A\n",
      "episode_reward_mean = 92.45625305175781:  63%|██████▎   | 38/60 [01:53<01:06,  3.00s/it]\u001B[A\n",
      "episode_reward_mean = 89.34683227539062:  65%|██████▌   | 39/60 [01:56<01:02,  2.98s/it]\u001B[A\n",
      "episode_reward_mean = 96.469970703125:  67%|██████▋   | 40/60 [01:59<00:59,  2.97s/it]  \u001B[A\n",
      "episode_reward_mean = 90.18204498291016:  68%|██████▊   | 41/60 [02:02<00:56,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 90.11185455322266:  70%|███████   | 42/60 [02:05<00:53,  3.00s/it]\u001B[A\n",
      "episode_reward_mean = 93.78421783447266:  72%|███████▏  | 43/60 [02:08<00:50,  2.98s/it]\u001B[A\n",
      "episode_reward_mean = 92.76905059814453:  73%|███████▎  | 44/60 [02:11<00:47,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 92.15203857421875:  75%|███████▌  | 45/60 [02:14<00:44,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 93.82109069824219:  77%|███████▋  | 46/60 [02:17<00:41,  2.99s/it]\u001B[A\n",
      "episode_reward_mean = 78.94998931884766:  78%|███████▊  | 47/60 [02:20<00:38,  2.98s/it]\u001B[A\n",
      "episode_reward_mean = 78.51112365722656:  80%|████████  | 48/60 [02:23<00:35,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 87.81295776367188:  82%|████████▏ | 49/60 [02:25<00:32,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 101.1557846069336:  83%|████████▎ | 50/60 [02:28<00:29,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 95.86479949951172:  85%|████████▌ | 51/60 [02:31<00:26,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 86.533935546875:  87%|████████▋ | 52/60 [02:34<00:23,  2.96s/it]  \u001B[A\n",
      "episode_reward_mean = 93.78211212158203:  88%|████████▊ | 53/60 [02:37<00:20,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 86.85851287841797:  90%|█████████ | 54/60 [02:40<00:17,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 87.09748077392578:  92%|█████████▏| 55/60 [02:43<00:14,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 91.20082092285156:  93%|█████████▎| 56/60 [02:46<00:11,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 95.12368774414062:  95%|█████████▌| 57/60 [02:49<00:08,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 85.55741119384766:  97%|█████████▋| 58/60 [02:52<00:05,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 86.541015625:  98%|█████████▊| 59/60 [02:55<00:03,  3.04s/it]     \u001B[A\n",
      "episode_reward_mean = 97.36692810058594: 100%|██████████| 60/60 [02:58<00:00,  2.98s/it]\u001B[A\n",
      "2025-08-20 22:05:41,195 [torchrl][INFO] Training time: 120.02 seconds\n",
      "2025-08-20 22:05:41,202 [torchrl][INFO] macs: 51.07 MMac  Params: 24.13 k\n",
      "2025-08-20 22:05:42,313 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:05:42,357 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -4.456461429595947:   2%|▏         | 1/60 [00:02<02:56,  2.99s/it]\u001B[A\n",
      "episode_reward_mean = -5.113592147827148:   3%|▎         | 2/60 [00:05<02:51,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = -8.91350269317627:   5%|▌         | 3/60 [00:08<02:48,  2.95s/it] \u001B[A\n",
      "episode_reward_mean = -8.05875301361084:   7%|▋         | 4/60 [00:11<02:45,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = -8.6346435546875:   8%|▊         | 5/60 [00:14<02:42,  2.96s/it] \u001B[A\n",
      "episode_reward_mean = -7.006138801574707:  10%|█         | 6/60 [00:17<02:40,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = -5.2121710777282715:  12%|█▏        | 7/60 [00:20<02:36,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = -4.906337261199951:  13%|█▎        | 8/60 [00:23<02:34,  2.96s/it] \u001B[A\n",
      "episode_reward_mean = 0.24218828976154327:  15%|█▌        | 9/60 [00:26<02:30,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 7.436748504638672:  17%|█▋        | 10/60 [00:29<02:28,  2.96s/it] \u001B[A\n",
      "episode_reward_mean = 5.002756118774414:  18%|█▊        | 11/60 [00:32<02:25,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 8.996241569519043:  20%|██        | 12/60 [00:35<02:22,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 15.057367324829102:  22%|██▏       | 13/60 [00:38<02:21,  3.02s/it]\u001B[A\n",
      "episode_reward_mean = 17.64275360107422:  23%|██▎       | 14/60 [00:41<02:18,  3.01s/it] \u001B[A\n",
      "episode_reward_mean = 18.19069480895996:  25%|██▌       | 15/60 [00:44<02:15,  3.01s/it]\u001B[A\n",
      "episode_reward_mean = 27.297883987426758:  27%|██▋       | 16/60 [00:47<02:11,  3.00s/it]\u001B[A\n",
      "episode_reward_mean = 26.05611801147461:  28%|██▊       | 17/60 [00:50<02:08,  3.00s/it] \u001B[A\n",
      "episode_reward_mean = 30.396446228027344:  30%|███       | 18/60 [00:53<02:05,  2.98s/it]\u001B[A\n",
      "episode_reward_mean = 42.409183502197266:  32%|███▏      | 19/60 [00:56<02:02,  2.98s/it]\u001B[A\n",
      "episode_reward_mean = 32.21417999267578:  33%|███▎      | 20/60 [00:59<01:58,  2.97s/it] \u001B[A\n",
      "episode_reward_mean = 40.155860900878906:  35%|███▌      | 21/60 [01:02<01:57,  3.00s/it]\u001B[A\n",
      "episode_reward_mean = 48.117862701416016:  37%|███▋      | 22/60 [01:05<01:53,  2.98s/it]\u001B[A\n",
      "episode_reward_mean = 56.731876373291016:  38%|███▊      | 23/60 [01:08<01:49,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 49.224544525146484:  40%|████      | 24/60 [01:11<01:46,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 60.97928237915039:  42%|████▏     | 25/60 [01:14<01:43,  2.97s/it] \u001B[A\n",
      "episode_reward_mean = 56.37037658691406:  43%|████▎     | 26/60 [01:17<01:41,  2.98s/it]\u001B[A\n",
      "episode_reward_mean = 65.9452896118164:  45%|████▌     | 27/60 [01:20<01:38,  2.97s/it] \u001B[A\n",
      "episode_reward_mean = 73.29842376708984:  47%|████▋     | 28/60 [01:23<01:35,  2.98s/it]\u001B[A\n",
      "episode_reward_mean = 75.43829345703125:  48%|████▊     | 29/60 [01:26<01:32,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 73.7370834350586:  50%|█████     | 30/60 [01:29<01:30,  3.00s/it] \u001B[A\n",
      "episode_reward_mean = 79.7320327758789:  52%|█████▏    | 31/60 [01:32<01:26,  2.99s/it]\u001B[A\n",
      "episode_reward_mean = 82.3294448852539:  53%|█████▎    | 32/60 [01:35<01:23,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 84.148681640625:  55%|█████▌    | 33/60 [01:38<01:20,  2.98s/it] \u001B[A\n",
      "episode_reward_mean = 74.57417297363281:  57%|█████▋    | 34/60 [01:41<01:17,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 84.57278442382812:  58%|█████▊    | 35/60 [01:44<01:13,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 83.84232330322266:  60%|██████    | 36/60 [01:47<01:10,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 84.3081283569336:  62%|██████▏   | 37/60 [01:50<01:07,  2.94s/it] \u001B[A\n",
      "episode_reward_mean = 84.62933349609375:  63%|██████▎   | 38/60 [01:52<01:05,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 83.989501953125:  65%|██████▌   | 39/60 [01:55<01:02,  2.96s/it]  \u001B[A\n",
      "episode_reward_mean = 94.68482208251953:  67%|██████▋   | 40/60 [01:58<00:59,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 91.61395263671875:  68%|██████▊   | 41/60 [02:01<00:56,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 84.60381317138672:  70%|███████   | 42/60 [02:04<00:52,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 93.35259246826172:  72%|███████▏  | 43/60 [02:07<00:49,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 84.32234191894531:  73%|███████▎  | 44/60 [02:10<00:46,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 89.20941925048828:  75%|███████▌  | 45/60 [02:13<00:43,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 93.01974487304688:  77%|███████▋  | 46/60 [02:16<00:41,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 93.66970825195312:  78%|███████▊  | 47/60 [02:19<00:38,  2.98s/it]\u001B[A\n",
      "episode_reward_mean = 85.64678955078125:  80%|████████  | 48/60 [02:22<00:35,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 97.51873779296875:  82%|████████▏ | 49/60 [02:25<00:32,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 95.3906021118164:  83%|████████▎ | 50/60 [02:28<00:29,  2.94s/it] \u001B[A\n",
      "episode_reward_mean = 98.66206359863281:  85%|████████▌ | 51/60 [02:31<00:26,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 94.95854187011719:  87%|████████▋ | 52/60 [02:34<00:23,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 90.70893859863281:  88%|████████▊ | 53/60 [02:37<00:20,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 92.99906921386719:  90%|█████████ | 54/60 [02:40<00:17,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 95.9435806274414:  92%|█████████▏| 55/60 [02:43<00:14,  2.95s/it] \u001B[A\n",
      "episode_reward_mean = 100.97623443603516:  93%|█████████▎| 56/60 [02:45<00:11,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 86.91044616699219:  95%|█████████▌| 57/60 [02:48<00:08,  2.93s/it] \u001B[A\n",
      "episode_reward_mean = 96.25209045410156:  97%|█████████▋| 58/60 [02:51<00:05,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 91.782958984375:  98%|█████████▊| 59/60 [02:54<00:02,  2.93s/it]  \u001B[A\n",
      "episode_reward_mean = 91.75768280029297: 100%|██████████| 60/60 [02:57<00:00,  2.96s/it]\u001B[A\n",
      "2025-08-20 22:08:40,041 [torchrl][INFO] Training time: 118.36 seconds\n",
      "2025-08-20 22:08:40,048 [torchrl][INFO] macs: 51.07 MMac  Params: 24.13 k\n",
      "2025-08-20 22:08:41,151 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:08:41,196 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -11.2173433303833:   2%|▏         | 1/60 [00:03<02:58,  3.02s/it]\u001B[A\n",
      "episode_reward_mean = -9.823323249816895:   3%|▎         | 2/60 [00:06<02:58,  3.08s/it]\u001B[A\n",
      "episode_reward_mean = -7.898640155792236:   5%|▌         | 3/60 [00:09<02:54,  3.05s/it]\u001B[A\n",
      "episode_reward_mean = -7.189778804779053:   7%|▋         | 4/60 [00:12<02:49,  3.03s/it]\u001B[A\n",
      "episode_reward_mean = -7.839827537536621:   8%|▊         | 5/60 [00:15<02:45,  3.02s/it]\u001B[A\n",
      "episode_reward_mean = -4.255246639251709:  10%|█         | 6/60 [00:18<02:42,  3.00s/it]\u001B[A\n",
      "episode_reward_mean = -3.6419730186462402:  12%|█▏        | 7/60 [00:21<02:38,  2.98s/it]\u001B[A\n",
      "episode_reward_mean = 0.8247300982475281:  13%|█▎        | 8/60 [00:24<02:34,  2.98s/it] \u001B[A\n",
      "episode_reward_mean = 0.803436279296875:  15%|█▌        | 9/60 [00:27<02:32,  2.98s/it] \u001B[A\n",
      "episode_reward_mean = 13.008039474487305:  17%|█▋        | 10/60 [00:29<02:28,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 14.213188171386719:  18%|█▊        | 11/60 [00:33<02:27,  3.01s/it]\u001B[A\n",
      "episode_reward_mean = 19.142934799194336:  20%|██        | 12/60 [00:36<02:23,  3.00s/it]\u001B[A\n",
      "episode_reward_mean = 36.40986251831055:  22%|██▏       | 13/60 [00:39<02:20,  2.99s/it] \u001B[A\n",
      "episode_reward_mean = 37.87156295776367:  23%|██▎       | 14/60 [00:41<02:17,  2.98s/it]\u001B[A\n",
      "episode_reward_mean = 39.78391647338867:  25%|██▌       | 15/60 [00:44<02:13,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 50.89252853393555:  27%|██▋       | 16/60 [00:47<02:11,  2.98s/it]\u001B[A\n",
      "episode_reward_mean = 52.592071533203125:  28%|██▊       | 17/60 [00:50<02:07,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 58.024044036865234:  30%|███       | 18/60 [00:53<02:05,  3.00s/it]\u001B[A\n",
      "episode_reward_mean = 58.16209030151367:  32%|███▏      | 19/60 [00:56<02:03,  3.01s/it] \u001B[A\n",
      "episode_reward_mean = 68.58023071289062:  33%|███▎      | 20/60 [00:59<01:59,  3.00s/it]\u001B[A\n",
      "episode_reward_mean = 68.920654296875:  35%|███▌      | 21/60 [01:02<01:57,  3.01s/it]  \u001B[A\n",
      "episode_reward_mean = 81.82737731933594:  37%|███▋      | 22/60 [01:05<01:54,  3.01s/it]\u001B[A\n",
      "episode_reward_mean = 77.89100646972656:  38%|███▊      | 23/60 [01:08<01:51,  3.01s/it]\u001B[A\n",
      "episode_reward_mean = 81.54061126708984:  40%|████      | 24/60 [01:11<01:47,  2.99s/it]\u001B[A\n",
      "episode_reward_mean = 83.75104522705078:  42%|████▏     | 25/60 [01:14<01:43,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 83.09603881835938:  43%|████▎     | 26/60 [01:17<01:40,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 93.04869079589844:  45%|████▌     | 27/60 [01:20<01:37,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 85.41505432128906:  47%|████▋     | 28/60 [01:23<01:34,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 88.61912536621094:  48%|████▊     | 29/60 [01:26<01:31,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 88.10487365722656:  50%|█████     | 30/60 [01:29<01:28,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 90.1544189453125:  52%|█████▏    | 31/60 [01:32<01:25,  2.94s/it] \u001B[A\n",
      "episode_reward_mean = 86.62531280517578:  53%|█████▎    | 32/60 [01:35<01:22,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 88.32669067382812:  55%|█████▌    | 33/60 [01:38<01:19,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 92.96153259277344:  57%|█████▋    | 34/60 [01:41<01:16,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 89.76549530029297:  58%|█████▊    | 35/60 [01:44<01:14,  2.99s/it]\u001B[A\n",
      "episode_reward_mean = 90.7226333618164:  60%|██████    | 36/60 [01:47<01:11,  2.98s/it] \u001B[A\n",
      "episode_reward_mean = 96.6414794921875:  62%|██████▏   | 37/60 [01:50<01:08,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 95.48639678955078:  63%|██████▎   | 38/60 [01:53<01:05,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 97.515625:  65%|██████▌   | 39/60 [01:56<01:01,  2.95s/it]        \u001B[A\n",
      "episode_reward_mean = 94.6518325805664:  67%|██████▋   | 40/60 [01:59<00:58,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 86.5278091430664:  68%|██████▊   | 41/60 [02:02<00:55,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 93.2530517578125:  70%|███████   | 42/60 [02:05<00:52,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 102.13337707519531:  72%|███████▏  | 43/60 [02:07<00:49,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 92.5334243774414:  73%|███████▎  | 44/60 [02:10<00:46,  2.93s/it]  \u001B[A\n",
      "episode_reward_mean = 94.1768798828125:  75%|███████▌  | 45/60 [02:13<00:43,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 93.74425506591797:  77%|███████▋  | 46/60 [02:16<00:40,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 91.27520751953125:  78%|███████▊  | 47/60 [02:19<00:38,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 102.02792358398438:  80%|████████  | 48/60 [02:22<00:35,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 96.11446380615234:  82%|████████▏ | 49/60 [02:25<00:32,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 92.7162857055664:  83%|████████▎ | 50/60 [02:28<00:29,  2.91s/it] \u001B[A\n",
      "episode_reward_mean = 97.68273162841797:  85%|████████▌ | 51/60 [02:31<00:26,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 99.04751586914062:  87%|████████▋ | 52/60 [02:34<00:23,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 97.77693176269531:  88%|████████▊ | 53/60 [02:37<00:20,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 101.89006042480469:  90%|█████████ | 54/60 [02:40<00:17,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 106.36991119384766:  92%|█████████▏| 55/60 [02:42<00:14,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 100.54399108886719:  93%|█████████▎| 56/60 [02:45<00:11,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 95.30228424072266:  95%|█████████▌| 57/60 [02:48<00:08,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 100.87842559814453:  97%|█████████▋| 58/60 [02:51<00:05,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 108.90987396240234:  98%|█████████▊| 59/60 [02:54<00:02,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 93.33711242675781: 100%|██████████| 60/60 [02:57<00:00,  2.96s/it] \u001B[A\n",
      "2025-08-20 22:11:38,685 [torchrl][INFO] Training time: 118.86 seconds\n",
      "2025-08-20 22:11:38,692 [torchrl][INFO] macs: 51.07 MMac  Params: 24.13 k\n",
      "2025-08-20 22:11:39,841 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:11:39,883 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -12.178915023803711:   2%|▏         | 1/60 [00:02<02:51,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = -12.822028160095215:   3%|▎         | 2/60 [00:05<02:46,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = -12.636509895324707:   5%|▌         | 3/60 [00:08<02:45,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = -12.63298511505127:   7%|▋         | 4/60 [00:11<02:42,  2.90s/it] \u001B[A\n",
      "episode_reward_mean = -10.60533618927002:   8%|▊         | 5/60 [00:14<02:39,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = -9.169739723205566:  10%|█         | 6/60 [00:17<02:37,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = -6.619562149047852:  12%|█▏        | 7/60 [00:20<02:34,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = -4.621779441833496:  13%|█▎        | 8/60 [00:23<02:31,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = -6.448094367980957:  15%|█▌        | 9/60 [00:26<02:28,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = -5.00224494934082:  17%|█▋        | 10/60 [00:29<02:25,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = -5.21426248550415:  18%|█▊        | 11/60 [00:32<02:22,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = -5.330531597137451:  20%|██        | 12/60 [00:34<02:19,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = -2.1358885765075684:  22%|██▏       | 13/60 [00:37<02:15,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = -0.8653019666671753:  23%|██▎       | 14/60 [00:40<02:12,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 2.4939305782318115:  25%|██▌       | 15/60 [00:43<02:09,  2.88s/it] \u001B[A\n",
      "episode_reward_mean = 6.087216854095459:  27%|██▋       | 16/60 [00:46<02:08,  2.91s/it] \u001B[A\n",
      "episode_reward_mean = 10.043425559997559:  28%|██▊       | 17/60 [00:49<02:04,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 17.15634536743164:  30%|███       | 18/60 [00:52<02:01,  2.89s/it] \u001B[A\n",
      "episode_reward_mean = 27.46704864501953:  32%|███▏      | 19/60 [00:55<01:58,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 23.431310653686523:  33%|███▎      | 20/60 [00:57<01:55,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 36.635414123535156:  35%|███▌      | 21/60 [01:00<01:53,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 33.607765197753906:  37%|███▋      | 22/60 [01:03<01:50,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 45.88180923461914:  38%|███▊      | 23/60 [01:06<01:48,  2.94s/it] \u001B[A\n",
      "episode_reward_mean = 33.13267135620117:  40%|████      | 24/60 [01:09<01:45,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 28.2750301361084:  42%|████▏     | 25/60 [01:12<01:42,  2.94s/it] \u001B[A\n",
      "episode_reward_mean = 43.259178161621094:  43%|████▎     | 26/60 [01:15<01:39,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 49.70782470703125:  45%|████▌     | 27/60 [01:18<01:36,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 55.292335510253906:  47%|████▋     | 28/60 [01:21<01:33,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 62.11361312866211:  48%|████▊     | 29/60 [01:24<01:30,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 68.17922973632812:  50%|█████     | 30/60 [01:27<01:27,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 74.95079803466797:  52%|█████▏    | 31/60 [01:30<01:24,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 73.98410034179688:  53%|█████▎    | 32/60 [01:33<01:21,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 69.3633804321289:  55%|█████▌    | 33/60 [01:35<01:18,  2.90s/it] \u001B[A\n",
      "episode_reward_mean = 76.96219635009766:  57%|█████▋    | 34/60 [01:38<01:15,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 82.193359375:  58%|█████▊    | 35/60 [01:41<01:12,  2.89s/it]     \u001B[A\n",
      "episode_reward_mean = 82.4521484375:  60%|██████    | 36/60 [01:44<01:09,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 83.70138549804688:  62%|██████▏   | 37/60 [01:47<01:06,  2.87s/it]\u001B[A\n",
      "episode_reward_mean = 77.98834991455078:  63%|██████▎   | 38/60 [01:50<01:03,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 91.1398696899414:  65%|██████▌   | 39/60 [01:53<01:00,  2.88s/it] \u001B[A\n",
      "episode_reward_mean = 92.68368530273438:  67%|██████▋   | 40/60 [01:56<00:58,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 81.65243530273438:  68%|██████▊   | 41/60 [01:59<00:55,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 81.58435821533203:  70%|███████   | 42/60 [02:01<00:52,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 87.48334503173828:  72%|███████▏  | 43/60 [02:04<00:49,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 93.75508880615234:  73%|███████▎  | 44/60 [02:07<00:46,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 91.16766357421875:  75%|███████▌  | 45/60 [02:10<00:43,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 97.04898071289062:  77%|███████▋  | 46/60 [02:13<00:40,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 84.94136047363281:  78%|███████▊  | 47/60 [02:16<00:37,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 91.305419921875:  80%|████████  | 48/60 [02:19<00:34,  2.90s/it]  \u001B[A\n",
      "episode_reward_mean = 91.00788879394531:  82%|████████▏ | 49/60 [02:22<00:31,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 101.73579406738281:  83%|████████▎ | 50/60 [02:25<00:28,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 93.79045104980469:  85%|████████▌ | 51/60 [02:27<00:26,  2.89s/it] \u001B[A\n",
      "episode_reward_mean = 98.77571105957031:  87%|████████▋ | 52/60 [02:30<00:23,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 97.01631164550781:  88%|████████▊ | 53/60 [02:33<00:20,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 87.38233184814453:  90%|█████████ | 54/60 [02:36<00:17,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 90.5739974975586:  92%|█████████▏| 55/60 [02:39<00:14,  2.89s/it] \u001B[A\n",
      "episode_reward_mean = 85.85802459716797:  93%|█████████▎| 56/60 [02:42<00:11,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 92.73728942871094:  95%|█████████▌| 57/60 [02:45<00:08,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 97.04042053222656:  97%|█████████▋| 58/60 [02:48<00:05,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 95.89236450195312:  98%|█████████▊| 59/60 [02:51<00:02,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 92.95838928222656: 100%|██████████| 60/60 [02:54<00:00,  2.90s/it]\u001B[A\n",
      "2025-08-20 22:14:33,961 [torchrl][INFO] Training time: 116.77 seconds\n",
      "2025-08-20 22:14:33,967 [torchrl][INFO] macs: 51.07 MMac  Params: 24.13 k\n",
      "2025-08-20 22:14:35,053 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:14:35,095 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -10.051684379577637:   2%|▏         | 1/60 [00:02<02:54,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = -9.578712463378906:   3%|▎         | 2/60 [00:05<02:51,  2.95s/it] \u001B[A\n",
      "episode_reward_mean = -10.154268264770508:   5%|▌         | 3/60 [00:08<02:46,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = -8.745455741882324:   7%|▋         | 4/60 [00:11<02:42,  2.90s/it] \u001B[A\n",
      "episode_reward_mean = -8.150851249694824:   8%|▊         | 5/60 [00:14<02:38,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = -5.500959873199463:  10%|█         | 6/60 [00:17<02:36,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = -4.776527404785156:  12%|█▏        | 7/60 [00:20<02:33,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = -4.153141498565674:  13%|█▎        | 8/60 [00:23<02:31,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = -3.473637104034424:  15%|█▌        | 9/60 [00:26<02:28,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 4.438315391540527:  17%|█▋        | 10/60 [00:29<02:28,  2.98s/it]\u001B[A\n",
      "episode_reward_mean = 5.53497838973999:  18%|█▊        | 11/60 [00:32<02:24,  2.96s/it] \u001B[A\n",
      "episode_reward_mean = 10.02992057800293:  20%|██        | 12/60 [00:35<02:21,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 11.016268730163574:  22%|██▏       | 13/60 [00:38<02:18,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 16.64740753173828:  23%|██▎       | 14/60 [00:41<02:15,  2.94s/it] \u001B[A\n",
      "episode_reward_mean = 25.213359832763672:  25%|██▌       | 15/60 [00:43<02:12,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 26.53818130493164:  27%|██▋       | 16/60 [00:46<02:08,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 34.95185089111328:  28%|██▊       | 17/60 [00:49<02:05,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 37.69647979736328:  30%|███       | 18/60 [00:52<02:03,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 49.624671936035156:  32%|███▏      | 19/60 [00:55<02:00,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 55.56304168701172:  33%|███▎      | 20/60 [00:58<01:57,  2.94s/it] \u001B[A\n",
      "episode_reward_mean = 59.85233688354492:  35%|███▌      | 21/60 [01:01<01:54,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 59.985816955566406:  37%|███▋      | 22/60 [01:04<01:51,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 64.61823272705078:  38%|███▊      | 23/60 [01:07<01:48,  2.94s/it] \u001B[A\n",
      "episode_reward_mean = 68.63465881347656:  40%|████      | 24/60 [01:10<01:46,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 62.8613395690918:  42%|████▏     | 25/60 [01:13<01:43,  2.97s/it] \u001B[A\n",
      "episode_reward_mean = 73.83831787109375:  43%|████▎     | 26/60 [01:16<01:41,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = 83.9203872680664:  45%|████▌     | 27/60 [01:19<01:39,  3.00s/it] \u001B[A\n",
      "episode_reward_mean = 82.73725128173828:  47%|████▋     | 28/60 [01:22<01:35,  3.00s/it]\u001B[A\n",
      "episode_reward_mean = 86.51834869384766:  48%|████▊     | 29/60 [01:25<01:33,  3.01s/it]\u001B[A\n",
      "episode_reward_mean = 87.8946762084961:  50%|█████     | 30/60 [01:28<01:30,  3.01s/it] \u001B[A\n",
      "episode_reward_mean = 88.55536651611328:  52%|█████▏    | 31/60 [01:31<01:27,  3.01s/it]\u001B[A\n",
      "episode_reward_mean = 87.98377227783203:  53%|█████▎    | 32/60 [01:34<01:24,  3.04s/it]\u001B[A\n",
      "episode_reward_mean = 96.14202117919922:  55%|█████▌    | 33/60 [01:37<01:21,  3.01s/it]\u001B[A\n",
      "episode_reward_mean = 97.64554595947266:  57%|█████▋    | 34/60 [01:40<01:18,  3.01s/it]\u001B[A\n",
      "episode_reward_mean = 97.59931945800781:  58%|█████▊    | 35/60 [01:43<01:14,  2.99s/it]\u001B[A\n",
      "episode_reward_mean = 95.61011505126953:  60%|██████    | 36/60 [01:46<01:11,  2.98s/it]\u001B[A\n",
      "episode_reward_mean = 93.0475845336914:  62%|██████▏   | 37/60 [01:49<01:08,  2.96s/it] \u001B[A\n",
      "episode_reward_mean = 91.77131652832031:  63%|██████▎   | 38/60 [01:52<01:04,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 102.27314758300781:  65%|██████▌   | 39/60 [01:55<01:01,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 93.94583129882812:  67%|██████▋   | 40/60 [01:58<00:58,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 105.47663116455078:  68%|██████▊   | 41/60 [02:01<00:55,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 92.83397674560547:  70%|███████   | 42/60 [02:03<00:52,  2.89s/it] \u001B[A\n",
      "episode_reward_mean = 96.67743682861328:  72%|███████▏  | 43/60 [02:06<00:49,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 95.42341613769531:  73%|███████▎  | 44/60 [02:09<00:46,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 92.95663452148438:  75%|███████▌  | 45/60 [02:12<00:43,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 90.49466705322266:  77%|███████▋  | 46/60 [02:15<00:40,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 96.16621398925781:  78%|███████▊  | 47/60 [02:18<00:38,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 91.99517822265625:  80%|████████  | 48/60 [02:21<00:35,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 101.88906860351562:  82%|████████▏ | 49/60 [02:24<00:32,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 106.06184387207031:  83%|████████▎ | 50/60 [02:27<00:29,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 103.09994506835938:  85%|████████▌ | 51/60 [02:30<00:26,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 96.95853424072266:  87%|████████▋ | 52/60 [02:33<00:23,  2.91s/it] \u001B[A\n",
      "episode_reward_mean = 102.37078857421875:  88%|████████▊ | 53/60 [02:35<00:20,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 96.4670639038086:  90%|█████████ | 54/60 [02:38<00:17,  2.91s/it]  \u001B[A\n",
      "episode_reward_mean = 100.42395782470703:  92%|█████████▏| 55/60 [02:41<00:14,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 96.6694107055664:  93%|█████████▎| 56/60 [02:44<00:11,  2.94s/it]  \u001B[A\n",
      "episode_reward_mean = 95.31900024414062:  95%|█████████▌| 57/60 [02:47<00:08,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 91.63549041748047:  97%|█████████▋| 58/60 [02:50<00:05,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 106.8252944946289:  98%|█████████▊| 59/60 [02:53<00:02,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 104.02400207519531: 100%|██████████| 60/60 [02:56<00:00,  2.94s/it]\u001B[A\n",
      "2025-08-20 22:17:31,510 [torchrl][INFO] Training time: 118.35 seconds\n",
      "2025-08-20 22:17:31,516 [torchrl][INFO] macs: 51.07 MMac  Params: 24.13 k\n",
      "2025-08-20 22:17:32,571 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:17:32,613 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -12.453493118286133:   2%|▏         | 1/60 [00:02<02:51,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = -12.371150016784668:   3%|▎         | 2/60 [00:05<02:48,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = -11.574104309082031:   5%|▌         | 3/60 [00:08<02:43,  2.87s/it]\u001B[A\n",
      "episode_reward_mean = -11.137479782104492:   7%|▋         | 4/60 [00:11<02:40,  2.87s/it]\u001B[A\n",
      "episode_reward_mean = -10.435185432434082:   8%|▊         | 5/60 [00:14<02:37,  2.87s/it]\u001B[A\n",
      "episode_reward_mean = -8.590542793273926:  10%|█         | 6/60 [00:17<02:36,  2.90s/it] \u001B[A\n",
      "episode_reward_mean = -8.754755020141602:  12%|█▏        | 7/60 [00:20<02:33,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = -8.194531440734863:  13%|█▎        | 8/60 [00:23<02:29,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = -8.960465431213379:  15%|█▌        | 9/60 [00:25<02:26,  2.87s/it]\u001B[A\n",
      "episode_reward_mean = -4.6022419929504395:  17%|█▋        | 10/60 [00:28<02:23,  2.87s/it]\u001B[A\n",
      "episode_reward_mean = -2.792541980743408:  18%|█▊        | 11/60 [00:31<02:20,  2.88s/it] \u001B[A\n",
      "episode_reward_mean = -1.3996634483337402:  20%|██        | 12/60 [00:34<02:19,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 2.8936004638671875:  22%|██▏       | 13/60 [00:37<02:17,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 5.333181858062744:  23%|██▎       | 14/60 [00:40<02:14,  2.93s/it] \u001B[A\n",
      "episode_reward_mean = 8.613030433654785:  25%|██▌       | 15/60 [00:43<02:11,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 14.061613082885742:  27%|██▋       | 16/60 [00:46<02:07,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 26.30653953552246:  28%|██▊       | 17/60 [00:49<02:03,  2.88s/it] \u001B[A\n",
      "episode_reward_mean = 28.460927963256836:  30%|███       | 18/60 [00:52<02:02,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 35.4552001953125:  32%|███▏      | 19/60 [00:54<01:58,  2.90s/it]  \u001B[A\n",
      "episode_reward_mean = 35.386749267578125:  33%|███▎      | 20/60 [00:57<01:55,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 35.063621520996094:  35%|███▌      | 21/60 [01:00<01:53,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 48.954917907714844:  37%|███▋      | 22/60 [01:03<01:49,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 46.04833984375:  38%|███▊      | 23/60 [01:06<01:47,  2.90s/it]    \u001B[A\n",
      "episode_reward_mean = 64.10830688476562:  40%|████      | 24/60 [01:09<01:44,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 57.83100509643555:  42%|████▏     | 25/60 [01:12<01:41,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 59.246543884277344:  43%|████▎     | 26/60 [01:15<01:38,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 72.78223419189453:  45%|████▌     | 27/60 [01:18<01:35,  2.90s/it] \u001B[A\n",
      "episode_reward_mean = 74.4794692993164:  47%|████▋     | 28/60 [01:21<01:32,  2.90s/it] \u001B[A\n",
      "episode_reward_mean = 72.19873809814453:  48%|████▊     | 29/60 [01:23<01:29,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 72.62361145019531:  50%|█████     | 30/60 [01:26<01:27,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 79.48011779785156:  52%|█████▏    | 31/60 [01:29<01:24,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 79.30451965332031:  53%|█████▎    | 32/60 [01:32<01:21,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 91.62911987304688:  55%|█████▌    | 33/60 [01:35<01:18,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 88.18680572509766:  57%|█████▋    | 34/60 [01:38<01:14,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 85.54267883300781:  58%|█████▊    | 35/60 [01:41<01:12,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 84.98186492919922:  60%|██████    | 36/60 [01:44<01:09,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 86.93379211425781:  62%|██████▏   | 37/60 [01:47<01:06,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 86.31443786621094:  63%|██████▎   | 38/60 [01:49<01:03,  2.87s/it]\u001B[A\n",
      "episode_reward_mean = 90.14981842041016:  65%|██████▌   | 39/60 [01:52<01:00,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 93.62625122070312:  67%|██████▋   | 40/60 [01:55<00:58,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 85.93327331542969:  68%|██████▊   | 41/60 [01:58<00:55,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 97.39868927001953:  70%|███████   | 42/60 [02:01<00:52,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 98.46224975585938:  72%|███████▏  | 43/60 [02:04<00:48,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 88.01831817626953:  73%|███████▎  | 44/60 [02:07<00:46,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 94.2613296508789:  75%|███████▌  | 45/60 [02:10<00:43,  2.90s/it] \u001B[A\n",
      "episode_reward_mean = 98.75333404541016:  77%|███████▋  | 46/60 [02:13<00:41,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 97.36103057861328:  78%|███████▊  | 47/60 [02:16<00:38,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 87.71417236328125:  80%|████████  | 48/60 [02:19<00:35,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 103.72555541992188:  82%|████████▏ | 49/60 [02:21<00:31,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 99.53375244140625:  83%|████████▎ | 50/60 [02:24<00:29,  2.91s/it] \u001B[A\n",
      "episode_reward_mean = 101.41120147705078:  85%|████████▌ | 51/60 [02:27<00:26,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 90.76943969726562:  87%|████████▋ | 52/60 [02:30<00:23,  2.90s/it] \u001B[A\n",
      "episode_reward_mean = 91.04190826416016:  88%|████████▊ | 53/60 [02:33<00:20,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 91.60124969482422:  90%|█████████ | 54/60 [02:36<00:17,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 87.7314682006836:  92%|█████████▏| 55/60 [02:39<00:14,  2.89s/it] \u001B[A\n",
      "episode_reward_mean = 98.3528060913086:  93%|█████████▎| 56/60 [02:42<00:11,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 102.14905548095703:  95%|█████████▌| 57/60 [02:45<00:08,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 95.3223876953125:  97%|█████████▋| 58/60 [02:48<00:05,  2.90s/it]  \u001B[A\n",
      "episode_reward_mean = 89.7148208618164:  98%|█████████▊| 59/60 [02:50<00:02,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 96.7796859741211: 100%|██████████| 60/60 [02:53<00:00,  2.90s/it]\u001B[A\n",
      "2025-08-20 22:20:26,458 [torchrl][INFO] Training time: 116.49 seconds\n",
      "2025-08-20 22:20:26,464 [torchrl][INFO] macs: 51.07 MMac  Params: 24.13 k\n",
      "2025-08-20 22:20:27,617 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:20:27,662 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -9.182226181030273:   2%|▏         | 1/60 [00:02<02:52,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = -5.62833833694458:   3%|▎         | 2/60 [00:05<02:49,  2.93s/it] \u001B[A\n",
      "episode_reward_mean = -4.172922134399414:   5%|▌         | 3/60 [00:08<02:48,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = -3.223130226135254:   7%|▋         | 4/60 [00:11<02:43,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = -3.9768571853637695:   8%|▊         | 5/60 [00:14<02:40,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = -6.094120502471924:  10%|█         | 6/60 [00:17<02:37,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = -4.941512107849121:  12%|█▏        | 7/60 [00:20<02:34,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = -5.97005033493042:  13%|█▎        | 8/60 [00:23<02:31,  2.91s/it] \u001B[A\n",
      "episode_reward_mean = -2.6485114097595215:  15%|█▌        | 9/60 [00:26<02:28,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 2.859128952026367:  17%|█▋        | 10/60 [00:29<02:25,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 12.114075660705566:  18%|█▊        | 11/60 [00:32<02:23,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 18.55031394958496:  20%|██        | 12/60 [00:35<02:19,  2.91s/it] \u001B[A\n",
      "episode_reward_mean = 26.001747131347656:  22%|██▏       | 13/60 [00:37<02:17,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 31.517148971557617:  23%|██▎       | 14/60 [00:40<02:14,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 32.15151596069336:  25%|██▌       | 15/60 [00:43<02:11,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 47.135284423828125:  27%|██▋       | 16/60 [00:46<02:10,  2.96s/it]\u001B[A\n",
      "episode_reward_mean = 43.84297180175781:  28%|██▊       | 17/60 [00:49<02:06,  2.95s/it] \u001B[A\n",
      "episode_reward_mean = 46.93386459350586:  30%|███       | 18/60 [00:52<02:03,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 55.58003234863281:  32%|███▏      | 19/60 [00:55<01:59,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 62.055877685546875:  33%|███▎      | 20/60 [00:58<01:56,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 64.23115539550781:  35%|███▌      | 21/60 [01:01<01:54,  2.93s/it] \u001B[A\n",
      "episode_reward_mean = 66.54359436035156:  37%|███▋      | 22/60 [01:04<01:50,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 75.82562255859375:  38%|███▊      | 23/60 [01:07<01:47,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 81.58477020263672:  40%|████      | 24/60 [01:10<01:44,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 85.11308288574219:  42%|████▏     | 25/60 [01:13<01:42,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 89.72390747070312:  43%|████▎     | 26/60 [01:15<01:38,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 85.53268432617188:  45%|████▌     | 27/60 [01:18<01:36,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 92.73680877685547:  47%|████▋     | 28/60 [01:21<01:33,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 80.50962829589844:  48%|████▊     | 29/60 [01:24<01:30,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 90.14653778076172:  50%|█████     | 30/60 [01:27<01:27,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 88.17949676513672:  52%|█████▏    | 31/60 [01:30<01:24,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 93.33563232421875:  53%|█████▎    | 32/60 [01:33<01:22,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 94.9444580078125:  55%|█████▌    | 33/60 [01:36<01:19,  2.94s/it] \u001B[A\n",
      "episode_reward_mean = 87.06270599365234:  57%|█████▋    | 34/60 [01:39<01:16,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 92.53165435791016:  58%|█████▊    | 35/60 [01:42<01:12,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 106.77955627441406:  60%|██████    | 36/60 [01:45<01:09,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 95.60061645507812:  62%|██████▏   | 37/60 [01:48<01:06,  2.91s/it] \u001B[A\n",
      "episode_reward_mean = 92.1944808959961:  63%|██████▎   | 38/60 [01:51<01:04,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 87.0391616821289:  65%|██████▌   | 39/60 [01:53<01:01,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 93.8288345336914:  67%|██████▋   | 40/60 [01:56<00:58,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 93.68125915527344:  68%|██████▊   | 41/60 [01:59<00:55,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 99.1482162475586:  70%|███████   | 42/60 [02:02<00:52,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 91.4166259765625:  72%|███████▏  | 43/60 [02:05<00:49,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 84.5320053100586:  73%|███████▎  | 44/60 [02:08<00:47,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 94.30657196044922:  75%|███████▌  | 45/60 [02:11<00:43,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 99.11050415039062:  77%|███████▋  | 46/60 [02:14<00:40,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 89.38702392578125:  78%|███████▊  | 47/60 [02:17<00:37,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 98.5422592163086:  80%|████████  | 48/60 [02:20<00:35,  2.94s/it] \u001B[A\n",
      "episode_reward_mean = 99.3359146118164:  82%|████████▏ | 49/60 [02:23<00:32,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 97.29360961914062:  83%|████████▎ | 50/60 [02:26<00:29,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 92.57865142822266:  85%|████████▌ | 51/60 [02:29<00:26,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 96.51315307617188:  87%|████████▋ | 52/60 [02:32<00:23,  2.95s/it]\u001B[A\n",
      "episode_reward_mean = 103.12496948242188:  88%|████████▊ | 53/60 [02:35<00:20,  2.94s/it]\u001B[A\n",
      "episode_reward_mean = 98.69771575927734:  90%|█████████ | 54/60 [02:37<00:17,  2.93s/it] \u001B[A\n",
      "episode_reward_mean = 97.43865966796875:  92%|█████████▏| 55/60 [02:40<00:14,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 99.56424713134766:  93%|█████████▎| 56/60 [02:43<00:11,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = 101.69922637939453:  95%|█████████▌| 57/60 [02:46<00:08,  2.92s/it]\u001B[A\n",
      "episode_reward_mean = 104.81407165527344:  97%|█████████▋| 58/60 [02:49<00:05,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 93.49198913574219:  98%|█████████▊| 59/60 [02:52<00:02,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 96.44271850585938: 100%|██████████| 60/60 [02:55<00:00,  2.92s/it]\u001B[A\n",
      "2025-08-20 22:23:23,080 [torchrl][INFO] Training time: 117.94 seconds\n",
      "2025-08-20 22:23:23,087 [torchrl][INFO] macs: 51.07 MMac  Params: 24.13 k\n",
      "2025-08-20 22:23:24,160 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:23:24,203 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -9.253687858581543:   2%|▏         | 1/60 [00:02<02:47,  2.84s/it]\u001B[A\n",
      "episode_reward_mean = -6.255570411682129:   3%|▎         | 2/60 [00:05<02:52,  2.97s/it]\u001B[A\n",
      "episode_reward_mean = -6.372480392456055:   5%|▌         | 3/60 [00:08<02:46,  2.93s/it]\u001B[A\n",
      "episode_reward_mean = -5.806334018707275:   7%|▋         | 4/60 [00:11<02:42,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = -6.809703350067139:   8%|▊         | 5/60 [00:14<02:40,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = -6.468746662139893:  10%|█         | 6/60 [00:17<02:36,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = -3.7344396114349365:  12%|█▏        | 7/60 [00:20<02:33,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = -4.7154083251953125:  13%|█▎        | 8/60 [00:23<02:31,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = -1.2276442050933838:  15%|█▌        | 9/60 [00:26<02:28,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 2.4537243843078613:  17%|█▋        | 10/60 [00:29<02:25,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 6.925158977508545:  18%|█▊        | 11/60 [00:31<02:21,  2.89s/it] \u001B[A\n",
      "episode_reward_mean = 7.75573205947876:  20%|██        | 12/60 [00:34<02:17,  2.87s/it] \u001B[A\n",
      "episode_reward_mean = 10.914186477661133:  22%|██▏       | 13/60 [00:37<02:13,  2.85s/it]\u001B[A\n",
      "episode_reward_mean = 20.655447006225586:  23%|██▎       | 14/60 [00:40<02:10,  2.85s/it]\u001B[A\n",
      "episode_reward_mean = 19.80113983154297:  25%|██▌       | 15/60 [00:43<02:09,  2.87s/it] \u001B[A\n",
      "episode_reward_mean = 28.502450942993164:  27%|██▋       | 16/60 [00:46<02:05,  2.86s/it]\u001B[A\n",
      "episode_reward_mean = 26.082857131958008:  28%|██▊       | 17/60 [00:48<02:02,  2.85s/it]\u001B[A\n",
      "episode_reward_mean = 22.922468185424805:  30%|███       | 18/60 [00:52<02:01,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 30.476470947265625:  32%|███▏      | 19/60 [00:54<01:58,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 43.288997650146484:  33%|███▎      | 20/60 [00:57<01:56,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 37.070838928222656:  35%|███▌      | 21/60 [01:00<01:52,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 50.339012145996094:  37%|███▋      | 22/60 [01:03<01:49,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 53.93588638305664:  38%|███▊      | 23/60 [01:06<01:46,  2.88s/it] \u001B[A\n",
      "episode_reward_mean = 60.466251373291016:  40%|████      | 24/60 [01:09<01:43,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 63.26314926147461:  42%|████▏     | 25/60 [01:12<01:40,  2.88s/it] \u001B[A\n",
      "episode_reward_mean = 63.9955940246582:  43%|████▎     | 26/60 [01:15<01:38,  2.88s/it] \u001B[A\n",
      "episode_reward_mean = 67.61605834960938:  45%|████▌     | 27/60 [01:17<01:35,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 75.74591064453125:  47%|████▋     | 28/60 [01:20<01:32,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 80.58377838134766:  48%|████▊     | 29/60 [01:23<01:29,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 71.61045837402344:  50%|█████     | 30/60 [01:26<01:26,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 74.45508575439453:  52%|█████▏    | 31/60 [01:29<01:23,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 85.14036560058594:  53%|█████▎    | 32/60 [01:32<01:20,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 85.0000228881836:  55%|█████▌    | 33/60 [01:35<01:18,  2.91s/it] \u001B[A\n",
      "episode_reward_mean = 83.35657501220703:  57%|█████▋    | 34/60 [01:38<01:15,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 84.57280731201172:  58%|█████▊    | 35/60 [01:41<01:12,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 74.65262603759766:  60%|██████    | 36/60 [01:44<01:09,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 93.71733093261719:  62%|██████▏   | 37/60 [01:46<01:06,  2.91s/it]\u001B[A\n",
      "episode_reward_mean = 85.1294937133789:  63%|██████▎   | 38/60 [01:49<01:03,  2.91s/it] \u001B[A\n",
      "episode_reward_mean = 90.74944305419922:  65%|██████▌   | 39/60 [01:52<01:00,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 88.92599487304688:  67%|██████▋   | 40/60 [01:55<00:57,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 95.32463836669922:  68%|██████▊   | 41/60 [01:58<00:54,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 82.1455078125:  70%|███████   | 42/60 [02:01<00:51,  2.88s/it]    \u001B[A\n",
      "episode_reward_mean = 91.33712768554688:  72%|███████▏  | 43/60 [02:04<00:48,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 94.19017791748047:  73%|███████▎  | 44/60 [02:07<00:45,  2.87s/it]\u001B[A\n",
      "episode_reward_mean = 88.47871398925781:  75%|███████▌  | 45/60 [02:09<00:43,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 84.47554016113281:  77%|███████▋  | 46/60 [02:12<00:40,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 89.4200210571289:  78%|███████▊  | 47/60 [02:15<00:37,  2.88s/it] \u001B[A\n",
      "episode_reward_mean = 83.84390258789062:  80%|████████  | 48/60 [02:18<00:34,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 83.6548843383789:  82%|████████▏ | 49/60 [02:21<00:32,  2.92s/it] \u001B[A\n",
      "episode_reward_mean = 90.77091217041016:  83%|████████▎ | 50/60 [02:24<00:29,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 90.30612182617188:  85%|████████▌ | 51/60 [02:27<00:26,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 95.5821762084961:  87%|████████▋ | 52/60 [02:30<00:23,  2.89s/it] \u001B[A\n",
      "episode_reward_mean = 95.04862213134766:  88%|████████▊ | 53/60 [02:33<00:20,  2.88s/it]\u001B[A\n",
      "episode_reward_mean = 91.4206314086914:  90%|█████████ | 54/60 [02:36<00:17,  2.89s/it] \u001B[A\n",
      "episode_reward_mean = 90.19457244873047:  92%|█████████▏| 55/60 [02:38<00:14,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 85.61213684082031:  93%|█████████▎| 56/60 [02:41<00:11,  2.90s/it]\u001B[A\n",
      "episode_reward_mean = 103.92391204833984:  95%|█████████▌| 57/60 [02:44<00:08,  2.89s/it]\u001B[A\n",
      "episode_reward_mean = 99.48866271972656:  97%|█████████▋| 58/60 [02:47<00:05,  2.89s/it] \u001B[A\n",
      "episode_reward_mean = 92.7044906616211:  98%|█████████▊| 59/60 [02:50<00:02,  2.89s/it] \u001B[A\n",
      "episode_reward_mean = 100.37115478515625: 100%|██████████| 60/60 [02:53<00:00,  2.89s/it]\u001B[A\n",
      "2025-08-20 22:26:17,557 [torchrl][INFO] Training time: 116.10 seconds\n",
      "2025-08-20 22:26:17,563 [torchrl][INFO] macs: 51.07 MMac  Params: 24.13 k\n",
      "2025-08-20 22:26:18,672 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:26:18,774 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 88.74816131591797:   5%|▌         | 1/20 [00:06<02:03,  6.50s/it]\u001B[A\n",
      "episode_reward_mean = 69.74681091308594:  10%|█         | 2/20 [00:13<01:57,  6.51s/it]\u001B[A\n",
      "episode_reward_mean = 90.69763946533203:  15%|█▌        | 3/20 [00:19<01:51,  6.57s/it]\u001B[A\n",
      "episode_reward_mean = 71.92588806152344:  20%|██        | 4/20 [00:26<01:45,  6.58s/it]\u001B[A\n",
      "episode_reward_mean = 82.49343872070312:  25%|██▌       | 5/20 [00:32<01:38,  6.56s/it]\u001B[A\n",
      "episode_reward_mean = 78.34893798828125:  30%|███       | 6/20 [00:39<01:32,  6.57s/it]\u001B[A\n",
      "episode_reward_mean = 87.35283660888672:  35%|███▌      | 7/20 [00:45<01:24,  6.53s/it]\u001B[A\n",
      "episode_reward_mean = 63.67155075073242:  40%|████      | 8/20 [00:52<01:18,  6.53s/it]\u001B[A\n",
      "episode_reward_mean = 97.61991119384766:  45%|████▌     | 9/20 [00:58<01:11,  6.53s/it]\u001B[A\n",
      "episode_reward_mean = 78.15275573730469:  50%|█████     | 10/20 [01:05<01:05,  6.58s/it]\u001B[A\n",
      "episode_reward_mean = 69.843505859375:  55%|█████▌    | 11/20 [01:12<00:59,  6.56s/it]  \u001B[A\n",
      "episode_reward_mean = 93.0898208618164:  60%|██████    | 12/20 [01:18<00:52,  6.55s/it]\u001B[A\n",
      "episode_reward_mean = 68.41959381103516:  65%|██████▌   | 13/20 [01:25<00:45,  6.53s/it]\u001B[A\n",
      "episode_reward_mean = 99.03624725341797:  70%|███████   | 14/20 [01:31<00:39,  6.51s/it]\u001B[A\n",
      "episode_reward_mean = 71.65535736083984:  75%|███████▌  | 15/20 [01:38<00:32,  6.51s/it]\u001B[A\n",
      "episode_reward_mean = 79.70124816894531:  80%|████████  | 16/20 [01:44<00:26,  6.55s/it]\u001B[A\n",
      "episode_reward_mean = 80.39736938476562:  85%|████████▌ | 17/20 [01:51<00:19,  6.56s/it]\u001B[A\n",
      "episode_reward_mean = 89.32683563232422:  90%|█████████ | 18/20 [01:58<00:13,  6.63s/it]\u001B[A\n",
      "episode_reward_mean = 87.94729614257812:  95%|█████████▌| 19/20 [02:04<00:06,  6.62s/it]\u001B[A\n",
      "episode_reward_mean = 92.54425811767578: 100%|██████████| 20/20 [02:11<00:00,  6.57s/it]\u001B[A\n",
      "2025-08-20 22:28:30,102 [torchrl][INFO] Training time: 78.38 seconds\n",
      "2025-08-20 22:28:30,114 [torchrl][INFO] macs: 235.02 MMac  Params: 24.13 k\n",
      "2025-08-20 22:28:32,910 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:28:33,015 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 75.99127960205078:   5%|▌         | 1/20 [00:06<01:59,  6.30s/it]\u001B[A\n",
      "episode_reward_mean = 73.32044219970703:  10%|█         | 2/20 [00:12<01:53,  6.32s/it]\u001B[A\n",
      "episode_reward_mean = 73.51966857910156:  15%|█▌        | 3/20 [00:18<01:47,  6.30s/it]\u001B[A\n",
      "episode_reward_mean = 84.78168487548828:  20%|██        | 4/20 [00:25<01:40,  6.31s/it]\u001B[A\n",
      "episode_reward_mean = 69.10797119140625:  25%|██▌       | 5/20 [00:31<01:34,  6.31s/it]\u001B[A\n",
      "episode_reward_mean = 85.74950408935547:  30%|███       | 6/20 [00:37<01:28,  6.32s/it]\u001B[A\n",
      "episode_reward_mean = 56.06452178955078:  35%|███▌      | 7/20 [00:44<01:22,  6.37s/it]\u001B[A\n",
      "episode_reward_mean = 69.35718536376953:  40%|████      | 8/20 [00:50<01:16,  6.39s/it]\u001B[A\n",
      "episode_reward_mean = 62.63103103637695:  45%|████▌     | 9/20 [00:57<01:10,  6.42s/it]\u001B[A\n",
      "episode_reward_mean = 72.5654296875:  50%|█████     | 10/20 [01:03<01:04,  6.41s/it]   \u001B[A\n",
      "episode_reward_mean = 70.13121032714844:  55%|█████▌    | 11/20 [01:10<00:58,  6.46s/it]\u001B[A\n",
      "episode_reward_mean = 47.1591682434082:  60%|██████    | 12/20 [01:16<00:51,  6.44s/it] \u001B[A\n",
      "episode_reward_mean = 59.40823745727539:  65%|██████▌   | 13/20 [01:23<00:45,  6.46s/it]\u001B[A\n",
      "episode_reward_mean = 64.95368957519531:  70%|███████   | 14/20 [01:29<00:39,  6.52s/it]\u001B[A\n",
      "episode_reward_mean = 60.29349136352539:  75%|███████▌  | 15/20 [01:36<00:32,  6.55s/it]\u001B[A\n",
      "episode_reward_mean = 71.71814727783203:  80%|████████  | 16/20 [01:42<00:26,  6.55s/it]\u001B[A\n",
      "episode_reward_mean = 66.552734375:  85%|████████▌ | 17/20 [01:49<00:19,  6.56s/it]     \u001B[A\n",
      "episode_reward_mean = 61.073341369628906:  90%|█████████ | 18/20 [01:56<00:13,  6.56s/it]\u001B[A\n",
      "episode_reward_mean = 66.85028839111328:  95%|█████████▌| 19/20 [02:02<00:06,  6.58s/it] \u001B[A\n",
      "episode_reward_mean = 73.14727020263672: 100%|██████████| 20/20 [02:09<00:00,  6.46s/it]\u001B[A\n",
      "2025-08-20 22:30:42,307 [torchrl][INFO] Training time: 76.71 seconds\n",
      "2025-08-20 22:30:42,320 [torchrl][INFO] macs: 235.02 MMac  Params: 24.13 k\n",
      "2025-08-20 22:30:45,162 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:30:45,267 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 75.584716796875:   5%|▌         | 1/20 [00:06<02:04,  6.54s/it]\u001B[A\n",
      "episode_reward_mean = 67.33075714111328:  10%|█         | 2/20 [00:13<01:57,  6.51s/it]\u001B[A\n",
      "episode_reward_mean = 84.00586700439453:  15%|█▌        | 3/20 [00:19<01:52,  6.59s/it]\u001B[A\n",
      "episode_reward_mean = 84.40650939941406:  20%|██        | 4/20 [00:26<01:44,  6.55s/it]\u001B[A\n",
      "episode_reward_mean = 93.56116485595703:  25%|██▌       | 5/20 [00:32<01:38,  6.56s/it]\u001B[A\n",
      "episode_reward_mean = 91.76580047607422:  30%|███       | 6/20 [00:39<01:31,  6.54s/it]\u001B[A\n",
      "episode_reward_mean = 76.76582336425781:  35%|███▌      | 7/20 [00:45<01:25,  6.55s/it]\u001B[A\n",
      "episode_reward_mean = 84.43756866455078:  40%|████      | 8/20 [00:52<01:18,  6.56s/it]\u001B[A\n",
      "episode_reward_mean = 87.92611694335938:  45%|████▌     | 9/20 [00:58<01:11,  6.53s/it]\u001B[A\n",
      "episode_reward_mean = 72.35087585449219:  50%|█████     | 10/20 [01:05<01:05,  6.53s/it]\u001B[A\n",
      "episode_reward_mean = 85.06367492675781:  55%|█████▌    | 11/20 [01:11<00:58,  6.51s/it]\u001B[A\n",
      "episode_reward_mean = 86.55772399902344:  60%|██████    | 12/20 [01:18<00:51,  6.49s/it]\u001B[A\n",
      "episode_reward_mean = 89.51847839355469:  65%|██████▌   | 13/20 [01:24<00:45,  6.46s/it]\u001B[A\n",
      "episode_reward_mean = 101.78407287597656:  70%|███████   | 14/20 [01:31<00:38,  6.43s/it]\u001B[A\n",
      "episode_reward_mean = 89.66047668457031:  75%|███████▌  | 15/20 [01:37<00:32,  6.43s/it] \u001B[A\n",
      "episode_reward_mean = 80.39978790283203:  80%|████████  | 16/20 [01:43<00:25,  6.43s/it]\u001B[A\n",
      "episode_reward_mean = 92.06938934326172:  85%|████████▌ | 17/20 [01:50<00:19,  6.42s/it]\u001B[A\n",
      "episode_reward_mean = 87.89374542236328:  90%|█████████ | 18/20 [01:56<00:12,  6.41s/it]\u001B[A\n",
      "episode_reward_mean = 94.170654296875:  95%|█████████▌| 19/20 [02:03<00:06,  6.42s/it]  \u001B[A\n",
      "episode_reward_mean = 72.55294799804688: 100%|██████████| 20/20 [02:09<00:00,  6.47s/it]\u001B[A\n",
      "2025-08-20 22:32:54,673 [torchrl][INFO] Training time: 77.77 seconds\n",
      "2025-08-20 22:32:54,684 [torchrl][INFO] macs: 235.02 MMac  Params: 24.13 k\n",
      "2025-08-20 22:32:57,348 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:32:57,451 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 71.52240753173828:   5%|▌         | 1/20 [00:06<01:59,  6.30s/it]\u001B[A\n",
      "episode_reward_mean = 76.95654296875:  10%|█         | 2/20 [00:12<01:53,  6.29s/it]   \u001B[A\n",
      "episode_reward_mean = 78.14842987060547:  15%|█▌        | 3/20 [00:18<01:46,  6.27s/it]\u001B[A\n",
      "episode_reward_mean = 85.47237396240234:  20%|██        | 4/20 [00:25<01:41,  6.31s/it]\u001B[A\n",
      "episode_reward_mean = 73.2983169555664:  25%|██▌       | 5/20 [00:31<01:34,  6.30s/it] \u001B[A\n",
      "episode_reward_mean = 89.13011169433594:  30%|███       | 6/20 [00:38<01:29,  6.39s/it]\u001B[A\n",
      "episode_reward_mean = 105.93375396728516:  35%|███▌      | 7/20 [00:44<01:23,  6.43s/it]\u001B[A\n",
      "episode_reward_mean = 80.96155548095703:  40%|████      | 8/20 [00:50<01:17,  6.43s/it] \u001B[A\n",
      "episode_reward_mean = 88.72396850585938:  45%|████▌     | 9/20 [00:57<01:10,  6.44s/it]\u001B[A\n",
      "episode_reward_mean = 75.45803833007812:  50%|█████     | 10/20 [01:03<01:04,  6.46s/it]\u001B[A\n",
      "episode_reward_mean = 90.5416259765625:  55%|█████▌    | 11/20 [01:10<00:58,  6.50s/it] \u001B[A\n",
      "episode_reward_mean = 86.12085723876953:  60%|██████    | 12/20 [01:17<00:52,  6.51s/it]\u001B[A\n",
      "episode_reward_mean = 81.7353744506836:  65%|██████▌   | 13/20 [01:23<00:45,  6.51s/it] \u001B[A\n",
      "episode_reward_mean = 93.08687591552734:  70%|███████   | 14/20 [01:30<00:39,  6.52s/it]\u001B[A\n",
      "episode_reward_mean = 87.601806640625:  75%|███████▌  | 15/20 [01:36<00:32,  6.47s/it]  \u001B[A\n",
      "episode_reward_mean = 103.2481918334961:  80%|████████  | 16/20 [01:42<00:25,  6.47s/it]\u001B[A\n",
      "episode_reward_mean = 72.21210479736328:  85%|████████▌ | 17/20 [01:49<00:19,  6.47s/it]\u001B[A\n",
      "episode_reward_mean = 77.4662857055664:  90%|█████████ | 18/20 [01:55<00:12,  6.47s/it] \u001B[A\n",
      "episode_reward_mean = 85.43232727050781:  95%|█████████▌| 19/20 [02:02<00:06,  6.47s/it]\u001B[A\n",
      "episode_reward_mean = 80.9818344116211: 100%|██████████| 20/20 [02:08<00:00,  6.43s/it] \u001B[A\n",
      "2025-08-20 22:35:06,138 [torchrl][INFO] Training time: 77.13 seconds\n",
      "2025-08-20 22:35:06,149 [torchrl][INFO] macs: 235.02 MMac  Params: 24.13 k\n",
      "2025-08-20 22:35:08,897 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:35:09,001 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 93.77471160888672:   5%|▌         | 1/20 [00:06<02:01,  6.41s/it]\u001B[A\n",
      "episode_reward_mean = 80.98963165283203:  10%|█         | 2/20 [00:12<01:55,  6.40s/it]\u001B[A\n",
      "episode_reward_mean = 65.29492950439453:  15%|█▌        | 3/20 [00:19<01:49,  6.42s/it]\u001B[A\n",
      "episode_reward_mean = 88.89509582519531:  20%|██        | 4/20 [00:25<01:42,  6.42s/it]\u001B[A\n",
      "episode_reward_mean = 76.95866394042969:  25%|██▌       | 5/20 [00:32<01:35,  6.39s/it]\u001B[A\n",
      "episode_reward_mean = 86.19042205810547:  30%|███       | 6/20 [00:38<01:29,  6.37s/it]\u001B[A\n",
      "episode_reward_mean = 83.40266418457031:  35%|███▌      | 7/20 [00:44<01:22,  6.37s/it]\u001B[A\n",
      "episode_reward_mean = 79.32218170166016:  40%|████      | 8/20 [00:51<01:16,  6.41s/it]\u001B[A\n",
      "episode_reward_mean = 83.16392517089844:  45%|████▌     | 9/20 [00:57<01:10,  6.42s/it]\u001B[A\n",
      "episode_reward_mean = 79.93138885498047:  50%|█████     | 10/20 [01:04<01:04,  6.45s/it]\u001B[A\n",
      "episode_reward_mean = 103.18929290771484:  55%|█████▌    | 11/20 [01:10<00:58,  6.46s/it]\u001B[A\n",
      "episode_reward_mean = 92.11653137207031:  60%|██████    | 12/20 [01:17<00:51,  6.47s/it] \u001B[A\n",
      "episode_reward_mean = 85.04303741455078:  65%|██████▌   | 13/20 [01:23<00:45,  6.48s/it]\u001B[A\n",
      "episode_reward_mean = 69.34307098388672:  70%|███████   | 14/20 [01:30<00:38,  6.49s/it]\u001B[A\n",
      "episode_reward_mean = 71.38752746582031:  75%|███████▌  | 15/20 [01:36<00:32,  6.49s/it]\u001B[A\n",
      "episode_reward_mean = 85.30415344238281:  80%|████████  | 16/20 [01:43<00:25,  6.47s/it]\u001B[A\n",
      "episode_reward_mean = 92.55673217773438:  85%|████████▌ | 17/20 [01:49<00:19,  6.50s/it]\u001B[A\n",
      "episode_reward_mean = 80.57658386230469:  90%|█████████ | 18/20 [01:56<00:13,  6.50s/it]\u001B[A\n",
      "episode_reward_mean = 77.73606872558594:  95%|█████████▌| 19/20 [02:02<00:06,  6.53s/it]\u001B[A\n",
      "episode_reward_mean = 78.35022735595703: 100%|██████████| 20/20 [02:09<00:00,  6.45s/it]\u001B[A\n",
      "2025-08-20 22:37:18,094 [torchrl][INFO] Training time: 76.73 seconds\n",
      "2025-08-20 22:37:18,105 [torchrl][INFO] macs: 235.02 MMac  Params: 24.13 k\n",
      "2025-08-20 22:37:20,816 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:37:20,919 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 81.1578140258789:   5%|▌         | 1/20 [00:06<02:01,  6.41s/it]\u001B[A\n",
      "episode_reward_mean = 82.4660873413086:  10%|█         | 2/20 [00:12<01:54,  6.37s/it]\u001B[A\n",
      "episode_reward_mean = 95.419921875:  15%|█▌        | 3/20 [00:19<01:49,  6.41s/it]    \u001B[A\n",
      "episode_reward_mean = 90.81304168701172:  20%|██        | 4/20 [00:25<01:42,  6.38s/it]\u001B[A\n",
      "episode_reward_mean = 89.30499267578125:  25%|██▌       | 5/20 [00:32<01:36,  6.43s/it]\u001B[A\n",
      "episode_reward_mean = 70.54315185546875:  30%|███       | 6/20 [00:38<01:30,  6.46s/it]\u001B[A\n",
      "episode_reward_mean = 89.44817352294922:  35%|███▌      | 7/20 [00:45<01:23,  6.45s/it]\u001B[A\n",
      "episode_reward_mean = 92.62897491455078:  40%|████      | 8/20 [00:51<01:17,  6.47s/it]\u001B[A\n",
      "episode_reward_mean = 86.58761596679688:  45%|████▌     | 9/20 [00:57<01:11,  6.46s/it]\u001B[A\n",
      "episode_reward_mean = 92.5152359008789:  50%|█████     | 10/20 [01:04<01:05,  6.51s/it]\u001B[A\n",
      "episode_reward_mean = 77.4499740600586:  55%|█████▌    | 11/20 [01:11<00:58,  6.52s/it]\u001B[A\n",
      "episode_reward_mean = 79.05953979492188:  60%|██████    | 12/20 [01:17<00:52,  6.56s/it]\u001B[A\n",
      "episode_reward_mean = 81.67900848388672:  65%|██████▌   | 13/20 [01:24<00:45,  6.55s/it]\u001B[A\n",
      "episode_reward_mean = 97.60977935791016:  70%|███████   | 14/20 [01:30<00:39,  6.55s/it]\u001B[A\n",
      "episode_reward_mean = 100.70785522460938:  75%|███████▌  | 15/20 [01:37<00:32,  6.56s/it]\u001B[A\n",
      "episode_reward_mean = 101.186279296875:  80%|████████  | 16/20 [01:43<00:26,  6.54s/it]  \u001B[A\n",
      "episode_reward_mean = 82.70137023925781:  85%|████████▌ | 17/20 [01:50<00:19,  6.53s/it]\u001B[A\n",
      "episode_reward_mean = 85.24214935302734:  90%|█████████ | 18/20 [01:56<00:13,  6.51s/it]\u001B[A\n",
      "episode_reward_mean = 102.71349334716797:  95%|█████████▌| 19/20 [02:03<00:06,  6.51s/it]\u001B[A\n",
      "episode_reward_mean = 82.69834899902344: 100%|██████████| 20/20 [02:10<00:00,  6.50s/it] \u001B[A\n",
      "2025-08-20 22:39:30,940 [torchrl][INFO] Training time: 77.01 seconds\n",
      "2025-08-20 22:39:30,955 [torchrl][INFO] macs: 235.02 MMac  Params: 24.13 k\n",
      "2025-08-20 22:39:33,773 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:39:33,877 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 81.40129089355469:   5%|▌         | 1/20 [00:06<02:03,  6.48s/it]\u001B[A\n",
      "episode_reward_mean = 77.01254272460938:  10%|█         | 2/20 [00:12<01:55,  6.39s/it]\u001B[A\n",
      "episode_reward_mean = 90.15049743652344:  15%|█▌        | 3/20 [00:19<01:48,  6.36s/it]\u001B[A\n",
      "episode_reward_mean = 74.05530548095703:  20%|██        | 4/20 [00:25<01:42,  6.43s/it]\u001B[A\n",
      "episode_reward_mean = 94.05039978027344:  25%|██▌       | 5/20 [00:32<01:36,  6.40s/it]\u001B[A\n",
      "episode_reward_mean = 80.499267578125:  30%|███       | 6/20 [00:38<01:30,  6.45s/it]  \u001B[A\n",
      "episode_reward_mean = 86.04653930664062:  35%|███▌      | 7/20 [00:45<01:23,  6.45s/it]\u001B[A\n",
      "episode_reward_mean = 75.8722915649414:  40%|████      | 8/20 [00:51<01:17,  6.48s/it] \u001B[A\n",
      "episode_reward_mean = 75.62828826904297:  45%|████▌     | 9/20 [00:58<01:11,  6.49s/it]\u001B[A\n",
      "episode_reward_mean = 57.153343200683594:  50%|█████     | 10/20 [01:04<01:05,  6.51s/it]\u001B[A\n",
      "episode_reward_mean = 97.91658782958984:  55%|█████▌    | 11/20 [01:11<00:58,  6.53s/it] \u001B[A\n",
      "episode_reward_mean = 76.55030059814453:  60%|██████    | 12/20 [01:17<00:52,  6.51s/it]\u001B[A\n",
      "episode_reward_mean = 55.16727828979492:  65%|██████▌   | 13/20 [01:24<00:45,  6.48s/it]\u001B[A\n",
      "episode_reward_mean = 54.765769958496094:  70%|███████   | 14/20 [01:30<00:38,  6.45s/it]\u001B[A\n",
      "episode_reward_mean = 89.21595001220703:  75%|███████▌  | 15/20 [01:36<00:32,  6.44s/it] \u001B[A\n",
      "episode_reward_mean = 102.09954071044922:  80%|████████  | 16/20 [01:43<00:25,  6.46s/it]\u001B[A\n",
      "episode_reward_mean = 71.43910217285156:  85%|████████▌ | 17/20 [01:49<00:19,  6.44s/it] \u001B[A\n",
      "episode_reward_mean = 69.2897720336914:  90%|█████████ | 18/20 [01:56<00:12,  6.42s/it] \u001B[A\n",
      "episode_reward_mean = 81.47479248046875:  95%|█████████▌| 19/20 [02:02<00:06,  6.47s/it]\u001B[A\n",
      "episode_reward_mean = 94.83332061767578: 100%|██████████| 20/20 [02:09<00:00,  6.47s/it]\u001B[A\n",
      "2025-08-20 22:41:43,249 [torchrl][INFO] Training time: 76.69 seconds\n",
      "2025-08-20 22:41:43,261 [torchrl][INFO] macs: 235.02 MMac  Params: 24.13 k\n",
      "2025-08-20 22:41:46,033 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:41:46,137 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 93.53585052490234:   5%|▌         | 1/20 [00:06<02:01,  6.40s/it]\u001B[A\n",
      "episode_reward_mean = 72.56613159179688:  10%|█         | 2/20 [00:12<01:55,  6.42s/it]\u001B[A\n",
      "episode_reward_mean = 91.01219940185547:  15%|█▌        | 3/20 [00:19<01:48,  6.39s/it]\u001B[A\n",
      "episode_reward_mean = 74.92788696289062:  20%|██        | 4/20 [00:25<01:42,  6.40s/it]\u001B[A\n",
      "episode_reward_mean = 72.28190612792969:  25%|██▌       | 5/20 [00:32<01:36,  6.45s/it]\u001B[A\n",
      "episode_reward_mean = 61.49233627319336:  30%|███       | 6/20 [00:38<01:30,  6.44s/it]\u001B[A\n",
      "episode_reward_mean = 91.50948333740234:  35%|███▌      | 7/20 [00:45<01:23,  6.45s/it]\u001B[A\n",
      "episode_reward_mean = 81.30137634277344:  40%|████      | 8/20 [00:51<01:17,  6.44s/it]\u001B[A\n",
      "episode_reward_mean = 85.11924743652344:  45%|████▌     | 9/20 [00:57<01:10,  6.42s/it]\u001B[A\n",
      "episode_reward_mean = 82.97076416015625:  50%|█████     | 10/20 [01:04<01:04,  6.46s/it]\u001B[A\n",
      "episode_reward_mean = 97.06192779541016:  55%|█████▌    | 11/20 [01:10<00:58,  6.47s/it]\u001B[A\n",
      "episode_reward_mean = 73.89641571044922:  60%|██████    | 12/20 [01:17<00:52,  6.51s/it]\u001B[A\n",
      "episode_reward_mean = 79.80206298828125:  65%|██████▌   | 13/20 [01:24<00:45,  6.53s/it]\u001B[A\n",
      "episode_reward_mean = 84.50269317626953:  70%|███████   | 14/20 [01:30<00:39,  6.52s/it]\u001B[A\n",
      "episode_reward_mean = 86.57718658447266:  75%|███████▌  | 15/20 [01:37<00:32,  6.52s/it]\u001B[A\n",
      "episode_reward_mean = 88.24254608154297:  80%|████████  | 16/20 [01:43<00:25,  6.50s/it]\u001B[A\n",
      "episode_reward_mean = 88.42051696777344:  85%|████████▌ | 17/20 [01:49<00:19,  6.49s/it]\u001B[A\n",
      "episode_reward_mean = 50.87472915649414:  90%|█████████ | 18/20 [01:56<00:13,  6.51s/it]\u001B[A\n",
      "episode_reward_mean = 78.53649139404297:  95%|█████████▌| 19/20 [02:02<00:06,  6.49s/it]\u001B[A\n",
      "episode_reward_mean = 91.15245056152344: 100%|██████████| 20/20 [02:09<00:00,  6.47s/it]\u001B[A\n",
      "2025-08-20 22:43:55,609 [torchrl][INFO] Training time: 77.22 seconds\n",
      "2025-08-20 22:43:55,621 [torchrl][INFO] macs: 235.02 MMac  Params: 24.13 k\n",
      "2025-08-20 22:43:58,446 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:43:58,550 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 75.81546020507812:   5%|▌         | 1/20 [00:06<01:59,  6.30s/it]\u001B[A\n",
      "episode_reward_mean = 83.78559875488281:  10%|█         | 2/20 [00:12<01:53,  6.32s/it]\u001B[A\n",
      "episode_reward_mean = 81.84545135498047:  15%|█▌        | 3/20 [00:18<01:47,  6.32s/it]\u001B[A\n",
      "episode_reward_mean = 73.44279479980469:  20%|██        | 4/20 [00:25<01:41,  6.37s/it]\u001B[A\n",
      "episode_reward_mean = 61.07659912109375:  25%|██▌       | 5/20 [00:31<01:35,  6.35s/it]\u001B[A\n",
      "episode_reward_mean = 101.77300262451172:  30%|███       | 6/20 [00:38<01:29,  6.37s/it]\u001B[A\n",
      "episode_reward_mean = 86.62580871582031:  35%|███▌      | 7/20 [00:44<01:23,  6.40s/it] \u001B[A\n",
      "episode_reward_mean = 75.59244537353516:  40%|████      | 8/20 [00:51<01:17,  6.44s/it]\u001B[A\n",
      "episode_reward_mean = 91.61475372314453:  45%|████▌     | 9/20 [00:57<01:10,  6.44s/it]\u001B[A\n",
      "episode_reward_mean = 109.89176940917969:  50%|█████     | 10/20 [01:04<01:04,  6.45s/it]\u001B[A\n",
      "episode_reward_mean = 78.87167358398438:  55%|█████▌    | 11/20 [01:10<00:58,  6.45s/it] \u001B[A\n",
      "episode_reward_mean = 100.0819320678711:  60%|██████    | 12/20 [01:16<00:51,  6.43s/it]\u001B[A\n",
      "episode_reward_mean = 85.21359252929688:  65%|██████▌   | 13/20 [01:23<00:45,  6.44s/it]\u001B[A\n",
      "episode_reward_mean = 76.73007202148438:  70%|███████   | 14/20 [01:29<00:38,  6.42s/it]\u001B[A\n",
      "episode_reward_mean = 98.6054458618164:  75%|███████▌  | 15/20 [01:35<00:31,  6.37s/it] \u001B[A\n",
      "episode_reward_mean = 94.43446350097656:  80%|████████  | 16/20 [01:42<00:25,  6.35s/it]\u001B[A\n",
      "episode_reward_mean = 103.24154663085938:  85%|████████▌ | 17/20 [01:48<00:19,  6.39s/it]\u001B[A\n",
      "episode_reward_mean = 99.89403533935547:  90%|█████████ | 18/20 [01:55<00:12,  6.43s/it] \u001B[A\n",
      "episode_reward_mean = 92.3861083984375:  95%|█████████▌| 19/20 [02:01<00:06,  6.47s/it] \u001B[A\n",
      "episode_reward_mean = 104.84627532958984: 100%|██████████| 20/20 [02:08<00:00,  6.41s/it]\u001B[A\n",
      "2025-08-20 22:46:06,734 [torchrl][INFO] Training time: 75.84 seconds\n",
      "2025-08-20 22:46:06,747 [torchrl][INFO] macs: 235.02 MMac  Params: 24.13 k\n",
      "2025-08-20 22:46:09,535 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:46:09,637 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 88.69599914550781:   5%|▌         | 1/20 [00:06<01:59,  6.28s/it]\u001B[A\n",
      "episode_reward_mean = 82.75740051269531:  10%|█         | 2/20 [00:12<01:54,  6.37s/it]\u001B[A\n",
      "episode_reward_mean = 78.05323791503906:  15%|█▌        | 3/20 [00:19<01:48,  6.38s/it]\u001B[A\n",
      "episode_reward_mean = 97.35015869140625:  20%|██        | 4/20 [00:25<01:42,  6.39s/it]\u001B[A\n",
      "episode_reward_mean = 87.6684799194336:  25%|██▌       | 5/20 [00:31<01:35,  6.40s/it] \u001B[A\n",
      "episode_reward_mean = 86.93289184570312:  30%|███       | 6/20 [00:38<01:29,  6.42s/it]\u001B[A\n",
      "episode_reward_mean = 78.49093627929688:  35%|███▌      | 7/20 [00:44<01:23,  6.40s/it]\u001B[A\n",
      "episode_reward_mean = 71.84180450439453:  40%|████      | 8/20 [00:51<01:16,  6.39s/it]\u001B[A\n",
      "episode_reward_mean = 84.31678771972656:  45%|████▌     | 9/20 [00:57<01:10,  6.42s/it]\u001B[A\n",
      "episode_reward_mean = 78.13114166259766:  50%|█████     | 10/20 [01:04<01:04,  6.45s/it]\u001B[A\n",
      "episode_reward_mean = 94.5625228881836:  55%|█████▌    | 11/20 [01:10<00:57,  6.43s/it] \u001B[A\n",
      "episode_reward_mean = 89.0978012084961:  60%|██████    | 12/20 [01:17<00:51,  6.47s/it]\u001B[A\n",
      "episode_reward_mean = 92.88477325439453:  65%|██████▌   | 13/20 [01:23<00:45,  6.49s/it]\u001B[A\n",
      "episode_reward_mean = 72.09980010986328:  70%|███████   | 14/20 [01:29<00:38,  6.45s/it]\u001B[A\n",
      "episode_reward_mean = 70.46472930908203:  75%|███████▌  | 15/20 [01:36<00:32,  6.47s/it]\u001B[A\n",
      "episode_reward_mean = 91.26689910888672:  80%|████████  | 16/20 [01:42<00:25,  6.48s/it]\u001B[A\n",
      "episode_reward_mean = 73.62736511230469:  85%|████████▌ | 17/20 [01:49<00:19,  6.46s/it]\u001B[A\n",
      "episode_reward_mean = 78.24102783203125:  90%|█████████ | 18/20 [01:55<00:12,  6.43s/it]\u001B[A\n",
      "episode_reward_mean = 75.09112548828125:  95%|█████████▌| 19/20 [02:02<00:06,  6.47s/it]\u001B[A\n",
      "episode_reward_mean = 94.08470153808594: 100%|██████████| 20/20 [02:08<00:00,  6.44s/it]\u001B[A\n",
      "2025-08-20 22:48:18,506 [torchrl][INFO] Training time: 77.53 seconds\n",
      "2025-08-20 22:48:18,520 [torchrl][INFO] macs: 235.02 MMac  Params: 24.13 k\n",
      "2025-08-20 22:48:21,172 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:48:21,217 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -8.453940391540527:   2%|▏         | 1/60 [00:02<02:15,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = -11.521111488342285:   3%|▎         | 2/60 [00:04<02:13,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = -10.832947731018066:   5%|▌         | 3/60 [00:06<02:10,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = -9.736932754516602:   7%|▋         | 4/60 [00:09<02:08,  2.30s/it] \u001B[A\n",
      "episode_reward_mean = -10.58211898803711:   8%|▊         | 5/60 [00:11<02:09,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = -6.869636058807373:  10%|█         | 6/60 [00:13<02:05,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = -4.973878383636475:  12%|█▏        | 7/60 [00:16<02:02,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = -5.8039774894714355:  13%|█▎        | 8/60 [00:18<02:01,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = -5.129594802856445:  15%|█▌        | 9/60 [00:20<01:58,  2.33s/it] \u001B[A\n",
      "episode_reward_mean = -5.356657028198242:  17%|█▋        | 10/60 [00:23<01:55,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = -4.704779148101807:  18%|█▊        | 11/60 [00:25<01:52,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = -1.3942856788635254:  20%|██        | 12/60 [00:27<01:49,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = -0.023150276392698288:  22%|██▏       | 13/60 [00:29<01:46,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = 0.7713693976402283:  23%|██▎       | 14/60 [00:32<01:44,  2.28s/it]   \u001B[A\n",
      "episode_reward_mean = 6.067811012268066:  25%|██▌       | 15/60 [00:34<01:42,  2.28s/it] \u001B[A\n",
      "episode_reward_mean = 9.58237361907959:  27%|██▋       | 16/60 [00:36<01:40,  2.28s/it] \u001B[A\n",
      "episode_reward_mean = 14.900808334350586:  28%|██▊       | 17/60 [00:38<01:36,  2.25s/it]\u001B[A\n",
      "episode_reward_mean = 19.589637756347656:  30%|███       | 18/60 [00:41<01:34,  2.24s/it]\u001B[A\n",
      "episode_reward_mean = 28.722759246826172:  32%|███▏      | 19/60 [00:43<01:32,  2.25s/it]\u001B[A\n",
      "episode_reward_mean = 36.824119567871094:  33%|███▎      | 20/60 [00:45<01:31,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 38.76155090332031:  35%|███▌      | 21/60 [00:48<01:29,  2.28s/it] \u001B[A\n",
      "episode_reward_mean = 39.505279541015625:  37%|███▋      | 22/60 [00:50<01:26,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 45.37361145019531:  38%|███▊      | 23/60 [00:52<01:24,  2.28s/it] \u001B[A\n",
      "episode_reward_mean = 43.19065856933594:  40%|████      | 24/60 [00:54<01:21,  2.25s/it]\u001B[A\n",
      "episode_reward_mean = 50.83069610595703:  42%|████▏     | 25/60 [00:57<01:18,  2.24s/it]\u001B[A\n",
      "episode_reward_mean = 51.72962188720703:  43%|████▎     | 26/60 [00:59<01:16,  2.25s/it]\u001B[A\n",
      "episode_reward_mean = 58.862091064453125:  45%|████▌     | 27/60 [01:01<01:14,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = 60.865928649902344:  47%|████▋     | 28/60 [01:03<01:12,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 56.596466064453125:  48%|████▊     | 29/60 [01:06<01:10,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 64.01152038574219:  50%|█████     | 30/60 [01:08<01:08,  2.30s/it] \u001B[A\n",
      "episode_reward_mean = 69.24066162109375:  52%|█████▏    | 31/60 [01:10<01:06,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 81.53694915771484:  53%|█████▎    | 32/60 [01:13<01:04,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 73.43042755126953:  55%|█████▌    | 33/60 [01:15<01:01,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 78.10017395019531:  57%|█████▋    | 34/60 [01:17<01:00,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 76.9614486694336:  58%|█████▊    | 35/60 [01:20<00:58,  2.34s/it] \u001B[A\n",
      "episode_reward_mean = 85.93597412109375:  60%|██████    | 36/60 [01:22<00:55,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 79.40271759033203:  62%|██████▏   | 37/60 [01:24<00:52,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 84.02339172363281:  63%|██████▎   | 38/60 [01:26<00:50,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 90.29013061523438:  65%|██████▌   | 39/60 [01:29<00:47,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 90.63240051269531:  67%|██████▋   | 40/60 [01:31<00:45,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = 91.1601791381836:  68%|██████▊   | 41/60 [01:33<00:42,  2.25s/it] \u001B[A\n",
      "episode_reward_mean = 85.02197265625:  70%|███████   | 42/60 [01:35<00:40,  2.27s/it]  \u001B[A\n",
      "episode_reward_mean = 86.5646743774414:  72%|███████▏  | 43/60 [01:38<00:38,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 89.15130615234375:  73%|███████▎  | 44/60 [01:40<00:36,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 90.62110900878906:  75%|███████▌  | 45/60 [01:42<00:34,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 93.94925689697266:  77%|███████▋  | 46/60 [01:45<00:32,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 93.3557357788086:  78%|███████▊  | 47/60 [01:47<00:29,  2.29s/it] \u001B[A\n",
      "episode_reward_mean = 95.68080139160156:  80%|████████  | 48/60 [01:49<00:27,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 94.64981842041016:  82%|████████▏ | 49/60 [01:51<00:25,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 99.1709976196289:  83%|████████▎ | 50/60 [01:54<00:22,  2.30s/it] \u001B[A\n",
      "episode_reward_mean = 101.18521118164062:  85%|████████▌ | 51/60 [01:56<00:21,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 92.10574340820312:  87%|████████▋ | 52/60 [01:59<00:18,  2.32s/it] \u001B[A\n",
      "episode_reward_mean = 96.20357513427734:  88%|████████▊ | 53/60 [02:01<00:16,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 97.05815124511719:  90%|█████████ | 54/60 [02:03<00:13,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 95.80242156982422:  92%|█████████▏| 55/60 [02:05<00:11,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = 99.63301849365234:  93%|█████████▎| 56/60 [02:07<00:09,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 94.68545532226562:  95%|█████████▌| 57/60 [02:10<00:06,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = 95.91870880126953:  97%|█████████▋| 58/60 [02:12<00:04,  2.25s/it]\u001B[A\n",
      "episode_reward_mean = 95.68272399902344:  98%|█████████▊| 59/60 [02:14<00:02,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = 93.82089233398438: 100%|██████████| 60/60 [02:17<00:00,  2.28s/it]\u001B[A\n",
      "2025-08-20 22:50:38,265 [torchrl][INFO] Training time: 78.79 seconds\n",
      "2025-08-20 22:50:38,271 [torchrl][INFO] macs: 62.9 MMac  Params: 57.54 k\n",
      "2025-08-20 22:50:39,361 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:50:39,410 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -9.537946701049805:   2%|▏         | 1/60 [00:02<02:17,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = -9.928342819213867:   3%|▎         | 2/60 [00:04<02:14,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = -6.774143695831299:   5%|▌         | 3/60 [00:06<02:12,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = -3.3434503078460693:   7%|▋         | 4/60 [00:09<02:12,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = -0.27974918484687805:   8%|▊         | 5/60 [00:11<02:07,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = -1.9638280868530273:  10%|█         | 6/60 [00:13<02:04,  2.30s/it] \u001B[A\n",
      "episode_reward_mean = 1.8029136657714844:  12%|█▏        | 7/60 [00:16<02:01,  2.29s/it] \u001B[A\n",
      "episode_reward_mean = 4.297289848327637:  13%|█▎        | 8/60 [00:18<02:01,  2.34s/it] \u001B[A\n",
      "episode_reward_mean = 8.327436447143555:  15%|█▌        | 9/60 [00:20<01:58,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 12.399663925170898:  17%|█▋        | 10/60 [00:23<01:56,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 17.909271240234375:  18%|█▊        | 11/60 [00:25<01:54,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 25.627079010009766:  20%|██        | 12/60 [00:27<01:52,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 29.00360679626465:  22%|██▏       | 13/60 [00:30<01:50,  2.34s/it] \u001B[A\n",
      "episode_reward_mean = 29.433971405029297:  23%|██▎       | 14/60 [00:32<01:47,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 30.75045394897461:  25%|██▌       | 15/60 [00:34<01:44,  2.32s/it] \u001B[A\n",
      "episode_reward_mean = 33.30253219604492:  27%|██▋       | 16/60 [00:37<01:41,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 33.335941314697266:  28%|██▊       | 17/60 [00:39<01:39,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 42.89932632446289:  30%|███       | 18/60 [00:41<01:37,  2.32s/it] \u001B[A\n",
      "episode_reward_mean = 49.72160339355469:  32%|███▏      | 19/60 [00:44<01:36,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 57.426876068115234:  33%|███▎      | 20/60 [00:46<01:32,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 63.46399688720703:  35%|███▌      | 21/60 [00:48<01:30,  2.32s/it] \u001B[A\n",
      "episode_reward_mean = 66.4578857421875:  37%|███▋      | 22/60 [00:51<01:28,  2.32s/it] \u001B[A\n",
      "episode_reward_mean = 65.00625610351562:  38%|███▊      | 23/60 [00:53<01:25,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 77.3781967163086:  40%|████      | 24/60 [00:55<01:23,  2.32s/it] \u001B[A\n",
      "episode_reward_mean = 78.43543243408203:  42%|████▏     | 25/60 [00:58<01:20,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 82.7491455078125:  43%|████▎     | 26/60 [01:00<01:18,  2.31s/it] \u001B[A\n",
      "episode_reward_mean = 86.58578491210938:  45%|████▌     | 27/60 [01:02<01:16,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 94.14850616455078:  47%|████▋     | 28/60 [01:05<01:14,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 82.09745788574219:  48%|████▊     | 29/60 [01:07<01:12,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 86.63414764404297:  50%|█████     | 30/60 [01:09<01:10,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 93.94073486328125:  52%|█████▏    | 31/60 [01:12<01:07,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 93.76187133789062:  53%|█████▎    | 32/60 [01:14<01:05,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 97.5130386352539:  55%|█████▌    | 33/60 [01:16<01:02,  2.32s/it] \u001B[A\n",
      "episode_reward_mean = 89.00586700439453:  57%|█████▋    | 34/60 [01:19<01:01,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 97.8980941772461:  58%|█████▊    | 35/60 [01:21<00:58,  2.35s/it] \u001B[A\n",
      "episode_reward_mean = 96.52423095703125:  60%|██████    | 36/60 [01:23<00:56,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 97.32653045654297:  62%|██████▏   | 37/60 [01:26<00:53,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 90.51134490966797:  63%|██████▎   | 38/60 [01:28<00:51,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 96.11203002929688:  65%|██████▌   | 39/60 [01:30<00:49,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 90.98639678955078:  67%|██████▋   | 40/60 [01:33<00:46,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 95.41211700439453:  68%|██████▊   | 41/60 [01:35<00:44,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 94.11486053466797:  70%|███████   | 42/60 [01:37<00:41,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 91.63064575195312:  72%|███████▏  | 43/60 [01:40<00:39,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 94.04725646972656:  73%|███████▎  | 44/60 [01:42<00:37,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 102.581787109375:  75%|███████▌  | 45/60 [01:44<00:34,  2.32s/it] \u001B[A\n",
      "episode_reward_mean = 93.8044662475586:  77%|███████▋  | 46/60 [01:47<00:32,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 101.14726257324219:  78%|███████▊  | 47/60 [01:49<00:29,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 95.54206085205078:  80%|████████  | 48/60 [01:51<00:27,  2.30s/it] \u001B[A\n",
      "episode_reward_mean = 99.23563385009766:  82%|████████▏ | 49/60 [01:53<00:25,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 93.07000732421875:  83%|████████▎ | 50/60 [01:56<00:23,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 106.95911407470703:  85%|████████▌ | 51/60 [01:58<00:20,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 100.51437377929688:  87%|████████▋ | 52/60 [02:00<00:18,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 93.78042602539062:  88%|████████▊ | 53/60 [02:03<00:15,  2.28s/it] \u001B[A\n",
      "episode_reward_mean = 107.61792755126953:  90%|█████████ | 54/60 [02:05<00:13,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 101.36967468261719:  92%|█████████▏| 55/60 [02:07<00:11,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 91.3930435180664:  93%|█████████▎| 56/60 [02:10<00:09,  2.31s/it]  \u001B[A\n",
      "episode_reward_mean = 111.64546966552734:  95%|█████████▌| 57/60 [02:12<00:06,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 100.2469482421875:  97%|█████████▋| 58/60 [02:14<00:04,  2.31s/it] \u001B[A\n",
      "episode_reward_mean = 103.2254638671875:  98%|█████████▊| 59/60 [02:16<00:02,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 104.4798355102539: 100%|██████████| 60/60 [02:19<00:00,  2.32s/it]\u001B[A\n",
      "2025-08-20 22:52:58,712 [torchrl][INFO] Training time: 80.42 seconds\n",
      "2025-08-20 22:52:58,718 [torchrl][INFO] macs: 62.9 MMac  Params: 57.54 k\n",
      "2025-08-20 22:52:59,818 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:52:59,861 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -11.659529685974121:   2%|▏         | 1/60 [00:02<02:13,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = -9.359892845153809:   3%|▎         | 2/60 [00:04<02:14,  2.31s/it] \u001B[A\n",
      "episode_reward_mean = -9.169668197631836:   5%|▌         | 3/60 [00:06<02:11,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = -7.846251010894775:   7%|▋         | 4/60 [00:09<02:09,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = -5.5644989013671875:   8%|▊         | 5/60 [00:11<02:06,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = -4.995756149291992:  10%|█         | 6/60 [00:13<02:03,  2.29s/it] \u001B[A\n",
      "episode_reward_mean = -4.6390252113342285:  12%|█▏        | 7/60 [00:16<02:00,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 0.12911838293075562:  13%|█▎        | 8/60 [00:18<01:58,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 0.008965712040662766:  15%|█▌        | 9/60 [00:20<01:55,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = 4.125644207000732:  17%|█▋        | 10/60 [00:22<01:53,  2.26s/it]  \u001B[A\n",
      "episode_reward_mean = 5.596941947937012:  18%|█▊        | 11/60 [00:25<01:50,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = 14.628034591674805:  20%|██        | 12/60 [00:27<01:48,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 20.83612632751465:  22%|██▏       | 13/60 [00:29<01:46,  2.26s/it] \u001B[A\n",
      "episode_reward_mean = 25.70370864868164:  23%|██▎       | 14/60 [00:31<01:43,  2.25s/it]\u001B[A\n",
      "episode_reward_mean = 27.696762084960938:  25%|██▌       | 15/60 [00:34<01:41,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = 31.597700119018555:  27%|██▋       | 16/60 [00:36<01:38,  2.24s/it]\u001B[A\n",
      "episode_reward_mean = 41.6074333190918:  28%|██▊       | 17/60 [00:38<01:37,  2.28s/it]  \u001B[A\n",
      "episode_reward_mean = 35.95064163208008:  30%|███       | 18/60 [00:40<01:35,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 54.15977096557617:  32%|███▏      | 19/60 [00:43<01:33,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 45.00606155395508:  33%|███▎      | 20/60 [00:45<01:30,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 55.67048263549805:  35%|███▌      | 21/60 [00:47<01:28,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 45.91201400756836:  37%|███▋      | 22/60 [00:49<01:26,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 66.52471160888672:  38%|███▊      | 23/60 [00:52<01:25,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 61.522525787353516:  40%|████      | 24/60 [00:54<01:22,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 76.95874786376953:  42%|████▏     | 25/60 [00:56<01:19,  2.28s/it] \u001B[A\n",
      "episode_reward_mean = 67.42192077636719:  43%|████▎     | 26/60 [00:59<01:17,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 77.63127899169922:  45%|████▌     | 27/60 [01:01<01:15,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 72.53919982910156:  47%|████▋     | 28/60 [01:03<01:13,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 79.24514770507812:  48%|████▊     | 29/60 [01:06<01:11,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 79.06237030029297:  50%|█████     | 30/60 [01:08<01:09,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 88.30718994140625:  52%|█████▏    | 31/60 [01:10<01:06,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 84.48970031738281:  53%|█████▎    | 32/60 [01:13<01:05,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 91.96285247802734:  55%|█████▌    | 33/60 [01:15<01:02,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 88.95016479492188:  57%|█████▋    | 34/60 [01:17<00:59,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 89.69865417480469:  58%|█████▊    | 35/60 [01:19<00:57,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 87.41041564941406:  60%|██████    | 36/60 [01:22<00:54,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 92.89803314208984:  62%|██████▏   | 37/60 [01:24<00:52,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 94.94361114501953:  63%|██████▎   | 38/60 [01:26<00:50,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 90.79452514648438:  65%|██████▌   | 39/60 [01:28<00:47,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = 89.72531127929688:  67%|██████▋   | 40/60 [01:31<00:45,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = 96.1307144165039:  68%|██████▊   | 41/60 [01:33<00:43,  2.27s/it] \u001B[A\n",
      "episode_reward_mean = 97.84785461425781:  70%|███████   | 42/60 [01:35<00:40,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = 98.42819213867188:  72%|███████▏  | 43/60 [01:38<00:38,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 94.78421783447266:  73%|███████▎  | 44/60 [01:40<00:36,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 94.68878173828125:  75%|███████▌  | 45/60 [01:42<00:34,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 104.91328430175781:  77%|███████▋  | 46/60 [01:44<00:31,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 95.83296966552734:  78%|███████▊  | 47/60 [01:47<00:30,  2.31s/it] \u001B[A\n",
      "episode_reward_mean = 86.52576446533203:  80%|████████  | 48/60 [01:49<00:27,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 94.07075500488281:  82%|████████▏ | 49/60 [01:51<00:25,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 103.50848388671875:  83%|████████▎ | 50/60 [01:54<00:23,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 92.94186401367188:  85%|████████▌ | 51/60 [01:56<00:20,  2.32s/it] \u001B[A\n",
      "episode_reward_mean = 89.88346099853516:  87%|████████▋ | 52/60 [01:58<00:18,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 96.34300231933594:  88%|████████▊ | 53/60 [02:01<00:16,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 96.3936538696289:  90%|█████████ | 54/60 [02:03<00:13,  2.30s/it] \u001B[A\n",
      "episode_reward_mean = 103.50533294677734:  92%|█████████▏| 55/60 [02:05<00:11,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 99.07431030273438:  93%|█████████▎| 56/60 [02:08<00:09,  2.31s/it] \u001B[A\n",
      "episode_reward_mean = 88.2165756225586:  95%|█████████▌| 57/60 [02:10<00:06,  2.33s/it] \u001B[A\n",
      "episode_reward_mean = 88.81497955322266:  97%|█████████▋| 58/60 [02:12<00:04,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 96.68986511230469:  98%|█████████▊| 59/60 [02:14<00:02,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 95.44767761230469: 100%|██████████| 60/60 [02:17<00:00,  2.29s/it]\u001B[A\n",
      "2025-08-20 22:55:17,125 [torchrl][INFO] Training time: 79.20 seconds\n",
      "2025-08-20 22:55:17,132 [torchrl][INFO] macs: 62.9 MMac  Params: 57.54 k\n",
      "2025-08-20 22:55:18,334 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:55:18,386 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -8.124335289001465:   2%|▏         | 1/60 [00:02<02:13,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = -8.231444358825684:   3%|▎         | 2/60 [00:04<02:12,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = -7.309197902679443:   5%|▌         | 3/60 [00:06<02:10,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = -6.08270788192749:   7%|▋         | 4/60 [00:09<02:10,  2.33s/it] \u001B[A\n",
      "episode_reward_mean = -3.429083824157715:   8%|▊         | 5/60 [00:11<02:07,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = -7.3784942626953125:  10%|█         | 6/60 [00:13<02:05,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = -2.4494330883026123:  12%|█▏        | 7/60 [00:16<02:03,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = -1.264485478401184:  13%|█▎        | 8/60 [00:18<01:59,  2.30s/it] \u001B[A\n",
      "episode_reward_mean = 0.34622156620025635:  15%|█▌        | 9/60 [00:20<01:56,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 2.8591644763946533:  17%|█▋        | 10/60 [00:22<01:54,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 9.656608581542969:  18%|█▊        | 11/60 [00:25<01:52,  2.30s/it] \u001B[A\n",
      "episode_reward_mean = 12.353530883789062:  20%|██        | 12/60 [00:27<01:50,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 22.241315841674805:  22%|██▏       | 13/60 [00:29<01:47,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 21.67678451538086:  23%|██▎       | 14/60 [00:32<01:46,  2.31s/it] \u001B[A\n",
      "episode_reward_mean = 21.46543312072754:  25%|██▌       | 15/60 [00:34<01:45,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 27.150911331176758:  27%|██▋       | 16/60 [00:36<01:42,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 29.17639923095703:  28%|██▊       | 17/60 [00:39<01:39,  2.31s/it] \u001B[A\n",
      "episode_reward_mean = 40.28065490722656:  30%|███       | 18/60 [00:41<01:36,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 30.77408790588379:  32%|███▏      | 19/60 [00:43<01:34,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 35.492340087890625:  33%|███▎      | 20/60 [00:46<01:31,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 39.474029541015625:  35%|███▌      | 21/60 [00:48<01:27,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = 44.28279495239258:  37%|███▋      | 22/60 [00:50<01:26,  2.27s/it] \u001B[A\n",
      "episode_reward_mean = 51.82057571411133:  38%|███▊      | 23/60 [00:52<01:24,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 63.5577278137207:  40%|████      | 24/60 [00:55<01:22,  2.28s/it] \u001B[A\n",
      "episode_reward_mean = 56.75418472290039:  42%|████▏     | 25/60 [00:57<01:19,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 66.42973327636719:  43%|████▎     | 26/60 [00:59<01:17,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 67.90364074707031:  45%|████▌     | 27/60 [01:02<01:15,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 81.35128784179688:  47%|████▋     | 28/60 [01:04<01:13,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 73.50904846191406:  48%|████▊     | 29/60 [01:06<01:11,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 76.72293090820312:  50%|█████     | 30/60 [01:08<01:09,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 85.12950897216797:  52%|█████▏    | 31/60 [01:11<01:06,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 84.69683837890625:  53%|█████▎    | 32/60 [01:13<01:05,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 89.67010498046875:  55%|█████▌    | 33/60 [01:15<01:02,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 88.09883880615234:  57%|█████▋    | 34/60 [01:18<01:00,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 91.16508483886719:  58%|█████▊    | 35/60 [01:20<00:58,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 90.59932708740234:  60%|██████    | 36/60 [01:22<00:55,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 97.15392303466797:  62%|██████▏   | 37/60 [01:25<00:52,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 98.7760238647461:  63%|██████▎   | 38/60 [01:27<00:50,  2.29s/it] \u001B[A\n",
      "episode_reward_mean = 89.8042221069336:  65%|██████▌   | 39/60 [01:29<00:48,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 91.46920776367188:  67%|██████▋   | 40/60 [01:31<00:45,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 88.55070495605469:  68%|██████▊   | 41/60 [01:34<00:43,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 91.62155151367188:  70%|███████   | 42/60 [01:36<00:41,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 89.25231170654297:  72%|███████▏  | 43/60 [01:38<00:39,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 91.82852935791016:  73%|███████▎  | 44/60 [01:41<00:37,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 96.29008483886719:  75%|███████▌  | 45/60 [01:43<00:34,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 95.61443328857422:  77%|███████▋  | 46/60 [01:45<00:32,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 99.61080169677734:  78%|███████▊  | 47/60 [01:48<00:30,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 101.95182800292969:  80%|████████  | 48/60 [01:50<00:27,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 96.68761444091797:  82%|████████▏ | 49/60 [01:52<00:25,  2.32s/it] \u001B[A\n",
      "episode_reward_mean = 94.81714630126953:  83%|████████▎ | 50/60 [01:55<00:23,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 86.7726058959961:  85%|████████▌ | 51/60 [01:57<00:20,  2.31s/it] \u001B[A\n",
      "episode_reward_mean = 80.8989486694336:  87%|████████▋ | 52/60 [01:59<00:18,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 96.34019470214844:  88%|████████▊ | 53/60 [02:02<00:15,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 95.30181121826172:  90%|█████████ | 54/60 [02:04<00:13,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 102.99317169189453:  92%|█████████▏| 55/60 [02:06<00:11,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 102.12541961669922:  93%|█████████▎| 56/60 [02:08<00:09,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 98.8815689086914:  95%|█████████▌| 57/60 [02:11<00:06,  2.28s/it]  \u001B[A\n",
      "episode_reward_mean = 99.16136169433594:  97%|█████████▋| 58/60 [02:13<00:04,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 103.86463928222656:  98%|█████████▊| 59/60 [02:15<00:02,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 105.78826141357422: 100%|██████████| 60/60 [02:18<00:00,  2.30s/it]\u001B[A\n",
      "2025-08-20 22:57:36,554 [torchrl][INFO] Training time: 80.06 seconds\n",
      "2025-08-20 22:57:36,560 [torchrl][INFO] macs: 62.9 MMac  Params: 57.54 k\n",
      "2025-08-20 22:57:37,648 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:57:37,692 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -12.189360618591309:   2%|▏         | 1/60 [00:02<02:17,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = -10.682466506958008:   3%|▎         | 2/60 [00:04<02:14,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = -7.650559902191162:   5%|▌         | 3/60 [00:06<02:11,  2.30s/it] \u001B[A\n",
      "episode_reward_mean = -8.357336044311523:   7%|▋         | 4/60 [00:09<02:09,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = -7.620173454284668:   8%|▊         | 5/60 [00:11<02:05,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = -8.251910209655762:  10%|█         | 6/60 [00:13<02:03,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = -5.620365619659424:  12%|█▏        | 7/60 [00:16<02:03,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = -5.011425018310547:  13%|█▎        | 8/60 [00:18<02:00,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 0.00294151296839118:  15%|█▌        | 9/60 [00:20<01:57,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 0.47319716215133667:  17%|█▋        | 10/60 [00:23<01:54,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 8.16097354888916:  18%|█▊        | 11/60 [00:25<01:52,  2.30s/it]   \u001B[A\n",
      "episode_reward_mean = 11.154816627502441:  20%|██        | 12/60 [00:27<01:52,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 15.928375244140625:  22%|██▏       | 13/60 [00:30<01:49,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 20.96221160888672:  23%|██▎       | 14/60 [00:32<01:46,  2.31s/it] \u001B[A\n",
      "episode_reward_mean = 22.923534393310547:  25%|██▌       | 15/60 [00:34<01:43,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 26.689208984375:  27%|██▋       | 16/60 [00:37<01:42,  2.32s/it]   \u001B[A\n",
      "episode_reward_mean = 30.066930770874023:  28%|██▊       | 17/60 [00:39<01:39,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 34.022987365722656:  30%|███       | 18/60 [00:41<01:37,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 39.72300338745117:  32%|███▏      | 19/60 [00:43<01:34,  2.30s/it] \u001B[A\n",
      "episode_reward_mean = 44.46529006958008:  33%|███▎      | 20/60 [00:46<01:31,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 50.30255889892578:  35%|███▌      | 21/60 [00:48<01:29,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 47.70610046386719:  37%|███▋      | 22/60 [00:50<01:27,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 52.289955139160156:  38%|███▊      | 23/60 [00:53<01:25,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 57.668861389160156:  40%|████      | 24/60 [00:55<01:23,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 63.40899658203125:  42%|████▏     | 25/60 [00:57<01:20,  2.30s/it] \u001B[A\n",
      "episode_reward_mean = 67.83761596679688:  43%|████▎     | 26/60 [01:00<01:19,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 80.986328125:  45%|████▌     | 27/60 [01:02<01:16,  2.31s/it]     \u001B[A\n",
      "episode_reward_mean = 75.48258972167969:  47%|████▋     | 28/60 [01:04<01:13,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 67.82691192626953:  48%|████▊     | 29/60 [01:06<01:10,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 88.35606384277344:  50%|█████     | 30/60 [01:09<01:08,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 85.28607940673828:  52%|█████▏    | 31/60 [01:11<01:06,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 87.7451400756836:  53%|█████▎    | 32/60 [01:13<01:03,  2.28s/it] \u001B[A\n",
      "episode_reward_mean = 86.69371032714844:  55%|█████▌    | 33/60 [01:16<01:01,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 88.22400665283203:  57%|█████▋    | 34/60 [01:18<00:59,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 94.39605712890625:  58%|█████▊    | 35/60 [01:20<00:56,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = 93.31134796142578:  60%|██████    | 36/60 [01:22<00:54,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = 95.15507507324219:  62%|██████▏   | 37/60 [01:25<00:52,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 91.16116333007812:  63%|██████▎   | 38/60 [01:27<00:49,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 91.82035827636719:  65%|██████▌   | 39/60 [01:29<00:47,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 91.81745147705078:  67%|██████▋   | 40/60 [01:31<00:45,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 97.21235656738281:  68%|██████▊   | 41/60 [01:34<00:44,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 98.33738708496094:  70%|███████   | 42/60 [01:36<00:41,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 99.02674865722656:  72%|███████▏  | 43/60 [01:38<00:38,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 89.99589538574219:  73%|███████▎  | 44/60 [01:41<00:36,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 105.34776306152344:  75%|███████▌  | 45/60 [01:43<00:34,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 98.1715087890625:  77%|███████▋  | 46/60 [01:46<00:33,  2.40s/it]  \u001B[A\n",
      "episode_reward_mean = 90.76538848876953:  78%|███████▊  | 47/60 [01:48<00:31,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 104.2828140258789:  80%|████████  | 48/60 [01:50<00:28,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 89.2787857055664:  82%|████████▏ | 49/60 [01:53<00:25,  2.33s/it] \u001B[A\n",
      "episode_reward_mean = 97.72785949707031:  83%|████████▎ | 50/60 [01:55<00:23,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 87.43544006347656:  85%|████████▌ | 51/60 [01:57<00:20,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 95.79525756835938:  87%|████████▋ | 52/60 [02:00<00:18,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 91.86183166503906:  88%|████████▊ | 53/60 [02:02<00:16,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 102.7212142944336:  90%|█████████ | 54/60 [02:04<00:14,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 97.67140197753906:  92%|█████████▏| 55/60 [02:07<00:11,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 104.42063903808594:  93%|█████████▎| 56/60 [02:09<00:09,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 100.03849792480469:  95%|█████████▌| 57/60 [02:11<00:06,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 98.37728118896484:  97%|█████████▋| 58/60 [02:13<00:04,  2.31s/it] \u001B[A\n",
      "episode_reward_mean = 91.45392608642578:  98%|█████████▊| 59/60 [02:16<00:02,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 98.75361633300781: 100%|██████████| 60/60 [02:18<00:00,  2.31s/it]\u001B[A\n",
      "2025-08-20 22:59:56,229 [torchrl][INFO] Training time: 79.92 seconds\n",
      "2025-08-20 22:59:56,234 [torchrl][INFO] macs: 62.9 MMac  Params: 57.54 k\n",
      "2025-08-20 22:59:57,302 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 22:59:57,352 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -12.488940238952637:   2%|▏         | 1/60 [00:02<02:23,  2.44s/it]\u001B[A\n",
      "episode_reward_mean = -11.465271949768066:   3%|▎         | 2/60 [00:04<02:19,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = -11.784276962280273:   5%|▌         | 3/60 [00:07<02:13,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = -11.143769264221191:   7%|▋         | 4/60 [00:09<02:10,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = -9.271770477294922:   8%|▊         | 5/60 [00:11<02:08,  2.34s/it] \u001B[A\n",
      "episode_reward_mean = -7.8716888427734375:  10%|█         | 6/60 [00:14<02:07,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = -9.08812141418457:  12%|█▏        | 7/60 [00:16<02:03,  2.33s/it]  \u001B[A\n",
      "episode_reward_mean = -5.571718215942383:  13%|█▎        | 8/60 [00:18<02:00,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = -6.962219715118408:  15%|█▌        | 9/60 [00:21<01:58,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = -5.722599983215332:  17%|█▋        | 10/60 [00:23<01:56,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = -7.086280345916748:  18%|█▊        | 11/60 [00:25<01:52,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = -4.700970649719238:  20%|██        | 12/60 [00:27<01:49,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = -4.982798099517822:  22%|██▏       | 13/60 [00:30<01:48,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = -4.403463363647461:  23%|██▎       | 14/60 [00:32<01:46,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = -2.169517755508423:  25%|██▌       | 15/60 [00:34<01:45,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 0.14554695785045624:  27%|██▋       | 16/60 [00:37<01:42,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 6.798368453979492:  28%|██▊       | 17/60 [00:39<01:38,  2.30s/it]  \u001B[A\n",
      "episode_reward_mean = 10.997930526733398:  30%|███       | 18/60 [00:41<01:36,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 14.288568496704102:  32%|███▏      | 19/60 [00:44<01:34,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 20.032426834106445:  33%|███▎      | 20/60 [00:46<01:33,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 27.944808959960938:  35%|███▌      | 21/60 [00:48<01:31,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 16.84092140197754:  37%|███▋      | 22/60 [00:51<01:28,  2.33s/it] \u001B[A\n",
      "episode_reward_mean = 24.90372657775879:  38%|███▊      | 23/60 [00:53<01:26,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 40.32281494140625:  40%|████      | 24/60 [00:55<01:24,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 40.162864685058594:  42%|████▏     | 25/60 [00:58<01:21,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 33.66081237792969:  43%|████▎     | 26/60 [01:00<01:19,  2.34s/it] \u001B[A\n",
      "episode_reward_mean = 42.41856384277344:  45%|████▌     | 27/60 [01:02<01:17,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 43.42354202270508:  47%|████▋     | 28/60 [01:05<01:14,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 50.993900299072266:  48%|████▊     | 29/60 [01:07<01:12,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 56.17347717285156:  50%|█████     | 30/60 [01:09<01:10,  2.36s/it] \u001B[A\n",
      "episode_reward_mean = 63.102638244628906:  52%|█████▏    | 31/60 [01:12<01:08,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 48.423805236816406:  53%|█████▎    | 32/60 [01:14<01:05,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 66.092529296875:  55%|█████▌    | 33/60 [01:16<01:03,  2.34s/it]   \u001B[A\n",
      "episode_reward_mean = 64.90644073486328:  57%|█████▋    | 34/60 [01:19<01:00,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 62.363956451416016:  58%|█████▊    | 35/60 [01:21<00:58,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 55.64432907104492:  60%|██████    | 36/60 [01:23<00:55,  2.32s/it] \u001B[A\n",
      "episode_reward_mean = 71.47376251220703:  62%|██████▏   | 37/60 [01:26<00:54,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 65.03380584716797:  63%|██████▎   | 38/60 [01:28<00:51,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 72.7094955444336:  65%|██████▌   | 39/60 [01:31<00:49,  2.35s/it] \u001B[A\n",
      "episode_reward_mean = 74.14982604980469:  67%|██████▋   | 40/60 [01:33<00:46,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 78.39781951904297:  68%|██████▊   | 41/60 [01:35<00:44,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 73.3974380493164:  70%|███████   | 42/60 [01:37<00:41,  2.32s/it] \u001B[A\n",
      "episode_reward_mean = 77.01512908935547:  72%|███████▏  | 43/60 [01:40<00:39,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 78.965576171875:  73%|███████▎  | 44/60 [01:42<00:36,  2.30s/it]  \u001B[A\n",
      "episode_reward_mean = 86.96122741699219:  75%|███████▌  | 45/60 [01:44<00:34,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 82.80532836914062:  77%|███████▋  | 46/60 [01:47<00:32,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 84.11212158203125:  78%|███████▊  | 47/60 [01:49<00:30,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 83.66361236572266:  80%|████████  | 48/60 [01:51<00:27,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 82.1229019165039:  82%|████████▏ | 49/60 [01:54<00:25,  2.33s/it] \u001B[A\n",
      "episode_reward_mean = 93.90827941894531:  83%|████████▎ | 50/60 [01:56<00:23,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 87.4464111328125:  85%|████████▌ | 51/60 [01:58<00:20,  2.30s/it] \u001B[A\n",
      "episode_reward_mean = 89.4921646118164:  87%|████████▋ | 52/60 [02:01<00:18,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 84.96949005126953:  88%|████████▊ | 53/60 [02:03<00:16,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 88.61634826660156:  90%|█████████ | 54/60 [02:05<00:13,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 88.79312896728516:  92%|█████████▏| 55/60 [02:07<00:11,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 94.16295623779297:  93%|█████████▎| 56/60 [02:10<00:09,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 86.85340118408203:  95%|█████████▌| 57/60 [02:12<00:06,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 97.39375305175781:  97%|█████████▋| 58/60 [02:14<00:04,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 94.34912109375:  98%|█████████▊| 59/60 [02:17<00:02,  2.28s/it]   \u001B[A\n",
      "episode_reward_mean = 94.48458099365234: 100%|██████████| 60/60 [02:19<00:00,  2.32s/it]\u001B[A\n",
      "2025-08-20 23:02:16,752 [torchrl][INFO] Training time: 80.36 seconds\n",
      "2025-08-20 23:02:16,760 [torchrl][INFO] macs: 62.9 MMac  Params: 57.54 k\n",
      "2025-08-20 23:02:17,838 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 23:02:17,881 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -10.424039840698242:   2%|▏         | 1/60 [00:02<02:14,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = -10.606161117553711:   3%|▎         | 2/60 [00:04<02:12,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = -9.73185920715332:   5%|▌         | 3/60 [00:07<02:15,  2.37s/it]  \u001B[A\n",
      "episode_reward_mean = -9.182640075683594:   7%|▋         | 4/60 [00:09<02:11,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = -9.096549987792969:   8%|▊         | 5/60 [00:11<02:07,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = -7.817607402801514:  10%|█         | 6/60 [00:13<02:05,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = -7.986297130584717:  12%|█▏        | 7/60 [00:16<02:03,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = -9.169530868530273:  13%|█▎        | 8/60 [00:18<02:01,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = -7.6958513259887695:  15%|█▌        | 9/60 [00:20<01:58,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = -5.843103408813477:  17%|█▋        | 10/60 [00:23<01:56,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 0.07832546532154083:  18%|█▊        | 11/60 [00:25<01:54,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 1.8050816059112549:  20%|██        | 12/60 [00:27<01:51,  2.33s/it] \u001B[A\n",
      "episode_reward_mean = 1.616716742515564:  22%|██▏       | 13/60 [00:30<01:51,  2.37s/it] \u001B[A\n",
      "episode_reward_mean = 7.698711395263672:  23%|██▎       | 14/60 [00:32<01:48,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 8.269888877868652:  25%|██▌       | 15/60 [00:35<01:45,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 12.57446575164795:  27%|██▋       | 16/60 [00:37<01:42,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 14.468423843383789:  28%|██▊       | 17/60 [00:39<01:40,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 16.643003463745117:  30%|███       | 18/60 [00:42<01:37,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 21.760786056518555:  32%|███▏      | 19/60 [00:44<01:34,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 27.717998504638672:  33%|███▎      | 20/60 [00:46<01:31,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 24.45008087158203:  35%|███▌      | 21/60 [00:48<01:29,  2.30s/it] \u001B[A\n",
      "episode_reward_mean = 34.97932434082031:  37%|███▋      | 22/60 [00:51<01:28,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 28.871070861816406:  38%|███▊      | 23/60 [00:53<01:25,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 32.39768600463867:  40%|████      | 24/60 [00:55<01:23,  2.32s/it] \u001B[A\n",
      "episode_reward_mean = 39.85723114013672:  42%|████▏     | 25/60 [00:58<01:21,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 39.65559768676758:  43%|████▎     | 26/60 [01:00<01:20,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 48.7357063293457:  45%|████▌     | 27/60 [01:02<01:17,  2.34s/it] \u001B[A\n",
      "episode_reward_mean = 51.155494689941406:  47%|████▋     | 28/60 [01:05<01:14,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 53.37606430053711:  48%|████▊     | 29/60 [01:07<01:11,  2.31s/it] \u001B[A\n",
      "episode_reward_mean = 63.20134735107422:  50%|█████     | 30/60 [01:09<01:09,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 61.01163101196289:  52%|█████▏    | 31/60 [01:12<01:06,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 66.56587982177734:  53%|█████▎    | 32/60 [01:14<01:04,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 74.69684600830078:  55%|█████▌    | 33/60 [01:16<01:02,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 67.5943832397461:  57%|█████▋    | 34/60 [01:19<01:00,  2.31s/it] \u001B[A\n",
      "episode_reward_mean = 83.5548095703125:  58%|█████▊    | 35/60 [01:21<00:57,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 72.3128433227539:  60%|██████    | 36/60 [01:23<00:54,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 74.45024871826172:  62%|██████▏   | 37/60 [01:25<00:52,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 83.915771484375:  63%|██████▎   | 38/60 [01:28<00:50,  2.31s/it]  \u001B[A\n",
      "episode_reward_mean = 85.0051040649414:  65%|██████▌   | 39/60 [01:30<00:48,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 74.88145446777344:  67%|██████▋   | 40/60 [01:32<00:46,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 86.13097381591797:  68%|██████▊   | 41/60 [01:35<00:43,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 90.78694152832031:  70%|███████   | 42/60 [01:37<00:42,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 92.56997680664062:  72%|███████▏  | 43/60 [01:40<00:40,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 87.10079956054688:  73%|███████▎  | 44/60 [01:42<00:37,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 75.90814208984375:  75%|███████▌  | 45/60 [01:44<00:35,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 90.30982208251953:  77%|███████▋  | 46/60 [01:46<00:32,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 89.93394470214844:  78%|███████▊  | 47/60 [01:49<00:30,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 93.55447387695312:  80%|████████  | 48/60 [01:51<00:27,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 93.7387924194336:  82%|████████▏ | 49/60 [01:53<00:25,  2.32s/it] \u001B[A\n",
      "episode_reward_mean = 90.4243392944336:  83%|████████▎ | 50/60 [01:56<00:23,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 94.60478973388672:  85%|████████▌ | 51/60 [01:58<00:20,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 93.72499084472656:  87%|████████▋ | 52/60 [02:01<00:18,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 87.20330810546875:  88%|████████▊ | 53/60 [02:03<00:16,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 94.46540832519531:  90%|█████████ | 54/60 [02:05<00:14,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 77.70069885253906:  92%|█████████▏| 55/60 [02:07<00:11,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 90.80027770996094:  93%|█████████▎| 56/60 [02:10<00:09,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 84.43956756591797:  95%|█████████▌| 57/60 [02:12<00:06,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 91.52853393554688:  97%|█████████▋| 58/60 [02:14<00:04,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 92.44610595703125:  98%|█████████▊| 59/60 [02:17<00:02,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 95.50322723388672: 100%|██████████| 60/60 [02:19<00:00,  2.32s/it]\u001B[A\n",
      "2025-08-20 23:04:37,267 [torchrl][INFO] Training time: 79.93 seconds\n",
      "2025-08-20 23:04:37,273 [torchrl][INFO] macs: 62.9 MMac  Params: 57.54 k\n",
      "2025-08-20 23:04:38,513 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 23:04:38,559 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -12.593670845031738:   2%|▏         | 1/60 [00:02<02:14,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = -12.403238296508789:   3%|▎         | 2/60 [00:04<02:13,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = -11.49841594696045:   5%|▌         | 3/60 [00:06<02:08,  2.26s/it] \u001B[A\n",
      "episode_reward_mean = -9.097390174865723:   7%|▋         | 4/60 [00:08<02:04,  2.23s/it]\u001B[A\n",
      "episode_reward_mean = -7.011137962341309:   8%|▊         | 5/60 [00:11<02:02,  2.22s/it]\u001B[A\n",
      "episode_reward_mean = -6.282215595245361:  10%|█         | 6/60 [00:13<02:02,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = -4.7697930335998535:  12%|█▏        | 7/60 [00:15<01:59,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = -8.324071884155273:  13%|█▎        | 8/60 [00:18<01:57,  2.26s/it] \u001B[A\n",
      "episode_reward_mean = -6.785956859588623:  15%|█▌        | 9/60 [00:20<01:54,  2.24s/it]\u001B[A\n",
      "episode_reward_mean = -8.713111877441406:  17%|█▋        | 10/60 [00:22<01:51,  2.23s/it]\u001B[A\n",
      "episode_reward_mean = -7.005589485168457:  18%|█▊        | 11/60 [00:24<01:48,  2.22s/it]\u001B[A\n",
      "episode_reward_mean = -7.944240570068359:  20%|██        | 12/60 [00:26<01:46,  2.22s/it]\u001B[A\n",
      "episode_reward_mean = -8.970190048217773:  22%|██▏       | 13/60 [00:29<01:44,  2.22s/it]\u001B[A\n",
      "episode_reward_mean = -9.492384910583496:  23%|██▎       | 14/60 [00:31<01:44,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = -7.866189956665039:  25%|██▌       | 15/60 [00:33<01:41,  2.25s/it]\u001B[A\n",
      "episode_reward_mean = -4.457878589630127:  27%|██▋       | 16/60 [00:35<01:38,  2.25s/it]\u001B[A\n",
      "episode_reward_mean = -2.518430233001709:  28%|██▊       | 17/60 [00:38<01:35,  2.23s/it]\u001B[A\n",
      "episode_reward_mean = 2.6283552646636963:  30%|███       | 18/60 [00:40<01:33,  2.22s/it]\u001B[A\n",
      "episode_reward_mean = 3.0694587230682373:  32%|███▏      | 19/60 [00:42<01:31,  2.23s/it]\u001B[A\n",
      "episode_reward_mean = 17.17801284790039:  33%|███▎      | 20/60 [00:44<01:28,  2.22s/it] \u001B[A\n",
      "episode_reward_mean = 12.226974487304688:  35%|███▌      | 21/60 [00:46<01:26,  2.22s/it]\u001B[A\n",
      "episode_reward_mean = 15.381265640258789:  37%|███▋      | 22/60 [00:49<01:25,  2.24s/it]\u001B[A\n",
      "episode_reward_mean = 15.68936824798584:  38%|███▊      | 23/60 [00:51<01:23,  2.25s/it] \u001B[A\n",
      "episode_reward_mean = 18.848167419433594:  40%|████      | 24/60 [00:53<01:21,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 26.399173736572266:  42%|████▏     | 25/60 [00:56<01:18,  2.24s/it]\u001B[A\n",
      "episode_reward_mean = 26.834606170654297:  43%|████▎     | 26/60 [00:58<01:15,  2.23s/it]\u001B[A\n",
      "episode_reward_mean = 34.9976692199707:  45%|████▌     | 27/60 [01:00<01:13,  2.23s/it]  \u001B[A\n",
      "episode_reward_mean = 37.57107925415039:  47%|████▋     | 28/60 [01:02<01:11,  2.23s/it]\u001B[A\n",
      "episode_reward_mean = 38.8658561706543:  48%|████▊     | 29/60 [01:05<01:10,  2.26s/it] \u001B[A\n",
      "episode_reward_mean = 33.997032165527344:  50%|█████     | 30/60 [01:07<01:07,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = 47.73280715942383:  52%|█████▏    | 31/60 [01:09<01:04,  2.24s/it] \u001B[A\n",
      "episode_reward_mean = 53.4904670715332:  53%|█████▎    | 32/60 [01:11<01:02,  2.25s/it] \u001B[A\n",
      "episode_reward_mean = 46.24333572387695:  55%|█████▌    | 33/60 [01:13<01:00,  2.25s/it]\u001B[A\n",
      "episode_reward_mean = 55.55078887939453:  57%|█████▋    | 34/60 [01:16<00:59,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 64.23861694335938:  58%|█████▊    | 35/60 [01:18<00:56,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 57.811954498291016:  60%|██████    | 36/60 [01:20<00:54,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 62.7625846862793:  62%|██████▏   | 37/60 [01:23<00:51,  2.26s/it]  \u001B[A\n",
      "episode_reward_mean = 77.10963439941406:  63%|██████▎   | 38/60 [01:25<00:49,  2.25s/it]\u001B[A\n",
      "episode_reward_mean = 74.09542083740234:  65%|██████▌   | 39/60 [01:27<00:46,  2.24s/it]\u001B[A\n",
      "episode_reward_mean = 71.06756591796875:  67%|██████▋   | 40/60 [01:29<00:44,  2.22s/it]\u001B[A\n",
      "episode_reward_mean = 75.9327621459961:  68%|██████▊   | 41/60 [01:31<00:42,  2.22s/it] \u001B[A\n",
      "episode_reward_mean = 76.63459777832031:  70%|███████   | 42/60 [01:34<00:40,  2.23s/it]\u001B[A\n",
      "episode_reward_mean = 77.4609603881836:  72%|███████▏  | 43/60 [01:36<00:38,  2.25s/it] \u001B[A\n",
      "episode_reward_mean = 86.43638610839844:  73%|███████▎  | 44/60 [01:38<00:36,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 79.48039245605469:  75%|███████▌  | 45/60 [01:41<00:34,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 89.96234893798828:  77%|███████▋  | 46/60 [01:43<00:31,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 87.97626495361328:  78%|███████▊  | 47/60 [01:45<00:29,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 97.46344757080078:  80%|████████  | 48/60 [01:47<00:27,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 92.03990936279297:  82%|████████▏ | 49/60 [01:50<00:25,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 85.656494140625:  83%|████████▎ | 50/60 [01:52<00:22,  2.27s/it]  \u001B[A\n",
      "episode_reward_mean = 86.30146789550781:  85%|████████▌ | 51/60 [01:54<00:20,  2.25s/it]\u001B[A\n",
      "episode_reward_mean = 90.34548950195312:  87%|████████▋ | 52/60 [01:56<00:17,  2.25s/it]\u001B[A\n",
      "episode_reward_mean = 92.01864624023438:  88%|████████▊ | 53/60 [01:59<00:15,  2.24s/it]\u001B[A\n",
      "episode_reward_mean = 91.85853576660156:  90%|█████████ | 54/60 [02:01<00:13,  2.27s/it]\u001B[A\n",
      "episode_reward_mean = 91.39476776123047:  92%|█████████▏| 55/60 [02:03<00:11,  2.25s/it]\u001B[A\n",
      "episode_reward_mean = 92.3901138305664:  93%|█████████▎| 56/60 [02:05<00:08,  2.25s/it] \u001B[A\n",
      "episode_reward_mean = 94.66513061523438:  95%|█████████▌| 57/60 [02:08<00:06,  2.25s/it]\u001B[A\n",
      "episode_reward_mean = 89.42359924316406:  97%|█████████▋| 58/60 [02:10<00:04,  2.25s/it]\u001B[A\n",
      "episode_reward_mean = 97.2377700805664:  98%|█████████▊| 59/60 [02:12<00:02,  2.24s/it] \u001B[A\n",
      "episode_reward_mean = 95.79644012451172: 100%|██████████| 60/60 [02:14<00:00,  2.25s/it]\u001B[A\n",
      "2025-08-20 23:06:53,401 [torchrl][INFO] Training time: 76.36 seconds\n",
      "2025-08-20 23:06:53,405 [torchrl][INFO] macs: 62.9 MMac  Params: 57.54 k\n",
      "2025-08-20 23:06:54,454 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 23:06:54,498 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -9.743342399597168:   2%|▏         | 1/60 [00:02<02:19,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = -3.363083839416504:   3%|▎         | 2/60 [00:04<02:15,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = -5.631872177124023:   5%|▌         | 3/60 [00:06<02:10,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = -4.387429714202881:   7%|▋         | 4/60 [00:09<02:07,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = -4.157115459442139:   8%|▊         | 5/60 [00:11<02:05,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = -3.098238229751587:  10%|█         | 6/60 [00:13<02:03,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = -2.6454575061798096:  12%|█▏        | 7/60 [00:16<02:01,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = -2.457761526107788:  13%|█▎        | 8/60 [00:18<01:59,  2.30s/it] \u001B[A\n",
      "episode_reward_mean = -3.605595111846924:  15%|█▌        | 9/60 [00:20<01:56,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = -2.5916881561279297:  17%|█▋        | 10/60 [00:22<01:54,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = -0.5465140342712402:  18%|█▊        | 11/60 [00:25<01:55,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 5.555840492248535:  20%|██        | 12/60 [00:27<01:52,  2.34s/it]  \u001B[A\n",
      "episode_reward_mean = 5.059090614318848:  22%|██▏       | 13/60 [00:30<01:49,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 7.989468097686768:  23%|██▎       | 14/60 [00:32<01:46,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 22.30133056640625:  25%|██▌       | 15/60 [00:34<01:43,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 32.83729553222656:  27%|██▋       | 16/60 [00:36<01:41,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 36.69534683227539:  28%|██▊       | 17/60 [00:39<01:38,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 40.733829498291016:  30%|███       | 18/60 [00:41<01:36,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 45.41045379638672:  32%|███▏      | 19/60 [00:43<01:33,  2.28s/it] \u001B[A\n",
      "episode_reward_mean = 48.86655044555664:  33%|███▎      | 20/60 [00:45<01:30,  2.25s/it]\u001B[A\n",
      "episode_reward_mean = 43.1409912109375:  35%|███▌      | 21/60 [00:48<01:28,  2.27s/it] \u001B[A\n",
      "episode_reward_mean = 52.96198272705078:  37%|███▋      | 22/60 [00:50<01:25,  2.25s/it]\u001B[A\n",
      "episode_reward_mean = 57.21792984008789:  38%|███▊      | 23/60 [00:52<01:23,  2.24s/it]\u001B[A\n",
      "episode_reward_mean = 63.27853012084961:  40%|████      | 24/60 [00:54<01:21,  2.26s/it]\u001B[A\n",
      "episode_reward_mean = 64.53338623046875:  42%|████▏     | 25/60 [00:57<01:19,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 68.1145248413086:  43%|████▎     | 26/60 [00:59<01:17,  2.28s/it] \u001B[A\n",
      "episode_reward_mean = 70.24549865722656:  45%|████▌     | 27/60 [01:01<01:15,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 70.39635467529297:  47%|████▋     | 28/60 [01:04<01:13,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 67.91401672363281:  48%|████▊     | 29/60 [01:06<01:11,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 85.35942077636719:  50%|█████     | 30/60 [01:08<01:08,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 82.41903686523438:  52%|█████▏    | 31/60 [01:11<01:07,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 84.49246978759766:  53%|█████▎    | 32/60 [01:13<01:04,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 89.47789764404297:  55%|█████▌    | 33/60 [01:15<01:02,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 85.03114318847656:  57%|█████▋    | 34/60 [01:18<00:59,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 93.80915069580078:  58%|█████▊    | 35/60 [01:20<00:57,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 94.69153594970703:  60%|██████    | 36/60 [01:22<00:55,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 89.96463775634766:  62%|██████▏   | 37/60 [01:24<00:53,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 96.35633087158203:  63%|██████▎   | 38/60 [01:27<00:50,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 88.87095642089844:  65%|██████▌   | 39/60 [01:29<00:48,  2.30s/it]\u001B[A\n",
      "episode_reward_mean = 94.27252197265625:  67%|██████▋   | 40/60 [01:31<00:45,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 90.62016296386719:  68%|██████▊   | 41/60 [01:34<00:44,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 92.09517669677734:  70%|███████   | 42/60 [01:36<00:41,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 88.23729705810547:  72%|███████▏  | 43/60 [01:38<00:39,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 96.10145568847656:  73%|███████▎  | 44/60 [01:41<00:37,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 92.47322082519531:  75%|███████▌  | 45/60 [01:43<00:34,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 100.1939926147461:  77%|███████▋  | 46/60 [01:45<00:32,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 100.05611419677734:  78%|███████▊  | 47/60 [01:48<00:30,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 102.11927795410156:  80%|████████  | 48/60 [01:50<00:27,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 103.43881225585938:  82%|████████▏ | 49/60 [01:52<00:25,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 97.98162841796875:  83%|████████▎ | 50/60 [01:55<00:23,  2.32s/it] \u001B[A\n",
      "episode_reward_mean = 102.10277557373047:  85%|████████▌ | 51/60 [01:57<00:21,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 104.05731964111328:  87%|████████▋ | 52/60 [01:59<00:18,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 89.71724700927734:  88%|████████▊ | 53/60 [02:02<00:16,  2.34s/it] \u001B[A\n",
      "episode_reward_mean = 100.35832214355469:  90%|█████████ | 54/60 [02:04<00:14,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 96.47643280029297:  92%|█████████▏| 55/60 [02:06<00:11,  2.34s/it] \u001B[A\n",
      "episode_reward_mean = 91.17692565917969:  93%|█████████▎| 56/60 [02:09<00:09,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 98.63064575195312:  95%|█████████▌| 57/60 [02:11<00:06,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 92.00897216796875:  97%|█████████▋| 58/60 [02:13<00:04,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 94.54663848876953:  98%|█████████▊| 59/60 [02:16<00:02,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 83.77485656738281: 100%|██████████| 60/60 [02:18<00:00,  2.31s/it]\u001B[A\n",
      "2025-08-20 23:09:12,924 [torchrl][INFO] Training time: 79.26 seconds\n",
      "2025-08-20 23:09:12,930 [torchrl][INFO] macs: 62.9 MMac  Params: 57.54 k\n",
      "2025-08-20 23:09:14,145 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 23:09:14,190 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/60 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = -7.952164173126221:   2%|▏         | 1/60 [00:02<02:21,  2.40s/it]\u001B[A\n",
      "episode_reward_mean = -6.528202056884766:   3%|▎         | 2/60 [00:04<02:15,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = -1.5454550981521606:   5%|▌         | 3/60 [00:06<02:12,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = -5.946118354797363:   7%|▋         | 4/60 [00:09<02:10,  2.33s/it] \u001B[A\n",
      "episode_reward_mean = -4.450091361999512:   8%|▊         | 5/60 [00:11<02:08,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = -1.8228733539581299:  10%|█         | 6/60 [00:14<02:06,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 2.5525684356689453:  12%|█▏        | 7/60 [00:16<02:03,  2.32s/it] \u001B[A\n",
      "episode_reward_mean = 1.9268980026245117:  13%|█▎        | 8/60 [00:18<02:00,  2.31s/it]\u001B[A\n",
      "episode_reward_mean = 9.625076293945312:  15%|█▌        | 9/60 [00:21<01:59,  2.35s/it] \u001B[A\n",
      "episode_reward_mean = 13.502067565917969:  17%|█▋        | 10/60 [00:23<01:56,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 20.329973220825195:  18%|█▊        | 11/60 [00:25<01:53,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 26.102201461791992:  20%|██        | 12/60 [00:27<01:52,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 29.154584884643555:  22%|██▏       | 13/60 [00:30<01:49,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 25.783557891845703:  23%|██▎       | 14/60 [00:32<01:47,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 33.00423049926758:  25%|██▌       | 15/60 [00:35<01:45,  2.34s/it] \u001B[A\n",
      "episode_reward_mean = 34.135929107666016:  27%|██▋       | 16/60 [00:37<01:42,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 42.06363296508789:  28%|██▊       | 17/60 [00:39<01:39,  2.32s/it] \u001B[A\n",
      "episode_reward_mean = 42.32135009765625:  30%|███       | 18/60 [00:41<01:37,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 47.474998474121094:  32%|███▏      | 19/60 [00:44<01:36,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 49.22114181518555:  33%|███▎      | 20/60 [00:46<01:34,  2.36s/it] \u001B[A\n",
      "episode_reward_mean = 60.53727340698242:  35%|███▌      | 21/60 [00:49<01:31,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 67.11138916015625:  37%|███▋      | 22/60 [00:51<01:28,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 61.65471267700195:  38%|███▊      | 23/60 [00:53<01:26,  2.34s/it]\u001B[A\n",
      "episode_reward_mean = 78.94017791748047:  40%|████      | 24/60 [00:56<01:23,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 68.84394073486328:  42%|████▏     | 25/60 [00:58<01:21,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 78.42227935791016:  43%|████▎     | 26/60 [01:00<01:19,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 79.21465301513672:  45%|████▌     | 27/60 [01:03<01:17,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 82.15936279296875:  47%|████▋     | 28/60 [01:05<01:14,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 86.79649353027344:  48%|████▊     | 29/60 [01:07<01:12,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 84.28498840332031:  50%|█████     | 30/60 [01:10<01:10,  2.35s/it]\u001B[A\n",
      "episode_reward_mean = 82.21087646484375:  52%|█████▏    | 31/60 [01:12<01:07,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 78.5268783569336:  53%|█████▎    | 32/60 [01:14<01:04,  2.31s/it] \u001B[A\n",
      "episode_reward_mean = 86.57809448242188:  55%|█████▌    | 33/60 [01:16<01:01,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 90.69479370117188:  57%|█████▋    | 34/60 [01:19<00:59,  2.28s/it]\u001B[A\n",
      "episode_reward_mean = 95.78905487060547:  58%|█████▊    | 35/60 [01:21<00:57,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 101.34132385253906:  60%|██████    | 36/60 [01:23<00:54,  2.29s/it]\u001B[A\n",
      "episode_reward_mean = 95.24775695800781:  62%|██████▏   | 37/60 [01:26<00:52,  2.30s/it] \u001B[A\n",
      "episode_reward_mean = 88.46377563476562:  63%|██████▎   | 38/60 [01:28<00:51,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 92.06065368652344:  65%|██████▌   | 39/60 [01:31<00:50,  2.39s/it]\u001B[A\n",
      "episode_reward_mean = 93.95767211914062:  67%|██████▋   | 40/60 [01:33<00:47,  2.38s/it]\u001B[A\n",
      "episode_reward_mean = 89.600830078125:  68%|██████▊   | 41/60 [01:35<00:44,  2.37s/it]  \u001B[A\n",
      "episode_reward_mean = 94.64682006835938:  70%|███████   | 42/60 [01:38<00:42,  2.37s/it]\u001B[A\n",
      "episode_reward_mean = 98.90541076660156:  72%|███████▏  | 43/60 [01:40<00:40,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 95.3935546875:  73%|███████▎  | 44/60 [01:42<00:38,  2.38s/it]    \u001B[A\n",
      "episode_reward_mean = 95.41667175292969:  75%|███████▌  | 45/60 [01:45<00:35,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 96.7746810913086:  77%|███████▋  | 46/60 [01:47<00:32,  2.35s/it] \u001B[A\n",
      "episode_reward_mean = 94.48391723632812:  78%|███████▊  | 47/60 [01:49<00:30,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 89.7739486694336:  80%|████████  | 48/60 [01:52<00:27,  2.33s/it] \u001B[A\n",
      "episode_reward_mean = 101.8791275024414:  82%|████████▏ | 49/60 [01:54<00:25,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 95.23805236816406:  83%|████████▎ | 50/60 [01:56<00:23,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 94.7953872680664:  85%|████████▌ | 51/60 [01:59<00:20,  2.31s/it] \u001B[A\n",
      "episode_reward_mean = 97.07051849365234:  87%|████████▋ | 52/60 [02:01<00:18,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 94.46050262451172:  88%|████████▊ | 53/60 [02:03<00:16,  2.33s/it]\u001B[A\n",
      "episode_reward_mean = 95.17091369628906:  90%|█████████ | 54/60 [02:06<00:13,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 101.71434020996094:  92%|█████████▏| 55/60 [02:08<00:11,  2.32s/it]\u001B[A\n",
      "episode_reward_mean = 99.37049865722656:  93%|█████████▎| 56/60 [02:10<00:09,  2.30s/it] \u001B[A\n",
      "episode_reward_mean = 93.9029769897461:  95%|█████████▌| 57/60 [02:13<00:07,  2.36s/it] \u001B[A\n",
      "episode_reward_mean = 106.53993225097656:  97%|█████████▋| 58/60 [02:15<00:04,  2.36s/it]\u001B[A\n",
      "episode_reward_mean = 93.53494262695312:  98%|█████████▊| 59/60 [02:17<00:02,  2.38s/it] \u001B[A\n",
      "episode_reward_mean = 103.12834930419922: 100%|██████████| 60/60 [02:20<00:00,  2.34s/it]\u001B[A\n",
      "2025-08-20 23:11:34,472 [torchrl][INFO] Training time: 80.62 seconds\n",
      "2025-08-20 23:11:34,478 [torchrl][INFO] macs: 62.9 MMac  Params: 57.54 k\n",
      "2025-08-20 23:11:35,613 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 23:11:35,718 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 88.60472869873047:   5%|▌         | 1/20 [00:06<02:12,  6.99s/it]\u001B[A\n",
      "episode_reward_mean = 70.27884674072266:  10%|█         | 2/20 [00:13<02:05,  6.98s/it]\u001B[A\n",
      "episode_reward_mean = 102.14085388183594:  15%|█▌        | 3/20 [00:20<01:58,  6.94s/it]\u001B[A\n",
      "episode_reward_mean = 70.8308334350586:  20%|██        | 4/20 [00:27<01:50,  6.88s/it]  \u001B[A\n",
      "episode_reward_mean = 94.9475326538086:  25%|██▌       | 5/20 [00:34<01:42,  6.84s/it]\u001B[A\n",
      "episode_reward_mean = 89.53219604492188:  30%|███       | 6/20 [00:41<01:36,  6.90s/it]\u001B[A\n",
      "episode_reward_mean = 87.59556579589844:  35%|███▌      | 7/20 [00:48<01:30,  6.94s/it]\u001B[A\n",
      "episode_reward_mean = 79.37659454345703:  40%|████      | 8/20 [00:55<01:23,  6.92s/it]\u001B[A\n",
      "episode_reward_mean = 75.64920806884766:  45%|████▌     | 9/20 [01:02<01:16,  6.94s/it]\u001B[A\n",
      "episode_reward_mean = 84.23494720458984:  50%|█████     | 10/20 [01:09<01:10,  7.02s/it]\u001B[A\n",
      "episode_reward_mean = 94.99235534667969:  55%|█████▌    | 11/20 [01:16<01:02,  6.99s/it]\u001B[A\n",
      "episode_reward_mean = 76.37953186035156:  60%|██████    | 12/20 [01:23<00:55,  6.93s/it]\u001B[A\n",
      "episode_reward_mean = 73.02751159667969:  65%|██████▌   | 13/20 [01:30<00:48,  6.95s/it]\u001B[A\n",
      "episode_reward_mean = 91.24141693115234:  70%|███████   | 14/20 [01:37<00:42,  7.03s/it]\u001B[A\n",
      "episode_reward_mean = 101.87706756591797:  75%|███████▌  | 15/20 [01:44<00:35,  7.00s/it]\u001B[A\n",
      "episode_reward_mean = 91.16077423095703:  80%|████████  | 16/20 [01:51<00:27,  6.99s/it] \u001B[A\n",
      "episode_reward_mean = 92.7392807006836:  85%|████████▌ | 17/20 [01:58<00:21,  7.02s/it] \u001B[A\n",
      "episode_reward_mean = 99.31009674072266:  90%|█████████ | 18/20 [02:05<00:13,  6.98s/it]\u001B[A\n",
      "episode_reward_mean = 92.44281768798828:  95%|█████████▌| 19/20 [02:12<00:06,  6.97s/it]\u001B[A\n",
      "episode_reward_mean = 74.73731994628906: 100%|██████████| 20/20 [02:19<00:00,  6.97s/it]\u001B[A\n",
      "2025-08-20 23:13:55,217 [torchrl][INFO] Training time: 86.89 seconds\n",
      "2025-08-20 23:13:55,231 [torchrl][INFO] macs: 211.52 MMac  Params: 57.54 k\n",
      "2025-08-20 23:13:58,030 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 23:13:58,134 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 87.28936767578125:   5%|▌         | 1/20 [00:06<02:12,  6.97s/it]\u001B[A\n",
      "episode_reward_mean = 72.12432861328125:  10%|█         | 2/20 [00:13<02:05,  6.98s/it]\u001B[A\n",
      "episode_reward_mean = 86.46228790283203:  15%|█▌        | 3/20 [00:20<01:58,  6.98s/it]\u001B[A\n",
      "episode_reward_mean = 92.10322570800781:  20%|██        | 4/20 [00:27<01:51,  6.96s/it]\u001B[A\n",
      "episode_reward_mean = 83.86959075927734:  25%|██▌       | 5/20 [00:34<01:44,  6.96s/it]\u001B[A\n",
      "episode_reward_mean = 77.2712631225586:  30%|███       | 6/20 [00:41<01:37,  7.00s/it] \u001B[A\n",
      "episode_reward_mean = 86.51017761230469:  35%|███▌      | 7/20 [00:48<01:31,  7.01s/it]\u001B[A\n",
      "episode_reward_mean = 85.18409729003906:  40%|████      | 8/20 [00:55<01:24,  7.02s/it]\u001B[A\n",
      "episode_reward_mean = 100.08373260498047:  45%|████▌     | 9/20 [01:02<01:17,  7.02s/it]\u001B[A\n",
      "episode_reward_mean = 86.63801574707031:  50%|█████     | 10/20 [01:10<01:10,  7.04s/it]\u001B[A\n",
      "episode_reward_mean = 88.28864288330078:  55%|█████▌    | 11/20 [01:16<01:03,  7.00s/it]\u001B[A\n",
      "episode_reward_mean = 93.64591979980469:  60%|██████    | 12/20 [01:23<00:55,  6.99s/it]\u001B[A\n",
      "episode_reward_mean = 103.81455993652344:  65%|██████▌   | 13/20 [01:30<00:48,  6.95s/it]\u001B[A\n",
      "episode_reward_mean = 89.12213134765625:  70%|███████   | 14/20 [01:37<00:41,  6.91s/it] \u001B[A\n",
      "episode_reward_mean = 74.9343490600586:  75%|███████▌  | 15/20 [01:44<00:34,  6.91s/it] \u001B[A\n",
      "episode_reward_mean = 68.01628112792969:  80%|████████  | 16/20 [01:51<00:27,  6.89s/it]\u001B[A\n",
      "episode_reward_mean = 82.70353698730469:  85%|████████▌ | 17/20 [01:58<00:20,  6.91s/it]\u001B[A\n",
      "episode_reward_mean = 69.57051086425781:  90%|█████████ | 18/20 [02:05<00:13,  6.89s/it]\u001B[A\n",
      "episode_reward_mean = 102.9847183227539:  95%|█████████▌| 19/20 [02:12<00:06,  6.95s/it]\u001B[A\n",
      "episode_reward_mean = 88.22689056396484: 100%|██████████| 20/20 [02:19<00:00,  6.96s/it]\u001B[A\n",
      "2025-08-20 23:16:17,392 [torchrl][INFO] Training time: 85.45 seconds\n",
      "2025-08-20 23:16:17,405 [torchrl][INFO] macs: 211.52 MMac  Params: 57.54 k\n",
      "2025-08-20 23:16:20,204 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 23:16:20,307 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 71.24449920654297:   5%|▌         | 1/20 [00:06<02:06,  6.66s/it]\u001B[A\n",
      "episode_reward_mean = 81.93744659423828:  10%|█         | 2/20 [00:13<02:01,  6.75s/it]\u001B[A\n",
      "episode_reward_mean = 78.41595458984375:  15%|█▌        | 3/20 [00:20<01:54,  6.73s/it]\u001B[A\n",
      "episode_reward_mean = 82.2433853149414:  20%|██        | 4/20 [00:26<01:47,  6.72s/it] \u001B[A\n",
      "episode_reward_mean = 88.146728515625:  25%|██▌       | 5/20 [00:33<01:40,  6.73s/it] \u001B[A\n",
      "episode_reward_mean = 106.81050109863281:  30%|███       | 6/20 [00:40<01:35,  6.81s/it]\u001B[A\n",
      "episode_reward_mean = 87.46317291259766:  35%|███▌      | 7/20 [00:47<01:28,  6.82s/it] \u001B[A\n",
      "episode_reward_mean = 98.62788391113281:  40%|████      | 8/20 [00:54<01:21,  6.82s/it]\u001B[A\n",
      "episode_reward_mean = 98.12320709228516:  45%|████▌     | 9/20 [01:01<01:14,  6.82s/it]\u001B[A\n",
      "episode_reward_mean = 93.642822265625:  50%|█████     | 10/20 [01:07<01:08,  6.84s/it] \u001B[A\n",
      "episode_reward_mean = 79.36843872070312:  55%|█████▌    | 11/20 [01:14<01:01,  6.84s/it]\u001B[A\n",
      "episode_reward_mean = 97.50151824951172:  60%|██████    | 12/20 [01:21<00:54,  6.87s/it]\u001B[A\n",
      "episode_reward_mean = 72.84519958496094:  65%|██████▌   | 13/20 [01:28<00:48,  6.87s/it]\u001B[A\n",
      "episode_reward_mean = 86.39258575439453:  70%|███████   | 14/20 [01:35<00:41,  6.89s/it]\u001B[A\n",
      "episode_reward_mean = 79.9781265258789:  75%|███████▌  | 15/20 [01:42<00:34,  6.89s/it] \u001B[A\n",
      "episode_reward_mean = 95.30980682373047:  80%|████████  | 16/20 [01:49<00:27,  6.91s/it]\u001B[A\n",
      "episode_reward_mean = 81.14962768554688:  85%|████████▌ | 17/20 [01:56<00:20,  6.90s/it]\u001B[A\n",
      "episode_reward_mean = 91.57814025878906:  90%|█████████ | 18/20 [02:03<00:13,  6.88s/it]\u001B[A\n",
      "episode_reward_mean = 90.9166488647461:  95%|█████████▌| 19/20 [02:09<00:06,  6.87s/it] \u001B[A\n",
      "episode_reward_mean = 86.6742172241211: 100%|██████████| 20/20 [02:16<00:00,  6.84s/it]\u001B[A\n",
      "2025-08-20 23:18:37,203 [torchrl][INFO] Training time: 84.94 seconds\n",
      "2025-08-20 23:18:37,216 [torchrl][INFO] macs: 211.52 MMac  Params: 57.54 k\n",
      "2025-08-20 23:18:39,967 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 23:18:40,071 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 83.43096923828125:   5%|▌         | 1/20 [00:06<02:09,  6.83s/it]\u001B[A\n",
      "episode_reward_mean = 70.85588073730469:  10%|█         | 2/20 [00:13<02:04,  6.90s/it]\u001B[A\n",
      "episode_reward_mean = 99.13072204589844:  15%|█▌        | 3/20 [00:20<01:56,  6.87s/it]\u001B[A\n",
      "episode_reward_mean = 82.35685729980469:  20%|██        | 4/20 [00:27<01:49,  6.86s/it]\u001B[A\n",
      "episode_reward_mean = 88.50203704833984:  25%|██▌       | 5/20 [00:34<01:42,  6.82s/it]\u001B[A\n",
      "episode_reward_mean = 108.72209167480469:  30%|███       | 6/20 [00:41<01:35,  6.82s/it]\u001B[A\n",
      "episode_reward_mean = 100.11534118652344:  35%|███▌      | 7/20 [00:47<01:28,  6.84s/it]\u001B[A\n",
      "episode_reward_mean = 99.82737731933594:  40%|████      | 8/20 [00:54<01:22,  6.84s/it] \u001B[A\n",
      "episode_reward_mean = 91.82292175292969:  45%|████▌     | 9/20 [01:01<01:15,  6.84s/it]\u001B[A\n",
      "episode_reward_mean = 98.2542953491211:  50%|█████     | 10/20 [01:08<01:08,  6.87s/it]\u001B[A\n",
      "episode_reward_mean = 102.90886688232422:  55%|█████▌    | 11/20 [01:15<01:02,  6.92s/it]\u001B[A\n",
      "episode_reward_mean = 93.69361114501953:  60%|██████    | 12/20 [01:22<00:54,  6.86s/it] \u001B[A\n",
      "episode_reward_mean = 76.16165161132812:  65%|██████▌   | 13/20 [01:29<00:47,  6.85s/it]\u001B[A\n",
      "episode_reward_mean = 89.33138275146484:  70%|███████   | 14/20 [01:35<00:40,  6.82s/it]\u001B[A\n",
      "episode_reward_mean = 92.23385620117188:  75%|███████▌  | 15/20 [01:42<00:34,  6.83s/it]\u001B[A\n",
      "episode_reward_mean = 96.2079086303711:  80%|████████  | 16/20 [01:49<00:27,  6.84s/it] \u001B[A\n",
      "episode_reward_mean = 100.31378936767578:  85%|████████▌ | 17/20 [01:56<00:20,  6.84s/it]\u001B[A\n",
      "episode_reward_mean = 80.68927764892578:  90%|█████████ | 18/20 [02:03<00:13,  6.84s/it] \u001B[A\n",
      "episode_reward_mean = 100.54112243652344:  95%|█████████▌| 19/20 [02:10<00:06,  6.84s/it]\u001B[A\n",
      "episode_reward_mean = 88.77342224121094: 100%|██████████| 20/20 [02:16<00:00,  6.85s/it] \u001B[A\n",
      "2025-08-20 23:20:57,038 [torchrl][INFO] Training time: 84.17 seconds\n",
      "2025-08-20 23:20:57,052 [torchrl][INFO] macs: 211.52 MMac  Params: 57.54 k\n",
      "2025-08-20 23:20:59,792 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 23:20:59,897 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 65.3212661743164:   5%|▌         | 1/20 [00:06<02:06,  6.65s/it]\u001B[A\n",
      "episode_reward_mean = 68.45563507080078:  10%|█         | 2/20 [00:13<01:59,  6.66s/it]\u001B[A\n",
      "episode_reward_mean = 62.09918975830078:  15%|█▌        | 3/20 [00:20<01:54,  6.71s/it]\u001B[A\n",
      "episode_reward_mean = 82.60728454589844:  20%|██        | 4/20 [00:26<01:47,  6.74s/it]\u001B[A\n",
      "episode_reward_mean = 66.20931243896484:  25%|██▌       | 5/20 [00:33<01:40,  6.73s/it]\u001B[A\n",
      "episode_reward_mean = 80.54353332519531:  30%|███       | 6/20 [00:40<01:35,  6.80s/it]\u001B[A\n",
      "episode_reward_mean = 65.77495574951172:  35%|███▌      | 7/20 [00:47<01:28,  6.79s/it]\u001B[A\n",
      "episode_reward_mean = 80.33908081054688:  40%|████      | 8/20 [00:54<01:21,  6.81s/it]\u001B[A\n",
      "episode_reward_mean = 76.39572143554688:  45%|████▌     | 9/20 [01:00<01:14,  6.81s/it]\u001B[A\n",
      "episode_reward_mean = 83.45051574707031:  50%|█████     | 10/20 [01:07<01:08,  6.80s/it]\u001B[A\n",
      "episode_reward_mean = 67.95709991455078:  55%|█████▌    | 11/20 [01:14<01:00,  6.77s/it]\u001B[A\n",
      "episode_reward_mean = 65.20167541503906:  60%|██████    | 12/20 [01:21<00:53,  6.74s/it]\u001B[A\n",
      "episode_reward_mean = 78.05233764648438:  65%|██████▌   | 13/20 [01:27<00:47,  6.75s/it]\u001B[A\n",
      "episode_reward_mean = 71.74774169921875:  70%|███████   | 14/20 [01:34<00:40,  6.77s/it]\u001B[A\n",
      "episode_reward_mean = 61.39485168457031:  75%|███████▌  | 15/20 [01:41<00:33,  6.76s/it]\u001B[A\n",
      "episode_reward_mean = 81.22686767578125:  80%|████████  | 16/20 [01:48<00:26,  6.75s/it]\u001B[A\n",
      "episode_reward_mean = 78.1422119140625:  85%|████████▌ | 17/20 [01:55<00:20,  6.80s/it] \u001B[A\n",
      "episode_reward_mean = 66.8049087524414:  90%|█████████ | 18/20 [02:01<00:13,  6.78s/it]\u001B[A\n",
      "episode_reward_mean = 66.90565490722656:  95%|█████████▌| 19/20 [02:08<00:06,  6.77s/it]\u001B[A\n",
      "episode_reward_mean = 62.14441680908203: 100%|██████████| 20/20 [02:15<00:00,  6.76s/it]\u001B[A\n",
      "2025-08-20 23:23:15,198 [torchrl][INFO] Training time: 83.00 seconds\n",
      "2025-08-20 23:23:15,211 [torchrl][INFO] macs: 211.52 MMac  Params: 57.54 k\n",
      "2025-08-20 23:23:17,928 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 23:23:18,032 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 63.61689376831055:   5%|▌         | 1/20 [00:06<02:04,  6.57s/it]\u001B[A\n",
      "episode_reward_mean = 83.77278137207031:  10%|█         | 2/20 [00:13<01:57,  6.55s/it]\u001B[A\n",
      "episode_reward_mean = 59.27825927734375:  15%|█▌        | 3/20 [00:19<01:52,  6.60s/it]\u001B[A\n",
      "episode_reward_mean = 64.16564178466797:  20%|██        | 4/20 [00:26<01:46,  6.65s/it]\u001B[A\n",
      "episode_reward_mean = 64.36164093017578:  25%|██▌       | 5/20 [00:33<01:40,  6.69s/it]\u001B[A\n",
      "episode_reward_mean = 74.00660705566406:  30%|███       | 6/20 [00:39<01:33,  6.69s/it]\u001B[A\n",
      "episode_reward_mean = 80.96678161621094:  35%|███▌      | 7/20 [00:46<01:27,  6.71s/it]\u001B[A\n",
      "episode_reward_mean = 66.35786437988281:  40%|████      | 8/20 [00:53<01:20,  6.72s/it]\u001B[A\n",
      "episode_reward_mean = 64.52789306640625:  45%|████▌     | 9/20 [01:00<01:13,  6.69s/it]\u001B[A\n",
      "episode_reward_mean = 54.46982192993164:  50%|█████     | 10/20 [01:06<01:06,  6.68s/it]\u001B[A\n",
      "episode_reward_mean = 76.62599182128906:  55%|█████▌    | 11/20 [01:13<01:00,  6.67s/it]\u001B[A\n",
      "episode_reward_mean = 64.94522857666016:  60%|██████    | 12/20 [01:19<00:53,  6.66s/it]\u001B[A\n",
      "episode_reward_mean = 83.00545501708984:  65%|██████▌   | 13/20 [01:26<00:46,  6.65s/it]\u001B[A\n",
      "episode_reward_mean = 83.68408203125:  70%|███████   | 14/20 [01:33<00:39,  6.66s/it]   \u001B[A\n",
      "episode_reward_mean = 62.157989501953125:  75%|███████▌  | 15/20 [01:39<00:33,  6.67s/it]\u001B[A\n",
      "episode_reward_mean = 75.9469223022461:  80%|████████  | 16/20 [01:46<00:26,  6.64s/it]  \u001B[A\n",
      "episode_reward_mean = 56.96189498901367:  85%|████████▌ | 17/20 [01:53<00:19,  6.64s/it]\u001B[A\n",
      "episode_reward_mean = 74.79861450195312:  90%|█████████ | 18/20 [02:00<00:13,  6.70s/it]\u001B[A\n",
      "episode_reward_mean = 75.68168640136719:  95%|█████████▌| 19/20 [02:06<00:06,  6.72s/it]\u001B[A\n",
      "episode_reward_mean = 78.461181640625: 100%|██████████| 20/20 [02:13<00:00,  6.67s/it]  \u001B[A\n",
      "2025-08-20 23:25:31,472 [torchrl][INFO] Training time: 81.83 seconds\n",
      "2025-08-20 23:25:31,485 [torchrl][INFO] macs: 211.52 MMac  Params: 57.54 k\n",
      "2025-08-20 23:25:34,294 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 23:25:34,397 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 66.39518737792969:   5%|▌         | 1/20 [00:06<02:07,  6.70s/it]\u001B[A\n",
      "episode_reward_mean = 67.72705078125:  10%|█         | 2/20 [00:13<01:59,  6.65s/it]   \u001B[A\n",
      "episode_reward_mean = 82.81837463378906:  15%|█▌        | 3/20 [00:19<01:52,  6.63s/it]\u001B[A\n",
      "episode_reward_mean = 76.38636779785156:  20%|██        | 4/20 [00:26<01:47,  6.69s/it]\u001B[A\n",
      "episode_reward_mean = 81.6419906616211:  25%|██▌       | 5/20 [00:33<01:41,  6.75s/it] \u001B[A\n",
      "episode_reward_mean = 75.50431060791016:  30%|███       | 6/20 [00:40<01:34,  6.72s/it]\u001B[A\n",
      "episode_reward_mean = 71.8484878540039:  35%|███▌      | 7/20 [00:46<01:27,  6.73s/it] \u001B[A\n",
      "episode_reward_mean = 70.60059356689453:  40%|████      | 8/20 [00:53<01:21,  6.75s/it]\u001B[A\n",
      "episode_reward_mean = 88.29077911376953:  45%|████▌     | 9/20 [01:00<01:14,  6.78s/it]\u001B[A\n",
      "episode_reward_mean = 82.82210540771484:  50%|█████     | 10/20 [01:07<01:07,  6.78s/it]\u001B[A\n",
      "episode_reward_mean = 83.88153076171875:  55%|█████▌    | 11/20 [01:14<01:00,  6.77s/it]\u001B[A\n",
      "episode_reward_mean = 81.43898010253906:  60%|██████    | 12/20 [01:20<00:54,  6.79s/it]\u001B[A\n",
      "episode_reward_mean = 78.0661392211914:  65%|██████▌   | 13/20 [01:27<00:47,  6.81s/it] \u001B[A\n",
      "episode_reward_mean = 74.45596313476562:  70%|███████   | 14/20 [01:34<00:41,  6.86s/it]\u001B[A\n",
      "episode_reward_mean = 95.89022064208984:  75%|███████▌  | 15/20 [01:41<00:34,  6.85s/it]\u001B[A\n",
      "episode_reward_mean = 80.3445053100586:  80%|████████  | 16/20 [01:48<00:27,  6.86s/it] \u001B[A\n",
      "episode_reward_mean = 74.66099548339844:  85%|████████▌ | 17/20 [01:55<00:20,  6.88s/it]\u001B[A\n",
      "episode_reward_mean = 64.75782012939453:  90%|█████████ | 18/20 [02:02<00:13,  6.87s/it]\u001B[A\n",
      "episode_reward_mean = 72.76493835449219:  95%|█████████▌| 19/20 [02:09<00:06,  6.88s/it]\u001B[A\n",
      "episode_reward_mean = 94.99859619140625: 100%|██████████| 20/20 [02:16<00:00,  6.81s/it]\u001B[A\n",
      "2025-08-20 23:27:50,540 [torchrl][INFO] Training time: 84.43 seconds\n",
      "2025-08-20 23:27:50,554 [torchrl][INFO] macs: 211.52 MMac  Params: 57.54 k\n",
      "2025-08-20 23:27:53,336 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 23:27:53,457 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 84.80160522460938:   5%|▌         | 1/20 [00:06<02:10,  6.89s/it]\u001B[A\n",
      "episode_reward_mean = 83.2098159790039:  10%|█         | 2/20 [00:13<02:02,  6.83s/it] \u001B[A\n",
      "episode_reward_mean = 73.33618927001953:  15%|█▌        | 3/20 [00:20<01:55,  6.78s/it]\u001B[A\n",
      "episode_reward_mean = 77.07393646240234:  20%|██        | 4/20 [00:27<01:48,  6.78s/it]\u001B[A\n",
      "episode_reward_mean = 74.08436584472656:  25%|██▌       | 5/20 [00:34<01:41,  6.80s/it]\u001B[A\n",
      "episode_reward_mean = 79.00894165039062:  30%|███       | 6/20 [00:40<01:34,  6.77s/it]\u001B[A\n",
      "episode_reward_mean = 77.34698486328125:  35%|███▌      | 7/20 [00:47<01:27,  6.74s/it]\u001B[A\n",
      "episode_reward_mean = 63.03123474121094:  40%|████      | 8/20 [00:54<01:21,  6.76s/it]\u001B[A\n",
      "episode_reward_mean = 75.20254516601562:  45%|████▌     | 9/20 [01:01<01:14,  6.81s/it]\u001B[A\n",
      "episode_reward_mean = 100.09454345703125:  50%|█████     | 10/20 [01:07<01:08,  6.81s/it]\u001B[A\n",
      "episode_reward_mean = 69.23649597167969:  55%|█████▌    | 11/20 [01:14<01:01,  6.81s/it] \u001B[A\n",
      "episode_reward_mean = 93.41131591796875:  60%|██████    | 12/20 [01:21<00:54,  6.75s/it]\u001B[A\n",
      "episode_reward_mean = 77.51444244384766:  65%|██████▌   | 13/20 [01:28<00:47,  6.76s/it]\u001B[A\n",
      "episode_reward_mean = 85.23004913330078:  70%|███████   | 14/20 [01:34<00:40,  6.72s/it]\u001B[A\n",
      "episode_reward_mean = 80.00000762939453:  75%|███████▌  | 15/20 [01:41<00:33,  6.72s/it]\u001B[A\n",
      "episode_reward_mean = 60.203060150146484:  80%|████████  | 16/20 [01:48<00:26,  6.71s/it]\u001B[A\n",
      "episode_reward_mean = 69.3511962890625:  85%|████████▌ | 17/20 [01:55<00:20,  6.77s/it]  \u001B[A\n",
      "episode_reward_mean = 79.54512023925781:  90%|█████████ | 18/20 [02:01<00:13,  6.81s/it]\u001B[A\n",
      "episode_reward_mean = 72.1695785522461:  95%|█████████▌| 19/20 [02:08<00:06,  6.82s/it] \u001B[A\n",
      "episode_reward_mean = 69.37770080566406: 100%|██████████| 20/20 [02:15<00:00,  6.78s/it]\u001B[A\n",
      "2025-08-20 23:30:09,043 [torchrl][INFO] Training time: 85.11 seconds\n",
      "2025-08-20 23:30:09,060 [torchrl][INFO] macs: 211.52 MMac  Params: 57.54 k\n",
      "2025-08-20 23:30:11,782 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 23:30:11,886 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 72.68563079833984:   5%|▌         | 1/20 [00:06<02:10,  6.88s/it]\u001B[A\n",
      "episode_reward_mean = 71.49580383300781:  10%|█         | 2/20 [00:13<02:03,  6.87s/it]\u001B[A\n",
      "episode_reward_mean = 80.6973648071289:  15%|█▌        | 3/20 [00:20<01:56,  6.85s/it] \u001B[A\n",
      "episode_reward_mean = 70.62207794189453:  20%|██        | 4/20 [00:27<01:49,  6.85s/it]\u001B[A\n",
      "episode_reward_mean = 59.57958221435547:  25%|██▌       | 5/20 [00:34<01:44,  6.96s/it]\u001B[A\n",
      "episode_reward_mean = 81.37939453125:  30%|███       | 6/20 [00:41<01:37,  6.98s/it]   \u001B[A\n",
      "episode_reward_mean = 87.04835510253906:  35%|███▌      | 7/20 [00:48<01:30,  6.97s/it]\u001B[A\n",
      "episode_reward_mean = 78.26744079589844:  40%|████      | 8/20 [00:55<01:23,  6.99s/it]\u001B[A\n",
      "episode_reward_mean = 68.65802764892578:  45%|████▌     | 9/20 [01:02<01:16,  6.96s/it]\u001B[A\n",
      "episode_reward_mean = 59.66327667236328:  50%|█████     | 10/20 [01:09<01:09,  6.98s/it]\u001B[A\n",
      "episode_reward_mean = 73.91493225097656:  55%|█████▌    | 11/20 [01:16<01:02,  6.96s/it]\u001B[A\n",
      "episode_reward_mean = 80.12158203125:  60%|██████    | 12/20 [01:23<00:55,  6.96s/it]   \u001B[A\n",
      "episode_reward_mean = 91.96633911132812:  65%|██████▌   | 13/20 [01:30<00:48,  6.99s/it]\u001B[A\n",
      "episode_reward_mean = 77.34889221191406:  70%|███████   | 14/20 [01:37<00:42,  7.04s/it]\u001B[A\n",
      "episode_reward_mean = 78.0311279296875:  75%|███████▌  | 15/20 [01:44<00:34,  6.99s/it] \u001B[A\n",
      "episode_reward_mean = 70.1417007446289:  80%|████████  | 16/20 [01:51<00:27,  6.96s/it]\u001B[A\n",
      "episode_reward_mean = 56.499237060546875:  85%|████████▌ | 17/20 [01:58<00:20,  6.92s/it]\u001B[A\n",
      "episode_reward_mean = 53.09695053100586:  90%|█████████ | 18/20 [02:05<00:13,  6.99s/it] \u001B[A\n",
      "episode_reward_mean = 85.67414093017578:  95%|█████████▌| 19/20 [02:12<00:06,  6.94s/it]\u001B[A\n",
      "episode_reward_mean = 88.30937957763672: 100%|██████████| 20/20 [02:19<00:00,  6.95s/it]\u001B[A\n",
      "2025-08-20 23:32:30,928 [torchrl][INFO] Training time: 86.99 seconds\n",
      "2025-08-20 23:32:30,942 [torchrl][INFO] macs: 211.52 MMac  Params: 57.54 k\n",
      "2025-08-20 23:32:33,680 [torchrl][INFO] check_env_specs succeeded!\n",
      "2025-08-20 23:32:33,783 [torchrl][INFO] check_env_specs succeeded!\n",
      "\n",
      "episode_reward_mean = 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "episode_reward_mean = 101.37767028808594:   5%|▌         | 1/20 [00:06<02:10,  6.88s/it]\u001B[A\n",
      "episode_reward_mean = 101.34910583496094:  10%|█         | 2/20 [00:13<02:02,  6.79s/it]\u001B[A\n",
      "episode_reward_mean = 88.21151733398438:  15%|█▌        | 3/20 [00:20<01:54,  6.75s/it] \u001B[A\n",
      "episode_reward_mean = 91.1095962524414:  20%|██        | 4/20 [00:27<01:48,  6.77s/it] \u001B[A\n",
      "episode_reward_mean = 83.31657409667969:  25%|██▌       | 5/20 [00:33<01:41,  6.73s/it]\u001B[A\n",
      "episode_reward_mean = 83.85049438476562:  30%|███       | 6/20 [00:40<01:34,  6.77s/it]\u001B[A\n",
      "episode_reward_mean = 87.28966522216797:  35%|███▌      | 7/20 [00:47<01:27,  6.77s/it]\u001B[A\n",
      "episode_reward_mean = 86.52816772460938:  40%|████      | 8/20 [00:54<01:21,  6.76s/it]\u001B[A\n",
      "episode_reward_mean = 84.8031005859375:  45%|████▌     | 9/20 [01:00<01:14,  6.76s/it] \u001B[A\n",
      "episode_reward_mean = 84.32368469238281:  50%|█████     | 10/20 [01:07<01:07,  6.77s/it]\u001B[A\n",
      "episode_reward_mean = 74.87190246582031:  55%|█████▌    | 11/20 [01:14<01:00,  6.77s/it]\u001B[A\n",
      "episode_reward_mean = 80.36215209960938:  60%|██████    | 12/20 [01:21<00:54,  6.79s/it]\u001B[A\n",
      "episode_reward_mean = 83.71695709228516:  65%|██████▌   | 13/20 [01:28<00:47,  6.81s/it]\u001B[A\n",
      "episode_reward_mean = 97.82355499267578:  70%|███████   | 14/20 [01:34<00:40,  6.79s/it]\u001B[A\n",
      "episode_reward_mean = 66.59608459472656:  75%|███████▌  | 15/20 [01:41<00:34,  6.81s/it]\u001B[A\n",
      "episode_reward_mean = 72.35303497314453:  80%|████████  | 16/20 [01:48<00:27,  6.77s/it]\u001B[A\n",
      "episode_reward_mean = 83.58023071289062:  85%|████████▌ | 17/20 [01:55<00:20,  6.74s/it]\u001B[A\n",
      "episode_reward_mean = 104.39257049560547:  90%|█████████ | 18/20 [02:01<00:13,  6.77s/it]\u001B[A\n",
      "episode_reward_mean = 71.07194519042969:  95%|█████████▌| 19/20 [02:08<00:06,  6.84s/it] \u001B[A\n",
      "episode_reward_mean = 84.85523223876953: 100%|██████████| 20/20 [02:15<00:00,  6.79s/it]\u001B[A\n",
      "2025-08-20 23:34:49,619 [torchrl][INFO] Training time: 84.62 seconds\n",
      "2025-08-20 23:34:49,631 [torchrl][INFO] macs: 211.52 MMac  Params: 57.54 k\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T21:34:52.301589Z",
     "start_time": "2025-08-20T21:34:52.298876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# file_name='2_navigation_mostly_low'\n",
    "# training_agent = 5\n",
    "# strategies = [\n",
    "#     EmbeddingStrategy.CONCAT,\n",
    "#     EmbeddingStrategy.MLP,\n",
    "#     EmbeddingStrategy.MLP_LOCAL,\n",
    "#     EmbeddingStrategy.MLP_GLOBAL,\n",
    "#     EmbeddingStrategy.GRAPH_SAGE,\n",
    "#     EmbeddingStrategy.GRAPH_GAT,\n",
    "#     EmbeddingStrategy.GRAPH_GAT_v2,\n",
    "#     EmbeddingStrategy.SET_TRANSFORMER_INV,\n",
    "#     EmbeddingStrategy.SAB_TRANSFORMER,\n",
    "#     EmbeddingStrategy.ISAB_TRANSFORMER\n",
    "# ]\n",
    "#\n",
    "# run_mostly_low(strategies, scenario='navigation', file_name=file_name, training_agent=training_agent, testing_agent=20, all_iters=80, steps=100)"
   ],
   "id": "308ac75cd4a77d1c",
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
